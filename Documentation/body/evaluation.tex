\chapter{Evaluation}\label{chap:eval}

A user study was conducted to evaluate Replico's efficiency, effectiveness, and user-friendliness. The main goal was determining how easily users could communicate points of interest within the virtual environment using Replico. Secondary goals included assessing ease of use and overall user experience. The study aimed to answer four key research questions:

\begin{itemize}
    \item \textbf{RQ1}: How efficiently can users create a point of interest on a given object?
    \item \textbf{RQ2}: How effectively does Replico notify users when a point of interest is created?
    \item \textbf{RQ3}: How useful is the world-in-miniature metaphor for communicating points of interest? How useful is the representation of user locations on the replica for understanding intent? % 2nd part is inconclusive, users didn't use it :(
    \item \textbf{RQ4}: How user-friendly is Replico, and how much physical effort is required to use it? 
\end{itemize}

\section{Setup}

    % TODO: get computer specs

    The user study was conducted at FEUP, in the GIG laboratory in room I220. The setup included two VR-ready computers connected to a local network. Each computer had an HTC Vive Pro 2 headset and a VR controller for table tracking. Two touch surfaces -- a 32-inch infrared frame and a 47-inch capacitive Displax Skin Ultra touchscreen -- were placed on opposite tables within the central VR play space. Participants, in pairs, were seated in front of each touch surface with their backs facing each other, as shown in Figure \ref{fig:eval_setup}.

    \begin{figure}[h]
        \centering
        \includegraphics[width=1\linewidth]{figures/setup.png}
        \caption{Setup for the user study. In image (a) one participant is seated in front of the Displax Skin Ultra. In image (b) the participant is seated in front of the infrared touch frame.}
        \label{fig:eval_setup}
    \end{figure}

    One computer served as the host, while the other connected as a client. The setup process involved a starting screen where the moderator could select the IP address of the host computer. The roles of each computer did not change throughout the study to simplify the setup process and avoid confusion.

\section{Methodology}

    The study was conducted in pairs, with initial tasks performed individually and later tasks performed collaboratively. Each session consisted of three main parts: an introduction, a training session, and the main tasks. Each session lasted approximately 60 minutes. After each session, participants received a chocolate bar as a token of appreciation.

    Before the study began, participants were introduced briefly to the study's purpose and the Replico system. They then completed a consent form and a profiling questionnaire. Following this, they watched a video presentation explaining Replico's features, usage, and the tasks they would perform.

    During the training session, participants familiarized themselves with the system by experimenting with all of Replico's features. Once comfortable with the solo interactions, a set of points of interest and a simulated player were added to the environment, allowing users to practice acknowledging points of interest and joining the other user's table. After becoming comfortable with these interactions, they proceeded to the main tasks.

    The main tasks were performed using two different 3D models: a city and the Perseverance rover, described in Section \ref{sec:test_scenarios}. The order in which the models were used alternated between pairs to avoid bias. These tasks aimed to assess the efficiency and effectiveness of Replico's features, as described in Section \ref{sec:tasks}. Metrics for each task were collected as detailed in Section \ref{sec:evaluation_metrics}. Participants completed a questionnaire on the tasks they performed between each test scenario, described in Section \ref{sec:qualitative_data}.

    \subsection{Test Scenarios} \label{sec:test_scenarios}

        Three scenarios were used during the study, two for the main tasks, as shown in Figure \ref{fig:test_scenarios}. The first scenario, used for training, is a small dungeon tavern built with the free version of the KayKit Dungeon Remastered Pack from itch.io\footnote{\url{https://kaylousberg.itch.io/kaykit-dungeon-remastered}} and the Modular Asset Staging Tool (MAST) for Unity\footnote{\url{https://fertile-soil-productions.itch.io/mast}}. The second scenario is a city from Synty's POLYGON City Pack\footnote{\url{https://assetstore.unity.com/packages/3d/environments/urban/polygon-city-low-poly-3d-art-by-synty-95214}}, obtained through Unity's student plan. The third scenario features the Perseverance rover, obtained from NASA's 3D model repository\footnote{\url{https://nasa3d.arc.nasa.gov/detail/perseverance-glb}}, with the surrounding environment created using Unity's terrain tools and a tinted sand texture from Polyhaven\footnote{\url{https://polyhaven.com/a/sand_01}}.

        \begin{figure}[h]
            \centering
            \includegraphics[width=1\linewidth]{figures/test_scenarios.png}
            \caption{The three test scenarios used in the user study. From left to right: the dungeon tavern, the city, and the Perseverance rover.}
            \label{fig:test_scenarios}
        \end{figure}

        Two scenarios were selected for the main tasks to evaluate how well the approach works with different 3D models. The city model was chosen for its large, complex structure, allowing users to immerse themselves within it. The Perseverance rover model was selected for its smaller size but sufficient detail, enabling users to view the entire model at once without being part of it. The dungeon tavern was used for practice, providing a small, enclosed environment distinct from the main task scenarios.

        In all scenarios, the virtual world includes surrounding environment elements to provide context and a sense of scale. For example, the city is bordered by an ocean, the rover by the Martian surface, and the dungeon tavern by some of its walls. The WIM does not replicate these environmental elements, as they are not part of the primary scenario.

    \subsection{Tasks} \label{sec:tasks}

        Participants performed five main tasks during the study, using each of the two main scenarios. The first three tasks were done individually, while the last two were collaborative. Each task evaluated different aspects of Replico's features and user experience. The tasks are as follows:

        \begin{itemize}
            \item \textbf{Task 1}: Create a point of interest on six different predefined objects in the scene. This task evaluates how efficiently users can create points of interest.
            \item \textbf{Task 2}: Acknowledge five out of twelve predefined points of interest created by a simulated user. This task assesses how effectively Replico notifies users of new points of interest.
            \item \textbf{Task 3}: Teleport to four different predefined zones and orient themselves to face a specific object. This task measures how well users can navigate the environment using Replico.
            \item \textbf{Task 4 \& 5}: One user must show another user a specific object in the scene without verbal communication. The other user then verbally confirms the object they believe the first user is referring to. Once confirmed, the roles are reversed, and the process is repeated with a different object. This task evaluates how well users can communicate points of interest using Replico
        \end{itemize}

        Between each task, the environment was reset to its initial state. This meant that any points of interest created during the previous task were removed, and participants were placed back at their starting positions. This was done to ensure that each task was performed under the same conditions for all participants.

        In the first task, the predefined objects were chosen with different sizes and at various heights, visible in Figure \ref{fig:task_01}. The goal was to evaluate how efficiently users could create points of interest, not to test their ability to identify objects. To draw attention to them, these objects glowed with an expanding and contracting effect in the WIM. They increased in size and changed their outline color from white to green when the balloon intersected with them. Creating a point of interest during this state allowed users to progress to the next object. The objects appeared one at a time, immediately after the user created a point of interest on the previous object. The order in which the objects appeared was consistent for all participants.

        \begin{figure}[h!]
            \centering
            \includegraphics[width=1\linewidth]{figures/task_01.png}
            \caption{The six predefined objects used in Task 1 for each scenario. They are ordered from left to right, with the top row showing the objects in the city scenario while the bottom row shows the objects in the Perseverance rover scenario. The top-right image shows the green outline effect that appears when the balloon intersects with the object.}
            \label{fig:task_01}
        \end{figure}

        In the second task, the predefined points of interest were placed around the environment simultaneously, as shown in Figure \ref{fig:task_02}. The goal was to evaluate how effectively Replico notifies users of new points of interest and helps distinguish acknowledged from unacknowledged points of interest. The task was complete when the user acknowledged the five unacknowledged points of interest. Participants could acknowledge the points of interest in any order.

        \begin{figure}[h!]
            \centering
            \includegraphics[width=1\linewidth]{figures/task_02.png}
            \caption{The created points of interest in Task 2. Image (a) shows the points of interest in the city scenario, while image (b) shows the points of interest in the Perseverance rover scenario.}
            \label{fig:task_02}
        \end{figure}

        In the third task, participants navigated through four predefined zones in the environment, as shown in Figure \ref{fig:task_03}. The goal was to assess how well users could navigate using Replico. Participants were instructed to teleport to each zone and orient themselves to face a specific object. Zones changed color from white to green when the balloon intersected with them, and the object's glow effect also changed from white to green when the user was correctly oriented. The moderator advanced the task manually, allowing participants to take their time exploring the environment. The sequence of zones was the same for all participants.

        \begin{figure}[h]
            \centering
            \includegraphics[width=1\linewidth]{figures/task_03.png}
            \caption{The four predefined zones used in Task 3 for each scenario. They are ordered from left to right, with the top row showing the zones in the city scenario while the bottom row shows the zones in the Perseverance rover scenario.}
            \label{fig:task_03}
        \end{figure}

        In the fourth and fifth tasks, participants communicated objects of interest without verbal communication. Figure \ref{fig:task_04} shows the objects used in these tasks. The goal was to assess how effectively users could communicate using Replico. The selected objects were small and difficult to identify from a distance, encouraging the use of the WIM, points of interest, and teleportation for effective communication. For the Perseverance rover model, since most people are unfamiliar with its components, the objects chosen were a small hidden star and a small hidden heart.

        \begin{figure}[h]
            \centering
            \includegraphics[width=1\linewidth]{figures/task_04.png}
            \caption{The objects used in Tasks 4 and 5 for each scenario. The top row shows the objects in the city scenario, while the bottom row shows the objects in the Perseverance rover scenario.}
            \label{fig:task_04}
        \end{figure}


    \subsection{Metrics} \label{sec:evaluation_metrics}

        During each task, several metrics were collected to evaluate the efficiency and effectiveness of Replico's features. These metrics, recorded automatically by the system, included the time in seconds taken to complete the task, the number of successful task steps, time spent transforming the replica, time spent in vertical transformation, and time spent in balloon selection. Additionally, the system tracked the number of times the transform gesture was detected (on entering \lstinline{TransformReplicaState}), the number of times vertical transform was detected (on entering \lstinline{TransformReplicaVerticalState}), and the number of times the balloon selection gesture was detected (on entering \lstinline{BalloonSelectionInitialState}). It also recorded the number of points of interest created, deleted, and acknowledged, the number of teleportations, the number of table joins, the number of touches on the touch surface, the cumulative sum of finger movement in pixels, the cumulative sum of head rotation in angles, cumulative head translation in meters, cumulative sum of replica rotation in angles, cumulative sum of replica translation in meters, and cumulative sum of replica scaling in meters.

        The time taken to complete each task was measured from the start to the end of the task. In Task 3, while the participant waited for the moderator to advance to the next zone, all metrics were paused. The number of successful task steps was the number of steps completed correctly by the user. For example, in Task 1, a successful step was creating a point of interest on a predefined object. In Task 2, a successful step was acknowledging a point of interest. In Task 3, a successful step was teleporting to a zone and orienting oneself to face a specific object. In Tasks 4 and 5, it referred to the number of guesses until the correct object was identified.

        Additionally, for each frame that showed a change, data was collected and timestamped. This included finger data, replica transform data, and head transform data
    
    \subsection{Qualitative Data} \label{sec:qualitative_data}

        After each test scenario, participants completed a questionnaire to provide qualitative feedback on their experience. The questionnaire included six questions from the NASA Task Load Index (NASA-TLX) \cite{hart1988development} for each task, along with three additional questions at the end of the questionnaire, each rated on a 5-point Likert scale. The NASA-TLX questions were:

        \begin{itemize}
            \item How mentally demanding was the task?
            \item How physically demanding was the task?
            \item How hurried or rushed was the pace of the task?
            \item How successful were you in accomplishing what you were asked to do?
            \item How hard did you have to work to accomplish your level of performance?
            \item How insecure, discouraged, irritated, stressed, and annoyed were you?
        \end{itemize}

        The final three questions were:

        \begin{itemize}
            \item How nauseous did you feel during the task?
            \item How useful was the world-in-miniature metaphor for communicating points of interest?
            \item How useful was the representation of user locations on the replica for understanding intent?
        \end{itemize}

        Participants were also encouraged to provide additional comments or suggestions for improvement. The aim was to gather feedback on the user experience and identify areas for improvement in the system.

\section{Participants}

    A total of 20 participants, forming 10 pairs, took part in the study. Among them, eleven were male, and 9 were female. Most participants, 19, were aged between 21 and 30 years old, with one participant aged between 31 and 40. 19 participants were right-handed, and one was left-handed. 17 participants were students, of whom six also worked, while three were exclusively workers. 15 participants had completed a bachelor's degree, two had a master's degree, and three had a high school diploma. Regarding VR experience, 11 had used VR once before, three had used it in the past year, one used it frequently, and five had never used VR before.

\section{Results}

    The results of the user study are presented in this section. Two samples were collected for each task metric, one for each test scenario. Appropriate statistical tests were performed to determine the significance of the results, with the significance level set at the conventional alpha value of 5\%.

    \subsection{Metrics}

        Using descriptive data analysis, outliers were identified and removed from the dataset. The Shapiro-Wilk test \cite{shapiroAnalysisVarianceTest1965} was used to determine if the data followed a normal distribution. Since the analysis involved testing two related samples, either the paired t-test or the Wilcoxon signed-rank test \cite{wilcoxonIndividualComparisonsRanking1945}was used to determine if the differences between the samples were statistically significant. The paired t-test was used for normally distributed data, while the Wilcoxon signed-rank test was used for non-normally distributed data.

        For task 4 and task 5 metrics, with the exception of task completion time (which was the same for both participants), the metrics were split into two separate tasks: one for the seeker (TaskSeek) and one for the shower (TaskShow). A two-way repeated measures ANOVA with Bonferroni correction was used to determine if there were significant differences between two factors: the object (task 4 or task 5) and the scenario (city or rover). If the sphericity assumption was violated, through Mauchly's test of sphericity, the Greenhouse-Geisser correction was applied. If the interaction effect was significant, simple main effects analysis was performed to determine where the differences lay. The studentized residuals were checked for outliers, and the data was checked for normality, to determine if the assumptions of the ANOVA were met.

        Five metrics were selected for analysis: task completion time, active time, finger movement, replica translation, and head translation. The results for each metric are presented in the following sections.

        \subsubsection{Task Completion Time}

            % task1city follows a normal distribution (p = .128)
            % task2city follows a normal distribution (p = .268)
            % task3city does not follow a normal distribution (p=.004) --!
            % task4city does not follow a normal distribution (p=.005)
            % task5city does not follow a normal distribution (p=.002)
            % task1rover follows a normal distribution (p = .064)
            % task2rover follows a normal distribution (p = .735)
            % task3rover follows a normal distribution (p = .580) -- !
            % task4rover does not follow a normal distribution (p=.026)
            % task5rover does not follow a normal distribution (p=.012)

            \begin{figure}[h!]
                \centering
                \includegraphics[width=1\linewidth]{figures/task_time_graph.pdf}
                \caption{Box-plot of the time taken to complete each task for each scenario. The symbol $\ast$ indicates a significant difference between the city and rover scenarios.}
                \label{fig:task_time}
            \end{figure}

            Task completion time, recorded in seconds, measures how long it took participants to complete each task. The results for each task are shown in Figure \ref{fig:task_time}. For the first task, there was no significant difference in completion time between the city and rover scenarios ($t(14) = 1.440,\ p = 0.172$). Similarly, the second task showed no significant difference in completion time between the two scenarios ($t(10) = 0.134,\ p = 0.896$). However, for the third task, there was a significant difference in completion time between the two scenarios ($Z = -3.248,\ p = 0.001$), with participants taking longer in the city scenario. The fourth task also showed no significant difference in completion time between the two scenarios ($Z = -1.415,\ p = 0.157$). The fifth task had a significant difference in completion time between the two scenarios ($Z = -3.154,\ p = 0.002$), with participants taking longer in the city scenario.

            Overall, task completion times were mostly similar between the city and rover scenarios, except for tasks 3 and 5. Task 3 took longer in the city scenario, likely due to the difficulty in locating the zones. These zones were more spread out and smaller, making balloon selection harder. Although participants could zoom in to aid in selection, they generally did not. Unlike teleport points in task 3, points of interest do not scale with the replica, making it easier to acknowledge them without scaling. Task 5 also took longer in the city scenario because the shower's object was small and hidden among other objects, making it harder for the seeker to find. In the rover scenario, the objects were easier to locate, with some participants finding them by accident. Points of interest in the WIM obscured the objects, further complicating the search. Instead of teleporting to the objects, participants preferred to zoom in, but the points of interest made this more challenging.

            % TODO: note, there is a reason why users do not teleport: teleporting was hard for many users!

        \subsubsection{Active Time}

            % task1city p = .042: does not follow a normal distribution
            % task2city p = .572: follows a normal distribution
            % task3city p=.005: does not follow a normal distribution
            % task1rover p = .068: follows a normal distribution
            % task2rover p = .802: follows a normal distribution
            % task3rover p = .209: follows a normal distribution
   
            \begin{figure}[h!]
                \centering
                \includegraphics[width=1\linewidth]{figures/active_time_graph.pdf}
                \caption{Box plot showing the active time for each task in both scenarios. The asterisk ($\ast$) indicates a significant difference between the city and rover scenarios. The dagger ($\dag$)  shows significant differences between objects in TaskSeek and TaskShow.}
                \label{fig:active_time}
            \end{figure}

            Active time, measured in seconds, represents the duration participants spent actively touching the touch surface. The results for each task are shown in Figure \ref{fig:active_time}. In the first task, there was no significant difference in active time between the city and rover scenarios ($t(12) = -1.913,\ p = 0.056$). The second task also showed no significant difference in active time between the two scenarios ($t(12) = -1.431,\ p = 0.178$). However, for the third task, participants spent significantly more active time in the city scenario ($Z = -3.051,\ p=0.002$).

            A repeated measures ANOVA with Greenhouse-Geisser correction revealed no significant difference in active time between the different scenarios in TaskSeek ($F(1, 9) = 4.680,\ p = 0.059$), and no significant difference between the different objects ($F(1, 9) = 2.468,\ p = 0.151$). However, there was a significant interaction effect between the scenarios and objects ($F(1, 9) = 12.630,\ p = 0.006$). Simple main effects analysis showed that, in the city scenario, there was a significant difference in active time between the different objects ($p=0.035$), with the second object having a significantly higher active time. In the rover scenario, there was no significant difference in active time between the different objects ($p=0.075$). For the first object, there was no significant difference in active time between the city and rover scenarios ($p=0.767$). However, for the second object, there was a significant difference in active time between the city and rover scenarios ($p=0.017$).

            A repeated measures ANOVA with Greenhouse-Geisser correction also determined no significant difference in active time between the different scenarios in TaskShow ($F(1, 6) = 4.472,\ p = 0.079$), and no significant difference between the different objects ($F(1, 6) = 3.217,\ p = 0.123$). However, there was a significant interaction effect between the scenarios and objects ($F(1, 6) = 16.370,\ p = 0.007$). Simple main effects analysis revealed that, in the city scenario, there was a significant difference in active time between the different objects ($p=0.032$), with the second object having a significantly higher active time. In the rover scenario, there was no significant difference in active time between the different objects ($p=0.429$). For the first object, there was no significant difference in active time between the city and rover scenarios ($p=0.935$). However, for the second object, there was a significant difference in active time between the city and rover scenarios ($p=0.025$).

            Overall, similar to task completion time, active time was mostly comparable between the city and rover scenarios, except for task 3, TaskSeek, and TaskShow, where participants spent more active time in the city scenario.
        \subsubsection{Finger Movement}

            % task1city p = .455: follows a normal distribution
            % task2city p = .2: follows a normal distribution
            % task3city p =.045: does not follow a normal distribution
            % task4seekCity p =.409: follows a normal distribution
            % task4showCity p =.479: follows a normal distribution
            % task5seekCity p =.621: follows a normal distribution
            % task5showCity p =.018: does not follow a normal distribution
            % task1rover p = .323: follows a normal distribution
            % task2rover p = .801: follows a normal distribution
            % task3rover p = .255: follows a normal distribution
            % task4seekRover p = .358: follows a normal distribution
            % task4showRover p = .582: follows a normal distribution
            % task5seekRover p = .177: follows a normal distribution
            % task5showRover p = .921: follows a normal distribution

            \begin{figure}[h!]
                \centering
                \includegraphics[width=1\linewidth]{figures/finger_movement_graph.pdf}
                \caption{Box plot showing the cumulative sum of finger movement in meters for each task in both scenarios. The asterisk ($\ast$) indicates a significant difference between the city and rover scenarios.}
                \label{fig:finger_movement}
            \end{figure}

            Finger movement, measured in meters, represents the cumulative distance traveled by participants' fingers on the touch surface. To convert pixel data to meters, each touch surface's pixels per inch (PPI) was calculated using its dimensions and screen resolution. The pixel finger movement data was then divided by the PPI to convert it to inches, and this value was subsequently converted to meters. The results for each task are shown in Figure \ref{fig:finger_movement}. 

            In the first task, there was a significant difference in finger movement between the city and rover scenarios ($t(14) = 2.976,\ p = 0.010$), with participants moving their fingers more in the city scenario. The second task showed no significant difference in finger movement between the two scenarios ($t(15) = 1.030,\ p = 0.320$). However, for the third task, there was a significant difference in finger movement between the city and rover scenarios ($Z = -3.285,\ p = 0.001$), with participants again moving their fingers more in the city scenario.

            A repeated measures ANOVA with Greenhouse-Geisser correction revealed a significant difference in finger movement between the different scenarios in TaskSeek ($F(1, 5) = 9.907,\ p = 0.025$), with participants moving their fingers more in the city scenario. There was no significant difference in finger movement between the different objects ($F(1, 5) = 2.048,\ p = 0.212$), and no significant interaction effect between the scenarios and objects ($F(1, 9) = 0.976,\ p = 0.368$). For TaskShow, there was a significant difference in finger movement between the different scenarios ($F(1, 6) = 6.409,\ p = 0.045$), with participants moving their fingers more in the city scenario. There was no significant difference in finger movement between the different objects ($F(1, 6) = 4.822,\ p = 0.070$), and no significant interaction effect between the scenarios and objects ($F(1, 6) = 5.897,\ p = 0.050$).

            Overall, participants moved their fingers more in the city scenario than in the rover scenario, except for Task 2. This difference is likely due to the city scenario's larger, more complex environment, with objects of interest more spread out, requiring more exploration and finger movement for transform gestures. The lack of difference in Task 2 may be because points of interest do not scale with the replica, making them easier to acknowledge without scaling as they remain the same size.

        \subsubsection{Replica Translation}

            % task1city p = .003: does not follow a normal distribution
            % task2city p = .066: follows a normal distribution
            % task3city p = .095: follows a normal distribution
            % task1rover p = .354: follows a normal distribution
            % task2rover p = .185: follows a normal distribution
            % task3rover p = .234: follows a normal distribution
            
            \begin{figure}[h!]
                \centering
                \includegraphics[width=1\linewidth]{figures/replica_translation_graph.pdf}
                \caption{Box-plot of the cumulative sum of replica translation in meters for each task for each scenario.}
                \label{fig:task_time}
            \end{figure}


            % task1 using Wilcoxon signed-rank test
                % Z = -3.058, p =.002
                % significant difference between the cumulative sum of replica translation in Task 1 in the city and rover scenarios: participants moved the replica more in the city scenario
            % task2 using paired t-test
                % t(16) = 3.594,p = .002
                % significant difference between the cumulative sum of replica translation in Task 2 in the city and rover scenarios: participants moved the replica more in the city scenario
            % task3 using Wilcoxon signed-rank test
                % t(16) = 6.724, p < .001
                % significant difference between the cumulative sum of replica translation in Task 3 in the city and rover scenarios: participants moved the replica more in the city scenario
            % taskseek using repeated measures ANOVA, with two factors: object and scenario
                % studentized residuals have no significant outliers, are normally distributed
                % sphericity assumption is violated, Greenhouse-Geisser correction applied
                % within-subjects effect of scenario: F(1, 4) = 34.498, p = .004, significant difference between the cumulative sum of replica translation in the city and rover scenarios
                % within-subjects effect of object: F(1, 4) = 4.246, p = 0.108, no significant difference between the cumulative sum of replica translation in the different objects
                % within-subjects interaction effect: F(1, 4) = 3.742, p = 0.125, no significant difference between the cumulative sum of replica translation in the city and rover scenarios for the different objects
            % taskshow using repeated measures ANOVA, with two factors: object and scenario
                % studentized residuals have no significant outliers, are normally distributed. Except for object 1 in the rover scenario, which is close to the threshold. Proceeding with the analysis, shouldn't reduce the power of the test by much
                % sphericity assumption is violated, Greenhouse-Geisser correction applied
                % within-subjects effect of scenario: F(1, 7) = 21.834, p = .002, significant difference between the cumulative sum of replica translation in the city and rover scenarios 
                % within-subjects effect of object: F(1, 7) = 3.039, p = 0.125, no significant difference between the cumulative sum of replica translation in the different objects
                % within-subjects interaction effect: F(1, 7) = 4.955, p = 0.065, no significant difference between the cumulative sum of replica translation in the city and rover scenarios for the different objects 

        \subsubsection{Head Translation}

            % task1city p = 0.902: follows a normal distribution
            % task2city p = 0.157: follows a normal distribution
            % task3city p = 0.013: does not follow a normal distribution
            % task1rover p = 0.425: follows a normal distribution
            % task2rover p = 0.007: does not follow a normal distribution
            % task3rover p = 0.443: follows a normal distribution    

            \begin{figure}[h!]
                \centering
                \includegraphics[width=1\linewidth]{figures/head_graph.pdf}
                \caption{Box-plot of the cumulative head translation in meters for each task for each scenario.}
                \label{fig:task_time}
            \end{figure}

            % task1 using paired t-test
                % t(9) = 0.550, p =.298
                % no significant difference between the cumulative head translation in Task 1 in the city and rover scenarios
            % task2 using wilcoxon signed-rank test
                % Z = -1.870, p =.062
                % no significant difference between the cumulative head translation in Task 2 in the city and rover scenarios
            % task3 using Wilcoxon signed-rank test
                % Z = -2.427, p=.015
                % significant difference between the cumulative head translation in Task 3 in the city and rover scenarios: participants moved their head more in the city scenario
            % taskseek using repeated measures ANOVA, with two factors: object and scenario
                % studentized residuals have no significant outliers, are normally distributed
                % sphericity assumption is violated, Greenhouse-Geisser correction applied
                % within-subjects effect of scenario: F(1, 5) = 7.679, p = .039, significant difference between the cumulative head translation in the city and rover scenarios
                % within-subjects effect of object: F(1, 5) = 4.505, p = 0.087, no significant difference between the cumulative head translation in the different objects
                % within-subjects interaction effect: F(1, 5) = 7.237, p = 0.043, significant difference between the cumulative head translation in the city and rover scenarios for the different objects
                    % simple main effects analysis
                        % for the city scenario, there is no significant difference between the cumulative head translation in the different objects: p = 0.064 
                        % for the rover scenario, there is no significant difference between the cumulative head translation in the different objects: p = 0.799
                        % for the first object, there is no significant difference between the cumulative head translation in the city and rover scenarios: p = 0.106
                        % for the second object, there is a significant difference between the cumulative head translation in the city and rover scenarios: p = 0.040 
       

    \subsection{Qualitative Data}

        The results obtained from the questionnaires are discrete, ordinal data, as opposed to the continuous data obtained from the metrics. As such, the Wilcoxon signed-rank test was used to determine if there were significant differences between the responses for each question in each scenario.

        For the first task, Table \ref{tab:analysis_qualitative_1} shows the results. There were no significant differences between the city and rover scenarios for any of the questions. This suggests that the task load was similar in both scenarios. Participants didn't find the tasks mentally or physically demanding, nor did they feel rushed or insecure. They also felt successful in accomplishing what they were asked to do and felt lower-to-moderate effort was required to achieve their level of performance.
       
        \begin{table}[h!]
            \caption{Median (IQR), and Wilcoxon Signed Ranks test results for each of the NASA-TLX questions for the first task in each scenario.}
            \begin{tabularx}{1\textwidth}{X l l l}
                \hline
                \multirow{2}{*}{Question} & \multicolumn{2}{c}{Median (IQR)} & \multirow{2}{*}{Wilcoxon Signed Ranks} \\
                \cline{2-3}
                & \makecell{City} & \makecell{Rover} &  \\
                \hline
                \hline
                How mentally demanding was the task? & 2 (1) & 1 (1) & $Z = -0.977,\ p = 0.329$ \\
                How physically demanding was the task? & 1 (1) & 1 (0.75) & $Z = -0.322,\ p = 0.748$  \\
                How hurried or rushed was the pace of the task? & 2 (1.75) & 1 (2) & $Z = -1.578,\ p = 0.115$\\
                How successful were you in accomplishing what you were asked to do? & 5 (1) & 5 (0.75) & $Z = -1.100,\ p = 0.271$ \\
                How hard did you have to work to accomplish your level of performance? & 2 (1.75) & 2 (2) & $Z = -0.844,\ p = 0.399$ \\
                How insecure, discouraged, irritated, stressed, and annoyed were you? & 1 (1) & 1 (0.75) & $Z = -0.551,\ p = 0.582$ \\
            \end{tabularx}
            \label{tab:analysis_qualitative_1}
        \end{table}
 
        For the second task, Table \ref{tab:analysis_qualitative_2} shows the results. The city scenario was found to be statistically significantly more physically demanding than the rover scenario

        \begin{table}[h!]
            \caption{Median (IQR), and Wilcoxon Signed Ranks test results for each of the NASA-TLX questions for the second task in each scenario.}
            \begin{tabularx}{1\textwidth}{X l l l}
                \hline
                \multirow{2}{*}{Question} & \multicolumn{2}{c}{Median (IQR)} & \multirow{2}{*}{Wilcoxon Signed Ranks} \\
                \cline{2-3}
                & \makecell{City} & \makecell{Rover} &  \\
                \hline
                \hline
                How mentally demanding was the task? & 2 (1) & 1 (1) & $Z = -1.311,\ p = 0.190$  \\
                How physically demanding was the task? & 1 (1) & 1 (0.75) & $Z = -1.265,\ p = 0.206$ \\
                How hurried or rushed was the pace of the task? & 2 (1.75) & 1.5 (1) & $Z = -0.905,\ p = 0.366$ \\
                How successful were you in accomplishing what you were asked to do? & 5 (1) & 5 (1) & $Z = -0.905,\ p = 0.366$ \\
                How hard did you have to work to accomplish your level of performance? $\ast$ & 2 (1.75) & 1.5 (1) & $Z = -2.055,\ p = 0.040$ \\
                How insecure, discouraged, irritated, stressed, and annoyed were you? & 1 (1) & 1 (0) & $Z = -0.921,\ p = 0.357$ \\
            \end{tabularx}

            \label{tab:analysis_qualitative_2}
        \end{table} 

        \begin{table}[h!]
            \caption{Median (IQR), and Wilcoxon Signed Ranks test results for each of the NASA-TLX questions for the third task in each scenario.}
            \begin{tabularx}{1\textwidth}{X l l l}
                \hline
                \multirow{2}{*}{Question} & \multicolumn{2}{c}{Median (IQR)} & \multirow{2}{*}{Wilcoxon Signed Ranks} \\
                \cline{2-3}
                & \makecell{City} & \makecell{Rover} &  \\
                \hline
                \hline
                How mentally demanding was the task? & 2.5 (1) & 2 (2) & $Z = -0.819,\ p = 0.413$ \\
                How physically demanding was the task? & 2 (2) & 1.5 (1) & $Z = -1.311,\ p = 0.190$ \\
                How hurried or rushed was the pace of the task? & 2 (2) & 1.5 (1) & $Z = -1.394,\ p = 0.163$ \\
                How successful were you in accomplishing what you were asked to do? & 4 (1) & 4 (1) & $Z = -0.262,\ p = 0.794$ \\
                How hard did you have to work to accomplish your level of performance? & 3 (2) & 2 (2.5) & $Z = -1.279,\ p = 0.201$ \\
                How insecure, discouraged, irritated, stressed, and annoyed were you? & 1 (1) & 1 (1) & $Z = -0.431,\ p = 0.666$ \\
            \end{tabularx}

            \label{tab:analysis_qualitative_3}
        \end{table} 

        \begin{table}[h!]
            \caption{Median (IQR), and Wilcoxon Signed Ranks test results for each of the NASA-TLX questions for the fourth task in each scenario.}
            \begin{tabularx}{1\textwidth}{X l l l}
                \hline
                \multirow{2}{*}{Question} & \multicolumn{2}{c}{Median (IQR)} & \multirow{2}{*}{Wilcoxon Signed Ranks} \\
                \cline{2-3}
                & \makecell{City} & \makecell{Rover} &  \\
                \hline
                \hline
                How mentally demanding was the task? $\ast$ & 2.5 (1) & 2 (1) & $Z = -2.365,\ p = 0.018$ \\
                How physically demanding was the task? $\ast$ & 2 (1) & 1 (0.75) & $Z = -2.310,\ p = 0.021$\\
                How hurried or rushed was the pace of the task? & 2 (1.75) & 1.5 (2) & $Z = -1.604,\ p = 0.109$ \\
                How successful were you in accomplishing what you were asked to do? $\ast$ & 4 (1) & 5 (0) & $Z = -2.581,\ p = 0.010$ \\
                How hard did you have to work to accomplish your level of performance? & 2 (1) & 1 (1.75) & $Z = -1.787,\ p = 0.074$ \\
                How insecure, discouraged, irritated, stressed, and annoyed were you? & 1 (1) & 1 (0) & $Z = -1.150,\ p = 0.250$ \\
            \end{tabularx}

            \label{tab:analysis_qualitative_4}
        \end{table} 

        \begin{table}[h!]
            \caption{Median (IQR), and Wilcoxon Signed Ranks test results for each of the general evaluation questions in each scenario.}
            \begin{tabularx}{1\textwidth}{X l l l}
                \hline
                \multirow{2}{*}{Question} & \multicolumn{2}{c}{Median (IQR)} & \multirow{2}{*}{Wilcoxon Signed Ranks} \\
                \cline{2-3}
                & \makecell{City} & \makecell{Rover} &  \\
                \hline
                \hline
                How nauseous did you feel during the task? & 1 (1) & 1 (0) & $Z = -0.636,\ p = 0.525$ \\
                How useful was the world-in-miniature metaphor for communicating points of interest? & 5 (0.75) & 5 (0.75) & $Z = -0.333,\ p = 0.739$ \\
                How useful was the representation of user locations on the replica for understanding intent? & 4 (1) & 5 (1) & $Z = -0.093,\ p = 0.926$ \\
            \end{tabularx}
            \label{tab:analysis_qualitative_g}
        \end{table}

    \subsection{Observations}

% do speak about how the frame being too sensitive to resting arms impacted the results
    % infrared table: because it does not track fingers on touch, but instead using infrared, it would pick up extra fingers if the user didn't lift their hands high enough, or fingers came too close together. This led to many points of interest created on accident
        % it also had a relatively low resolution, which made it hard to select small objects. This could be mitigated by scaling the replica, but it was not intuitive for some users to do so.
    % displax table: had some issue with losing fingers when moving them, which would sometimes cancel gestures. It also required fingers to be more spread out or else it would consider them as one finger. Adittionaly, because the table was so light-reflective, if the user approached the table, the headset tracking would start jittering.
        % even with the mitigation of not terminating the gesture for some time after the finger was lost, it was still an issue as it would sometimes switch fingers from on hand to the other, changing the balloon's position.
% teleport destination did not match the user's head, but instead the bottom of the table, which was confusing for some users.
% vertical transform rarely used
% depth perception issues
    % the limits of the table were not clear to some users because of this. This is also because the limit effect is not perfect, it has the same direction as the user's view point, instead of outwards or inwards from the limits of the table.
    % it was very discouraging for some users.
% headset tracking issues
% people forgetting how to teleport
% teleporting hard to understand
    % especially in regards to long press, and the rotation of the balloon
% many points of interest created by accident
% points of interest obscuring objects
% no easy way to know your color
% task 3 was confusing for some people because they didn't know what they were supposed to do. They either thought you only had to teleport to the zones, or they didn't know they had to orient themselves to face the object.
    % another issue was that if you created a point of interest by accident near a zone, it would obscure the zone. Because the system prioritizes the zones over the points of interest, users usually had a hard time deleting those points of interest, and so they couldnt see the zone and if they were inside the zone correctly (see it change to green)
% usually the second scenario would always be better than the first, because people would get used to the system.
% would be cool to see exactly where someone is looking at, like a raycast from the eyes
% -- positives --
% users liked that they could see each other.
% users found balloon selection easy to use, and intuitive

    \subsection{Discussion}

    \section{Summary}
