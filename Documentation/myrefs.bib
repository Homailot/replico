%bibtex entries 

@book{aguerrecheEvaluationReconfigurableTangible2011,
  title = {Evaluation of a {{Reconfigurable Tangible Device}} for {{Collaborative Manipulation}} of {{Objects}} in {{Virtual Reality}}},
  author = {Aguerreche, Laurent and Duval, Thierry and L{\'e}cuyer, Anatole},
  year = {2011},
  publisher = {{The Eurographics Association}},
  doi = {10.2312/LocalChapterEvents/TPCG/TPCG11/081-088},
  urldate = {2023-11-17},
  abstract = {In this paper we introduce an evaluation of a Reconfigurable Tangible Device (RTD) for collaborative manipulation of objects in virtual environments. The considered RTD, called RTD-3, has a triangular shape that naturally provides three points of manipulation. The shape of the tangible triangle can be reconfigured at any time as its branches can be shrunk or stretched by users at will. Thanks to this simple shape the RTD-3 can be easily attached to any 3D virtual object and fully defines its virtual motion in 6 Degrees of Freedom. We have conducted an experiment to assess the potential of the RTD-3 and compare it with classical techniques used for collaborative virtual manipulation. Participants were asked to manipulate and assemble, in a collaborative manner, virtual parts. Our results suggest that the physical manipulation proposed by the tangible device is significantly preferred by participants in terms of immersion, realism of interaction and preparation to the real task. Although our approach is slightly slower than the other tested methods, it produces the fewest collisions.},
  isbn = {978-3-905673-83-8},
  langid = {english},
  keywords = {notion},
  annotation = {Accepted: 2013-10-31T10:30:56Z},
  file = {/home/nunoa/Zotero/storage/77KVIWPB/Aguerreche et al. - 2011 - Evaluation of a Reconfigurable Tangible Device for.pdf}
}

@inproceedings{aguerrecheReconfigurableTangibleDevices2010,
  title = {Reconfigurable Tangible Devices for {{3D}} Virtual Object Manipulation by Single or Multiple Users},
  booktitle = {Proceedings of the 17th {{ACM Symposium}} on {{Virtual Reality Software}} and {{Technology}}},
  author = {Aguerreche, Laurent and Duval, Thierry and L{\'e}cuyer, Anatole},
  year = {2010},
  month = nov,
  series = {{{VRST}} '10},
  pages = {227--230},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1889863.1889913},
  urldate = {2023-11-09},
  abstract = {In this paper we introduce the concept of a Reconfigurable Tangible Device for manipulation of 3D objects in virtual environments by single or multiple users. This Reconfigurable Tangible Device (RTD) provides points of manipulation rigidly linked together. The shape of the RTD can be reconfigured at any time as its arms can be compressed or stretched by users at will. Due to its simple shape the Reconfigurable Tangible Device can be attached to any 3D virtual object. Then, it can fully define the motion of the virtual object in 6 Degrees of Freedom. Two examples of Reconfigurable Tangible Device were developed: one with three points of manipulation (a reconfigurable triangle) and one with four points. We illustrate how these two simple devices can match many different shapes of 3D objects, and in different contexts. Preliminary testing was conducted with the RTD based on three points of manipulation involving a collaborative manipulation task in virtual reality. It produced better subjective appreciation for the RTD compared to more classical 3D collaborative techniques.},
  isbn = {978-1-4503-0441-2},
  keywords = {3D interaction,collaborative interaction,notion,reconfigurable tangible user interface,virtual reality},
  file = {/home/nunoa/Zotero/storage/U5D2P2PZ/Aguerreche et al. - 2010 - Reconfigurable tangible devices for 3D virtual obj.pdf}
}

@inproceedings{allisonEffectsNetworkDelay2004,
  title = {Effects of Network Delay on a Collaborative Motor Task with Telehaptic and Televisual Feedback},
  booktitle = {Proceedings of the 2004 {{ACM SIGGRAPH}} International Conference on {{Virtual Reality}} Continuum and Its Applications in Industry},
  author = {Allison, Robert S. and Zacher, James E. and Wang, David and Shu, Joseph},
  year = {2004},
  month = jun,
  series = {{{VRCAI}} '04},
  pages = {375--381},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1044588.1044670},
  urldate = {2023-10-22},
  abstract = {The incorporation of haptic interfaces into collaborative virtual environments is challenging when the users are geographically distributed. Reduction of latency is essential for maintaining realism, causality and the sense of co-presence in collaborative virtual environments during closely-coupled haptic tasks. In this study we consider the effects of varying amounts of simulated constant delay on the performance of a simple collaborative haptic task. The task was performed with haptic feedback alone or with visual feedback alone. Subjects were required to make a coordinated movement of their haptic displays as rapidly as possible, while maintaining a target simulated spring force between their end effector and that of their collaborator. Increasing simulated delay resulted in a decrease in performance, either in deviation from target spring force and in increased time to complete the task. At large latencies, there was evidence of dissociation between the states of the system that was observed by each of the collaborating users. This confirms earlier anecdotal evidence that users can be essentially seeing qualitatively different simulations with typical long distance network delays.},
  isbn = {978-1-58113-884-9},
  keywords = {collaborative virtual environments,delay,haptics,notion,teleoperation,virtual environments}
}

@article{almeidaObjectManipulationDesk2023,
  title = {Object {{Manipulation}} in {{Desk VR}}},
  author = {de Almeida, Diogo Henrique Pinto},
  year = {2023},
  month = jul,
  urldate = {2023-09-26},
  copyright = {openAccess},
  langid = {english},
  keywords = {notion},
  annotation = {Accepted: 2023-08-09T23:18:34Z},
  file = {/home/nunoa/Zotero/storage/V3UUWABM/Almeida - 2023 - Object Manipulation in Desk VR.pdf}
}

@inproceedings{amaroDesignEvaluationTravel2022,
  title = {Design and {{Evaluation}} of {{Travel}} and {{Orientation Techniques}} for {{Desk VR}}},
  booktitle = {2022 {{IEEE Conference}} on {{Virtual Reality}} and {{3D User Interfaces}} ({{VR}})},
  author = {Amaro, Guilherme and Mendes, Daniel and Rodrigues, Rui},
  year = {2022},
  month = mar,
  pages = {222--231},
  issn = {2642-5254},
  doi = {10.1109/VR51125.2022.00041},
  urldate = {2023-09-26},
  abstract = {Typical VR interactions can be tiring, including standing up, walking, and mid-air gestures. Such interactions result in decreased comfort and session duration compared with traditional non-VR interfaces, which may, in turn, reduce productivity. Nevertheless, current approaches often neglect this aspect, making the VR experience not as promising as it can be. As we see it, desk VR experiences provide the convenience and comfort of a desktop experience and the benefits of VR immersion, being a good compromise between the overall experience and ergonomics. In this work, we explore navigation techniques targeted at desk VR users, using both controllers and a large multi-touch surface. We address travel and orientation techniques independently, considering only continuous approaches for travel as these are better suited for exploration and both continuous and discrete approaches for orientation. Results revealed advantages for a continuous controller-based travel method and a trend for a dragging-based orientation technique. Also, we identified possible trends towards task focus affecting overall cybersickness symptomatology.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/3HHZAUEN/Amaro et al. - 2022 - Design and Evaluation of Travel and Orientation Te.pdf;/home/nunoa/Zotero/storage/8GBDD6ZC/9756776.html}
}

@article{baniasadiChallengesPracticalConsiderations2020,
  title = {Challenges and {{Practical Considerations}} in {{Applying Virtual Reality}} in {{Medical Education}} and {{Treatment}}},
  author = {Baniasadi, Tayebeh and Ayyoubzadeh, Seyed Mohammad and Mohammadzadeh, Niloofar},
  year = {2020},
  month = may,
  journal = {Oman Medical Journal},
  volume = {35},
  number = {3},
  pages = {e125-e125},
  issn = {1999768X, 20705204},
  doi = {10.5001/omj.2020.43},
  urldate = {2023-10-12},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/HLM3FESU/Baniasadi et al. - 2020 - Challenges and Practical Considerations in Applyin.pdf}
}

@misc{BeingThereSubjective,
  title = {Being {{There}}: {{The Subjective Experience}} of {{Presence}}},
  urldate = {2023-11-15},
  howpublished = {https://www.researchgate.net/publication/200772979\_Being\_There\_The\_Subjective\_Experience\_of\_Presence},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/YNPLZ7W3/200772979_Being_There_The_Subjective_Experience_of_Presence.html}
}

@inproceedings{bellgardtUtilizingImmersiveVirtual2017,
  title = {Utilizing Immersive Virtual Reality in Everydaywork},
  booktitle = {2017 {{IEEE}} 3rd {{Workshop}} on {{Everyday Virtual Reality}} ({{WEVR}})},
  author = {Bellgardt, Martin and Pick, Sebastian and Zielasko, Daniel and Vierjahn, Tom and Weyers, Benjamin and Kuhlen, Torsten W.},
  year = {2017},
  month = mar,
  pages = {1--4},
  doi = {10.1109/WEVR.2017.7957708},
  urldate = {2023-09-26},
  abstract = {Applications of Virtual Reality (VR) have been repeatedly explored with the goal to improve the data analysis process of users from different application domains, such as architecture and simulation sciences. Unfortunately, making VR available in professional application scenarios or even using it on a regular basis has proven to be challenging. We argue that everyday usage environments, such as office spaces, have introduced constraints that critically affect the design of interaction concepts since well-established techniques might be difficult to use. In our opinion, it is crucial to understand the impact of usage scenarios on interaction design, to successfully develop VR applications for everyday use. To substantiate our claim, we define three distinct usage scenarios in this work that primarily differ in the amount of mobility they allow for. We outline each scenario's inherent constraints but also point out opportunities that may be used to design novel, well-suited interaction techniques for different everyday usage environments. In addition, we link each scenario to a concrete application example to clarify its relevance and show how it affects interaction design.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/LBJ75EEJ/Bellgardt et al. - 2017 - Utilizing immersive virtual reality in everydaywor.pdf;/home/nunoa/Zotero/storage/UXBJH4WW/7957708.html}
}

@book{boulangerImprovingMedicalSimulation2022,
  title = {Improving {{Medical Simulation Using Virtual Reality Augmented}} by {{Haptic Proxy}}},
  author = {Boulanger, Pierre and Wang, Thea and Hanzaki, Mahdi Rahmani and Boulanger, Pierre and Wang, Thea and Hanzaki, Mahdi Rahmani},
  year = {2022},
  month = nov,
  publisher = {{IntechOpen}},
  doi = {10.5772/intechopen.108330},
  urldate = {2023-10-12},
  abstract = {This chapter explores how the realism of haptic perception in virtual reality can be significantly enhanced with the help of the concept of haptic proxy. In haptic proxy, the position and orientation of physical objects are tracked in real-time and registered to their virtual counterparts. A compelling sense of tactile immersion can be achieved if the tracked objects have similar tactile properties to their virtual counterpart. A haptic proxy prototype was developed, and a pilot study was conducted to determine if the haptic proxy system is more credible than standard virtual reality. To test our prototype, we performed simple medical tasks such as moving a patient's arm and aiming a syringe to specific locations. Our results suggest that simulation using a haptic proxy system is more believable and user-friendly and can be extended to developing new generations of open surgery simulators.},
  isbn = {978-1-83768-433-5},
  langid = {english},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/3EZRBM4K/Boulanger et al. - 2022 - Improving Medical Simulation Using Virtual Reality.pdf}
}

@inproceedings{brollInteractingDistributedCollaborative1995,
  title = {Interacting in Distributed Collaborative Virtual Environments},
  booktitle = {Proceedings {{Virtual Reality Annual International Symposium}} '95},
  author = {Broll, W.},
  year = {1995},
  month = mar,
  pages = {148--155},
  doi = {10.1109/VRAIS.1995.512490},
  urldate = {2023-10-26},
  abstract = {Virtual reality toolkits and systems for computer supported cooperative work are often treated separately. However, combining them offers new possibilities for remote cooperative (or collaborative) group working. We review existing distribution models of virtual environments and propose a new method of concurrent interaction management. We examine the different types of communication layers, which are needed by collaborative virtual reality (VR) applications to achieve complex user interaction. Finally we propose a model for handling the different requirements of such applications, depending on the connection strategies used within a distributed VR system.},
  file = {/home/nunoa/Zotero/storage/4THBQPUG/Broll - 1995 - Interacting in distributed collaborative virtual e.pdf;/home/nunoa/Zotero/storage/ISTZ9LDU/512490.html}
}

@inproceedings{bujdosoDevelopingCognitiveProcesses2017,
  title = {Developing Cognitive Processes for Improving Inventive Thinking in System Development Using a Collaborative Virtual Reality System},
  booktitle = {2017 8th {{IEEE International Conference}} on {{Cognitive Infocommunications}} ({{CogInfoCom}})},
  author = {Bujdos{\'o}, Gy{\"o}ngyi and Novac, Ovidiu Constantin and Szimkovics, Tam{\'a}s},
  year = {2017},
  month = sep,
  pages = {000079--000084},
  doi = {10.1109/CogInfoCom.2017.8268220},
  urldate = {2023-10-12},
  abstract = {Computer Sciences (CS) require inventive thinking from computer science experts. CS education has to pay attention on that field of education, too. Universities have to educate programmers for the future where the CS experts need the most important skills of creativity, invention and innovation. These skills can be developed by using virtual reality. In this paper we deal with those techniques that we used for developing the students' skills on creativity and invention. We supposed that the ability to spatially organize learning materials and working documents and collaborative spaces may stimulate creative thinking. In this method we applied the MaxWhere, an immersive virtual reality (iVR) system that gives wide range of possibilities for cooperative and collaborative work. This iVR system provides many means that can be used for activating cognitive processes and improving inventive thinking. This method provided positive results.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/EFR5V4I6/Bujdosó et al. - 2017 - Developing cognitive processes for improving inven.pdf;/home/nunoa/Zotero/storage/B9DXBPLP/8268220.html}
}

@inproceedings{capalleraTrainingNursesVR2023,
  title = {Training Nurses in {{VR}}: Exploring Spatial Mapping and Free-Hand Interaction},
  shorttitle = {Training Nurses in {{VR}}},
  booktitle = {Proceedings of the 34th {{Conference}} on l'{{Interaction Humain-Machine}}},
  author = {Capallera, Marine and Angelini, Leonardo and Favre, Alexandre and Magnin, Fran{\c c}ois and De Vito Woods, Mariateresa and Abou Khaled, Omar and Mugellini, Elena},
  year = {2023},
  month = may,
  series = {{{IHM}} '23},
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3583961.3583980},
  urldate = {2023-10-12},
  abstract = {Virtual Reality can provide an immersive training environment in nursing education, in particular to experience situations that cannot be perceived in the reality. Through a user-centered design process with educators and students from a nursing school, we developed a VR application for asepsis training in a blood sampling scenario. The simulation allows to visualize the contamination risks during the procedures. We conducted three tests and continuously improved the application in order to increase the usability and the feeling of immersion. During the tests, we compared two interaction modalities, two visualizations of the hands and different degrees of immersion (teleportation, or mapping a real simulation room, with the possibility to walk and touch physical elements). These experiments allowed us to draw a conclusion on the level of immersion reached by the new virtual reality technologies and on their possible implication in the practical training of students.},
  isbn = {978-1-4503-9824-4},
  keywords = {asepsie,asepsis,contamination,formation,notion,Oculus Quest,R\'ealit\'e virtuelle,simulation,training,Virtual Reality},
  file = {/home/nunoa/Zotero/storage/TF4IG59Q/Capallera et al. - 2023 - Training nurses in VR exploring spatial mapping a.pdf}
}

@inproceedings{chenechalWhenGiantMeets2016,
  title = {When the Giant Meets the Ant an Asymmetric Approach for Collaborative and Concurrent Object Manipulation in a Multi-Scale Environment},
  booktitle = {2016 {{IEEE Third VR International Workshop}} on {{Collaborative Virtual Environments}} ({{3DCVE}})},
  author = {Chenechal, Morgan Le and Lacoche, Jeremy and Royan, Jerome and Duval, Thierry and Gouranton, Valerie and Arnaldi, Bruno},
  year = {2016},
  month = mar,
  pages = {18--22},
  doi = {10.1109/3DCVE.2016.7563562},
  urldate = {2023-11-08},
  abstract = {In this paper, we propose an innovative approach that enables two or more users to manipulate an object collaboratively. Our goal is to benefits from the wide variety of todays VR devices. Therefore, our solution is based on an asymmetric collaboration pattern at different scales in which users benefit from suited points of views and interaction techniques according to their device setups. Indeed, each user application is adapted thanks to plasticity mechanisms. Our system provides an efficient way to co-manipulate an object within irregular and narrow courses, taking advantages of asymmetric roles in synchronous collaboration. Moreover, its aims to provide a way to maximize the filling of the courses while the object moves on its path.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/4YVV2GRW/Chenechal et al. - 2016 - When the giant meets the ant an asymmetric approac.pdf;/home/nunoa/Zotero/storage/PUKZJTYN/7563562.html}
}

@article{chheangCollaborativeVirtualReality2021,
  title = {A Collaborative Virtual Reality Environment for Liver Surgery Planning},
  author = {Chheang, Vuthea and Saalfeld, Patrick and Joeres, Fabian and Boedecker, Christian and Huber, Tobias and Huettl, Florentine and Lang, Hauke and Preim, Bernhard and Hansen, Christian},
  year = {2021},
  month = oct,
  journal = {Computers \& Graphics},
  volume = {99},
  pages = {234--246},
  issn = {0097-8493},
  doi = {10.1016/j.cag.2021.07.009},
  urldate = {2023-10-12},
  abstract = {Surgical planning software is a key component in the treatment of tumor diseases. However, desktop-based systems provide only limited visualization and interaction opportunities. Moreover, collaborative planning among members of a surgical team is only possible to a limited extent. In this work, a collaborative virtual reality (VR) environment to assist liver surgeons in tumor surgery planning is presented. Our aim is to improve virtual resection planning between surgeons in a remote or co-located environment. The system allows surgeons to define and adjust virtual resections on patient-specific organ 3D surfaces and 2D image slices. Changes on both modalities are synchronized, which will enable surgeons to iterate and refine the resection surfaces quickly. In addition, a real-time risk map visualization is presented that displays safety margins around tumors. An evaluation performed by liver surgeons provides information on potential benefits, such as the possibility to visualize complex cases and assessing the safety-critical areas, applicability, and limitations for further improvement.},
  keywords = {Human-computer interaction,Medical visualization,notion,Surgical planning,Virtual reality},
  file = {/home/nunoa/Zotero/storage/6IMSU93D/Chheang et al. - 2021 - A collaborative virtual reality environment for li.pdf;/home/nunoa/Zotero/storage/UCU7SNFX/S0097849321001400.html}
}

@article{chungVRCATVRCollision2023,
  title = {{{VRCAT}}: {{VR}} Collision Alarming Technique for User Safety},
  shorttitle = {{{VRCAT}}},
  author = {Chung, SeungJeh and Lee, TaeHun and Jeong, BoRa and Jeong, JongWook and Kang, HyeongYeop},
  year = {2023},
  month = jul,
  journal = {The Visual Computer},
  volume = {39},
  number = {7},
  pages = {3145--3159},
  issn = {1432-2315},
  doi = {10.1007/s00371-022-02676-y},
  urldate = {2023-10-12},
  abstract = {The rapid advancement of virtual reality (VR) head-mounted displays (HMDs) has made it possible to experience immersive VR at home. However, such immersion inevitably disconnects users from reality and puts their safety at risk. We suggest a VR Collision Alarming Technique (VRCAT) to overcome this problem. The fundamental idea is to use an RGB camera to identify physical obstacles around VR users and warn them about potential collisions via the HMD. We built VRCAT on a smartphone platform that is readily available to the general public to reduce learning expenses and boost accessibility to our system. To validate whether the VRCAT improves user safety and is easy to use, we ran an evaluation test and an application test. The evaluation test reveals that VRCAT can be installed by novice users in about a minute. It also demonstrates that VRCAT can estimate the 3D positions of the user and obstacles with an error of 5\textendash 7~cm every 0.09~s. The application test conducted in real-world scenarios reveals that VRCAT successfully improved user safety without compromising the user's attention and performance on VR tasks.},
  langid = {english},
  keywords = {Collision alarming,Mobile application,notion,User safety,Virtual reality},
  file = {/home/nunoa/Zotero/storage/KYLJ3W9A/Chung et al. - 2023 - VRCAT VR collision alarming technique for user sa.pdf}
}

@inproceedings{coolsBlendingSpacesCrossReality2022,
  title = {Blending {{Spaces}}: {{Cross-Reality Interaction Techniques}} for {{Object Transitions Between Distinct Virtual}} and {{Augmented Realities}}},
  shorttitle = {Blending {{Spaces}}},
  booktitle = {2022 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},
  author = {Cools, Robbe and Esteves, Augusto and Simeone, Adalberto L.},
  year = {2022},
  month = oct,
  pages = {528--537},
  issn = {1554-7868},
  doi = {10.1109/ISMAR55827.2022.00069},
  urldate = {2023-10-19},
  abstract = {Cross-Reality (CR) involves interaction between different modalities and levels of immersion such as Virtual and Augmented Reality, as we explore in this paper. Whereas previous work assumed similarity between their respective Virtual and Augmented Environment (VE and AE), we explore the case in which VE and AE are distinct. This gives rise to novel and critical problems, such as how to visualise and interact with the other environment. In this context we investigate the fundamental interaction of transitioning an object across environments, to which we contribute five interaction techniques. Two are inspired by literature: Virtual Magic Lens and Binary Transition; while the other three are entirely novel: Auto Blended Space, Manual Blended Space - Button Transition and Manual Blended Space - Touch Transition. In a study evaluating the first four techniques, we found that participants (N=20) performed a CR object manipulation and transition task significantly faster using our Auto Blended Space technique. We then modified Manual Blended Space - Button Transition into Manual Blended Space - Touch Transition in response to these results, and reassessed the four techniques in a more complex object manipulation task (N=16). We found that this type of task was better suited to manual transition methods rather than automatic methods. Taken together, our final contribution are five blended space design factors, and timely Cross-Reality transition design guidelines.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/4DHNIERI/Cools et al. - 2022 - Blending Spaces Cross-Reality Interaction Techniq.pdf;/home/nunoa/Zotero/storage/PLEG6TFJ/9994912.html}
}

@article{cordeilImmersiveCollaborativeAnalysis2017,
  title = {Immersive {{Collaborative Analysis}} of {{Network Connectivity}}: {{CAVE-style}} or {{Head-Mounted Display}}?},
  shorttitle = {Immersive {{Collaborative Analysis}} of {{Network Connectivity}}},
  author = {Cordeil, Maxime and Dwyer, Tim and Klein, Karsten and Laha, Bireswar and Marriott, Kim and Thomas, Bruce H.},
  year = {2017},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {23},
  number = {1},
  pages = {441--450},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2016.2599107},
  urldate = {2023-11-01},
  abstract = {High-quality immersive display technologies are becoming mainstream with the release of head-mounted displays (HMDs) such as the Oculus Rift. These devices potentially represent an affordable alternative to the more traditional, centralised CAVE-style immersive environments. One driver for the development of CAVE-style immersive environments has been collaborative sense-making. Despite this, there has been little research on the effectiveness of collaborative visualisation in CAVE-style facilities, especially with respect to abstract data visualisation tasks. Indeed, very few studies have focused on the use of these displays to explore and analyse abstract data such as networks and there have been no formal user studies investigating collaborative visualisation of abstract data in immersive environments. In this paper we present the results of the first such study. It explores the relative merits of HMD and CAVE-style immersive environments for collaborative analysis of network connectivity, a common and important task involving abstract data. We find significant differences between the two conditions in task completion time and the physical movements of the participants within the space: participants using the HMD were faster while the CAVE2 condition introduced an asymmetry in movement between collaborators. Otherwise, affordances for collaborative data analysis offered by the low-cost HMD condition were not found to be different for accuracy and communication with the CAVE2. These results are notable, given that the latest HMDs will soon be accessible (in terms of cost and potentially ubiquity) to a massive audience.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/7HSRXAHL/7539620.html}
}

@article{cruz-neiraCAVEAudioVisual1992,
  title = {The {{CAVE}}: Audio Visual Experience Automatic Virtual Environment},
  shorttitle = {The {{CAVE}}},
  author = {{Cruz-Neira}, Carolina and Sandin, Daniel J. and DeFanti, Thomas A. and Kenyon, Robert V. and Hart, John C.},
  year = {1992},
  month = jun,
  journal = {Communications of the ACM},
  volume = {35},
  number = {6},
  pages = {64--72},
  issn = {0001-0782},
  doi = {10.1145/129888.129892},
  urldate = {2023-10-22},
  file = {/home/nunoa/Zotero/storage/P92XYB7J/Cruz-Neira et al. - 1992 - The CAVE audio visual experience automatic virtua.pdf}
}

@inproceedings{cutlerTwohandedDirectManipulation1997,
  title = {Two-Handed Direct Manipulation on the Responsive Workbench},
  booktitle = {Proceedings of the 1997 Symposium on {{Interactive 3D}} Graphics},
  author = {Cutler, Lawrence D. and Fr{\"o}hlich, Bernd and Hanrahan, Pat},
  year = {1997},
  month = apr,
  series = {{{I3D}} '97},
  pages = {107--114},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/253284.253315},
  urldate = {2023-11-09},
  isbn = {978-0-89791-884-8},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/HCKNVN9X/Cutler et al. - 1997 - Two-handed direct manipulation on the responsive w.pdf}
}

@inproceedings{deaconInvokeCollaborativeVirtual2023,
  title = {Invoke: {{A Collaborative Virtual Reality Tool}} for {{Spatial Audio Production Using Voice-Based Trajectory Sketching}}},
  shorttitle = {Invoke},
  booktitle = {Proceedings of the 18th {{International Audio Mostly Conference}}},
  author = {Deacon, Thomas and Barthet, Mathieu},
  year = {2023},
  month = oct,
  series = {{{AM}} '23},
  pages = {161--168},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3616195.3616211},
  urldate = {2023-10-19},
  abstract = {VR could transform creative engagement with spatial audio, given affordances for spatial visualisation and embodied interaction. But, issues exist addressing how to support collaboration for spatial audio production (SAP). Exploring this problem, we made a VR voice-based trajectory sketching tool, named Invoke, that allows two users to shape sonic ideas together. In this paper, thematic analysis is used to review two areas of a formative evaluation with expert users: (i) video analysis of VR interactions; and (ii) analysis of open questions about using the tool. Implications present new opportunities to explore co-creative VR tools for SAP.},
  isbn = {9798400708183},
  keywords = {Embodied Sound Design,notion,Sketching,Spatial Audio,Virtual Reality},
  file = {/home/nunoa/Zotero/storage/TGG2IQTH/Deacon and Barthet - 2023 - Invoke A Collaborative Virtual Reality Tool for S.pdf}
}

@incollection{dominguesCollaborative3DInteraction2011,
  title = {Collaborative {{3D Interaction}} in {{Virtual Environments}}: A {{Workflow-based Approach}}},
  shorttitle = {Collaborative {{3D Interaction}} in {{Virtual Environments}}},
  booktitle = {Virtual {{Reality}}},
  author = {Domingues, Christophe and Davesne, Frederic and Mallem, Malik and Otmane, Samir},
  year = {2011},
  chapter = {3},
  month = jan,
  publisher = {{IntechOpen}},
  doi = {10.5772/13013},
  urldate = {2023-10-26},
  abstract = {Open access peer-reviewed chapter},
  isbn = {978-953-307-518-1},
  langid = {english},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/MQL77HI7/Domingues et al. - 2011 - Collaborative 3D Interaction in Virtual Environmen.pdf}
}

@inproceedings{donalekImmersiveCollaborativeData2014,
  title = {Immersive and Collaborative Data Visualization Using Virtual Reality Platforms},
  booktitle = {2014 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Donalek, Ciro and Djorgovski, S. G. and Cioc, Alex and Wang, Anwell and Zhang, Jerry and Lawler, Elizabeth and Yeh, Stacy and Mahabal, Ashish and Graham, Matthew and Drake, Andrew and Davidoff, Scott and Norris, Jeffrey S. and Longo, Giuseppe},
  year = {2014},
  month = oct,
  pages = {609--614},
  doi = {10.1109/BigData.2014.7004282},
  urldate = {2023-10-25},
  abstract = {Effective data visualization is a key part of the discovery process in the era of ``big data''. It is the bridge between the quantitative content of the data and human intuition, and thus an essential component of the scientific path from data into knowledge and understanding. Visualization is also essential in the data mining process, directing the choice of the applicable algorithms, and in helping to identify and remove bad data from the analysis. However, a high complexity or a high dimensionality of modern data sets represents a critical obstacle. How do we visualize interesting structures and patterns that may exist in hyper-dimensional data spaces? A better understanding of how we can perceive and interact with multidimensional information poses some deep questions in the field of cognition technology and human-computer interaction. To this effect, we are exploring the use of immersive virtual reality platforms for scientific data visualization, both as software and inexpensive commodity hardware. These potentially powerful and innovative tools for multi-dimensional data visualization can also provide an easy and natural path to a collaborative data visualization and exploration, where scientists can interact with their data and their colleagues in the same visual space. Immersion provides benefits beyond the traditional ``desktop'' visualization tools: it leads to a demonstrably better perception of a datascape geometry, more intuitive data understanding, and a better retention of the perceived relationships in the data.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/PXV6SZ34/Donalek et al. - 2014 - Immersive and collaborative data visualization usi.pdf;/home/nunoa/Zotero/storage/6T6XS3RS/7004282.html}
}

@misc{DoseRealityProceedings,
  title = {A {{Dose}} of {{Reality}} | {{Proceedings}} of the 33rd {{Annual ACM Conference}} on {{Human Factors}} in {{Computing Systems}}},
  urldate = {2023-11-01},
  howpublished = {https://dl.acm.org/doi/10.1145/2702123.2702382},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/3T8FUF4J/2702123.html}
}

@inproceedings{duvalSkeweR3DInteraction2006,
  title = {{{SkeweR}}: A {{3D Interaction Technique}} for 2-{{User Collaborative Manipulation}} of {{Objects}} in {{Virtual Environments}}},
  shorttitle = {{{SkeweR}}},
  booktitle = {{{3D User Interfaces}} ({{3DUI}}'06)},
  author = {Duval, T. and Lecuyer, A. and Thomas, S.},
  year = {2006},
  month = mar,
  pages = {69--72},
  doi = {10.1109/VR.2006.119},
  urldate = {2023-11-02},
  abstract = {This paper describes a novel 3D interaction technique called the "SkeweR", dedicated to the 2-user collaborative manipulation of objects in virtual environments. This technique enables two users to move simultaneously the same virtual object in 3D. For this aim, each user manipulates the object by one crushing point, like handling the extremity of a skewer. The SkeweR uses only translation information from the users' motions to change both the position and orientation of the manipulated object. By using more crushing points, this technique could easily be extended to 3 or more users. Thus, the SkeweR technique could be used to improve the collaborative manipulation of objects in numerous applications of Virtual Reality, such as: virtual prototyping, maintenance and training simulations, architectural mock-up reviews, etc.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/EJBRVR37/Duval et al. - 2006 - SkeweR a 3D Interaction Technique for 2-User Coll.pdf;/home/nunoa/Zotero/storage/4LCWF6KR/1647510.html}
}

@inproceedings{elvezioCollaborativeVirtualReality2018,
  title = {Collaborative {{Virtual Reality}} for {{Low-Latency Interaction}}},
  booktitle = {Adjunct {{Proceedings}} of the 31st {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Elvezio, Carmine and Ling, Frank and Liu, Jen-Shuo and Feiner, Steven},
  year = {2018},
  month = oct,
  series = {{{UIST}} '18 {{Adjunct}}},
  pages = {179--181},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3266037.3271643},
  urldate = {2023-10-12},
  abstract = {In collaborative virtual environments, users must often perform tasks requiring coordinated action between multiple parties. Some cases are symmetric, in which users work together on equal footing, while others are asymmetric, in which one user may have more experience or capabilities than another (e.g., one may guide another in completing a task). We present a multi-user virtual reality system that supports interactions of both these types. Two collaborating users, whether co-located or remote, simultaneously manipulate the same virtual objects in a physics simulation, in tasks that require low latency networking to perform successfully. We are currently applying this approach to motor rehabilitation, in which a therapist and patient work together.},
  isbn = {978-1-4503-5949-8},
  keywords = {collaboration,games,notion,rehabilitation,virtual reality},
  file = {/home/nunoa/Zotero/storage/CU6QCI8K/Elvezio et al. - 2018 - Collaborative Virtual Reality for Low-Latency Inte.pdf}
}

@article{ericksonSocialTranslucenceApproach2000,
  title = {Social Translucence: An Approach to Designing Systems That Support Social Processes},
  shorttitle = {Social Translucence},
  author = {Erickson, Thomas and Kellogg, Wendy A.},
  year = {2000},
  month = mar,
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {7},
  number = {1},
  pages = {59--83},
  issn = {1073-0516},
  doi = {10.1145/344949.345004},
  urldate = {2023-10-21},
  abstract = {We are interested in desiging systems that support communication and collaboration among large groups of people over computing networks. We begin by asking what properties of the physical world support graceful human-human communication in face-to-face situations, and argue that it is possible to design digital systems that support coherent behavior by making participants and their activites visible to one another. We call such systems ``socially translucent systems'' and suggest that they have three characteristics\textemdash visbility, awareness, and accountability\textemdash which enable people to draw upon their experience and expertise to structure their interactions with one another. To motivate and focus our ideas we develop a vision of knowledge communities, conversationally based systems that support the creation, management and reuse of knowledge in a social context. We describe our experience in designing and deploying one layer of functionality for knowledge communities, embodied in a working system called ``Barbie'' and discuss research issues raised by a socially translucent approach to design.},
  keywords = {CMC,CMI,computer-mediated communication,CSCW,notion,social computing,social navigation,social visualization,visualization},
  file = {/home/nunoa/Zotero/storage/TGB8WD2S/Erickson and Kellogg - 2000 - Social translucence an approach to designing syst.pdf}
}

@article{fanRedirectedWalkingExploring2023,
  title = {Redirected {{Walking}} for {{Exploring Immersive Virtual Spaces With HMD}}: {{A Comprehensive Review}} and {{Recent Advances}}},
  shorttitle = {Redirected {{Walking}} for {{Exploring Immersive Virtual Spaces With HMD}}},
  author = {Fan, Linwei and Li, Huiyu and Shi, Miaowen},
  year = {2023},
  month = oct,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {29},
  number = {10},
  pages = {4104--4123},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2022.3179269},
  urldate = {2023-10-12},
  abstract = {Real walking techniques can provide the user with a more natural, highly immersive walking experience compared to the experience of other locomotion techniques. In contrast to the direct mapping between the virtual space and an equal-sized physical space that can be simply realized, the nonequivalent mapping that enables the user to explore a large virtual space by real walking within a confined physical space is complex. To address this issue, the redirected walking (RDW) technique is proposed by many works to adjust the user's virtual and physical movements based on some redirection manipulations. In this manner, subtle or overt motion deviations can be injected between the user's virtual and physical movements, allowing the user to undertake real walking in large virtual spaces by using different redirection controller methods. In this paper, we present a brief review to describe major concepts and methodologies in the field of redirected walking. First, we provide the fundamentals and basic criteria of RDW, and then we describe the redirection manipulations that can be applied to adjust the user's movements during virtual exploration. Furthermore, we clarify the redirection controller methods that properly adopt strategies for combining different redirection manipulations and present a classification of these methods by several categories. Finally, we summarize several experimental metrics to evaluate the performance of redirection controller methods and discuss current challenges and future work. Our study systematically classifies the relevant theories, concepts, and methods of RDW, and provides assistance to the newcomers in understanding and implementing the RDW technique.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/48ZWAH5W/Fan et al. - 2023 - Redirected Walking for Exploring Immersive Virtual.pdf;/home/nunoa/Zotero/storage/SZYCGYX8/9785918.html}
}

@inproceedings{feickInvestigatingNoticeableHand2023,
  title = {Investigating {{Noticeable Hand Redirection}} in {{Virtual Reality}} Using {{Physiological}} and {{Interaction Data}}},
  booktitle = {2023 {{IEEE Conference Virtual Reality}} and {{3D User Interfaces}} ({{VR}})},
  author = {Feick, Martin and Regitz, Kora P. and Tang, Anthony and Jungbluth, Tobias and Rekrut, Maurice and Kr{\"u}ger, Antonio},
  year = {2023},
  month = mar,
  pages = {194--204},
  issn = {2642-5254},
  doi = {10.1109/VR55154.2023.00035},
  urldate = {2023-10-12},
  abstract = {Hand redirection is effective so long as the introduced offsets are not noticeably disruptive to users. In this work we investigate the use of physiological and interaction data to detect movement discrepancies between a user's real and virtual hand, pushing towards a novel approach to identify discrepancies which are too large and therefore can be noticed. We ran a study with 22 participants, collecting EEG, ECG, EDA, RSP, and interaction data. Our results suggest that EEG and interaction data can be reliably used to detect visuo-motor discrepancies, whereas ECG and RSP seem to suffer from inconsistencies. Our findings also show that participants quickly adapt to large discrepancies, and that they constantly attempt to establish a stable mental model of their environment. Together, these findings suggest that there is no absolute threshold for possible non-detectable discrepancies; instead, it depends primarily on participants' most recent experience with this kind of interaction.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/MKHY3CU5/Feick et al. - 2023 - Investigating Noticeable Hand Redirection in Virtu.pdf;/home/nunoa/Zotero/storage/EVD7BCP2/10108468.html}
}

@article{fristonConsensusBasedNetworking2022,
  title = {Consensus {{Based Networking}} of {{Distributed Virtual Environments}}},
  author = {Friston, Sebastian and Griffith, Elias and Swapp, David and Julier, Simon and Irondi, Caleb and Jjunju, Fred and Ward, Ryan and Marshall, Alan and Steed, Anthony},
  year = {2022},
  month = sep,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {28},
  number = {9},
  pages = {3138--3153},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2021.3052580},
  urldate = {2023-10-25},
  abstract = {Distributed virtual environments (DVEs) are challenging to create as the goals of consistency and responsiveness become contradictory under increasing latency. DVEs have been considered as both distributed transactional databases and force-reflection systems. Both are good approaches, but they do have drawbacks. Transactional systems do not support Level 3 (L3) collaboration: manipulating the same degree-of-freedom at the same time. Force-reflection requires a client-server architecture and stabilisation techniques. With Consensus Based Networking (CBN), we suggest DVEs be considered as a distributed data-fusion problem. Many simulations run in parallel and exchange their states, with remote states integrated with continous authority. Over time the exchanges average out local differences, performing a distribued-average of a consistent, shared state. CBN aims to build simulations that are highly responsive, but consistent enough for use cases such as the piano-movers problem. CBN's support for heterogeneous nodes can transparently couple different input methods, avoid the requirement of determinism, and provide more options for personal control over the shared experience. Our work is early, however we demonstrate many successes, including L3 collaboration in room-scale VR, 1000's of interacting objects, complex configurations such as stacking, and transparent coupling of haptic devices. These have been shown before, but each with a different technique; CBN supports them all within a single, unified system.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/5NJNIHWC/Friston et al. - 2022 - Consensus Based Networking of Distributed Virtual .pdf;/home/nunoa/Zotero/storage/GYNIQTJT/9328611.html}
}

@article{fristonConsensusBasedNetworking2022a,
  title = {Consensus {{Based Networking}} of {{Distributed Virtual Environments}}},
  author = {Friston, Sebastian and Griffith, Elias and Swapp, David and Julier, Simon and Irondi, Caleb and Jjunju, Fred and Ward, Ryan and Marshall, Alan and Steed, Anthony},
  year = {2022},
  month = sep,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {28},
  number = {9},
  pages = {3138--3153},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2021.3052580},
  urldate = {2023-10-25},
  abstract = {Distributed virtual environments (DVEs) are challenging to create as the goals of consistency and responsiveness become contradictory under increasing latency. DVEs have been considered as both distributed transactional databases and force-reflection systems. Both are good approaches, but they do have drawbacks. Transactional systems do not support Level 3 (L3) collaboration: manipulating the same degree-of-freedom at the same time. Force-reflection requires a client-server architecture and stabilisation techniques. With Consensus Based Networking (CBN), we suggest DVEs be considered as a distributed data-fusion problem. Many simulations run in parallel and exchange their states, with remote states integrated with continous authority. Over time the exchanges average out local differences, performing a distribued-average of a consistent, shared state. CBN aims to build simulations that are highly responsive, but consistent enough for use cases such as the piano-movers problem. CBN's support for heterogeneous nodes can transparently couple different input methods, avoid the requirement of determinism, and provide more options for personal control over the shared experience. Our work is early, however we demonstrate many successes, including L3 collaboration in room-scale VR, 1000's of interacting objects, complex configurations such as stacking, and transparent coupling of haptic devices. These have been shown before, but each with a different technique; CBN supports them all within a single, unified system.},
  file = {/home/nunoa/Zotero/storage/VRGVBI7G/Friston et al. - 2022 - Consensus Based Networking of Distributed Virtual .pdf;/home/nunoa/Zotero/storage/EEWPFBGF/9328611.html}
}

@article{fujitaHumanWorkspaceInteractionPrior2023,
  title = {Human-{{Workspace Interaction}}: Prior Research Efforts and Future Challenges for Supporting Knowledge Workers},
  shorttitle = {Human-{{Workspace Interaction}}},
  author = {Fujita, Kazuyuki and Takashima, Kazuki and Itoh, Yuichi and Kitamura, Yoshifumi},
  year = {2023},
  month = aug,
  journal = {Quality and User Experience},
  volume = {8},
  number = {1},
  pages = {7},
  issn = {2366-0147},
  doi = {10.1007/s41233-023-00060-9},
  urldate = {2023-10-12},
  abstract = {Research efforts have previously explored various components of physical/virtual workspaces that adaptively interact with knowledge workers in order to support them in their work. In this paper, we propose an encompassing framework for these efforts, which we refer to as Human-Workspace Interaction (HWI), with the goal of increasing awareness and understanding of the research area and encouraging its further development. Specifically, we present a taxonomy of HWI focusing on the types of components, research approaches, interaction targets and objectives, and then review the prior research efforts over the past two decades based on these criteria. Finally, we discuss challenges to further advance the development of HWI and future prospects, taking into account the impact of the societal changes caused by the COVID-19 pandemic.},
  langid = {english},
  keywords = {Interactive workspaces,notion,Offices,Robotic furniture,Virtual workspaces},
  file = {/home/nunoa/Zotero/storage/YBCPH9HA/Fujita et al. - 2023 - Human-Workspace Interaction prior research effort.pdf}
}

@incollection{furhtDesktopVirtualReality2008,
  title = {Desktop {{Virtual Reality}}},
  booktitle = {Encyclopedia of {{Multimedia}}},
  editor = {Furht, Borko},
  year = {2008},
  pages = {150--151},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-0-387-78414-4_294},
  urldate = {2023-11-01},
  isbn = {978-0-387-78414-4},
  langid = {english},
  file = {/home/nunoa/Zotero/storage/M2JXQH9T/Furht - 2008 - Desktop Virtual Reality.pdf}
}

@inproceedings{gauglitzWorldstabilizedAnnotationsVirtual2014,
  title = {World-Stabilized Annotations and Virtual Scene Navigation for Remote Collaboration},
  booktitle = {Proceedings of the 27th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},
  author = {Gauglitz, Steffen and Nuernberger, Benjamin and Turk, Matthew and H{\"o}llerer, Tobias},
  year = {2014},
  month = oct,
  series = {{{UIST}} '14},
  pages = {449--459},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2642918.2647372},
  urldate = {2023-10-22},
  abstract = {We present a system that supports an augmented shared visual space for live mobile remote collaboration on physical tasks. The remote user can explore the scene independently of the local user's current camera position and can communicate via spatial annotations that are immediately visible to the local user in augmented reality. Our system operates on off-the-shelf hardware and uses real-time visual tracking and modeling, thus not requiring any preparation or instrumentation of the environment. It creates a synergy between video conferencing and remote scene exploration under a unique coherent interface. To evaluate the collaboration with our system, we conducted an extensive outdoor user study with 60 participants comparing our system with two baseline interfaces. Our results indicate an overwhelming user preference (80\%) for our system, a high level of usability, as well as performance benefits compared with one of the two baselines.},
  isbn = {978-1-4503-3069-5},
  keywords = {augmented reality,CSCW,telepresence,video-mediated communication}
}

@inproceedings{grandiDesignEvaluationHandheldbased2017,
  title = {Design and {{Evaluation}} of a {{Handheld-based 3D User Interface}} for {{Collaborative Object Manipulation}}},
  booktitle = {Proceedings of the 2017 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Grandi, Jer{\^o}nimo Gustavo and Debarba, Henrique Galvan and Nedel, Luciana and Maciel, Anderson},
  year = {2017},
  month = may,
  series = {{{CHI}} '17},
  pages = {5881--5891},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3025453.3025935},
  urldate = {2023-11-08},
  abstract = {Object manipulation in 3D virtual environments demands a combined coordination of rotations, translations and scales, as well as the camera control to change the user's viewpoint. Then, for many manipulation tasks, it would be advantageous to share the interaction complexity among team members. In this paper we propose a novel 3D manipulation interface based on a collaborative action coordination approach. Our technique explores a smartphone -- the touchscreen and inertial sensors -- as input interface, enabling several users to collaboratively manipulate the same virtual object with their own devices. We first assessed our interface design on a docking and an obstacle crossing tasks with teams of two users. Then, we conducted a study with 60 users to understand the influence of group size in collaborative 3D manipulation. We evaluated teams in combinations of one, two, three and four participants. Experimental results show that teamwork increases accuracy when compared with a single user. The accuracy increase is correlated with the number of individuals in the team and their work division strategy.},
  isbn = {978-1-4503-4655-9},
  keywords = {3D user interfaces,collaborative manipulation,notion,user studies},
  file = {/home/nunoa/Zotero/storage/LEDNW8E6/Grandi et al. - 2017 - Design and Evaluation of a Handheld-based 3D User .pdf}
}

@inproceedings{gronbaekPartiallyBlendedRealities2023,
  title = {Partially {{Blended Realities}}: {{Aligning Dissimilar Spaces}} for {{Distributed Mixed Reality Meetings}}},
  shorttitle = {Partially {{Blended Realities}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Gr{\o}nb{\ae}k, Jens Emil Sloth and Pfeuffer, Ken and Velloso, Eduardo and Astrup, Morten and Pedersen, Melanie Isabel S{\o}nderk{\ae}r and Kj{\ae}r, Martin and Leiva, Germ{\'a}n and Gellersen, Hans},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--16},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3544548.3581515},
  urldate = {2023-10-12},
  abstract = {Mixed Reality allows for distributed meetings where people's local physical spaces are virtually aligned into blended interaction spaces. In many cases, people's physical rooms are dissimilar, making it challenging to design a coherent blended space. We introduce the concept of Partially Blended Realities (PBR) \textemdash{} using Mixed Reality to support remote collaborators in partially aligning their physical spaces. As physical surfaces are central in collaborative work, PBR supports users in transitioning between different configurations of tables and whiteboard surfaces. In this paper, we 1) describe the design space of PBR, 2) present RealityBlender to explore interaction techniques for how users may configure and transition between blended spaces, and 3) provide insights from a study on how users experience transitions in a remote collaboration task. With this work, we demonstrate new potential for using partial solutions to tackle the alignment problem of dissimilar spaces in distributed Mixed Reality meetings.},
  isbn = {978-1-4503-9421-5},
  keywords = {augmented and virtual reality,blended realities,mixed reality,notion,proxemic transitions,remote collaboration},
  file = {/home/nunoa/Zotero/storage/IQK58YIV/Grønbæk et al. - 2023 - Partially Blended Realities Aligning Dissimilar S.pdf}
}

@inproceedings{grotenEfficiencyAnalysisCollaborative2009,
  title = {Efficiency Analysis in a Collaborative Task with Reciprocal Haptic Feedback},
  booktitle = {2009 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Groten, Raphaela and Feth, Daniela and Peer, Angelika and Buss, Martin and Klatzky, Roberta},
  year = {2009},
  month = oct,
  pages = {461--466},
  issn = {2153-0866},
  doi = {10.1109/IROS.2009.5354612},
  urldate = {2023-11-12},
  abstract = {Although it is reported in the literature that haptic feedback leads to improved performance in kinesthetic collaborative tasks, it has not been investigated so far whether this advantage is accompanied by a higher physical workload. This paper is an initial effort to examine efficiency in haptic interaction: We relate physical effort to a performance outcome in a virtual pursuit tracking task. An experimental study is conducted to compare efficiency in a collaborative mutual haptic feedback condition to three control conditions, where participants either acted alone or collaboratively without haptic feedback from the partner. Results show that reciprocal haptic feedback does not improve efficiency, although participants' performance was generally improved when doing the task with a partner, relative to executing it alone. This is due to the greater effort associated with physical connection between partners. However, the effort is more fairly distributed between partners when haptic feedback from the partner is provided. Haptic feedback may be more efficient when the amount of necessary communication between partners increases compared to the task studied here.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/BPGQFG45/5354612.html}
}

@book{GroupProblemSolving2011,
  title = {Group {{Problem Solving}}},
  year = {Sun, 02/13/2011 - 12:00},
  urldate = {2023-11-17},
  isbn = {978-0-691-14791-8},
  langid = {english},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/KK9G3RMY/group-problem-solving.html}
}

@article{gutwinDescriptiveFrameworkWorkspace2002,
  title = {A {{Descriptive Framework}} of {{Workspace Awareness}} for {{Real-Time Groupware}}},
  author = {Gutwin, Carl and Greenberg, Saul},
  year = {2002},
  month = sep,
  journal = {Computer Supported Cooperative Work (CSCW)},
  volume = {11},
  number = {3},
  pages = {411--446},
  issn = {1573-7551},
  doi = {10.1023/A:1021271517844},
  urldate = {2023-10-20},
  abstract = {Supporting awareness of others is an idea that holds promise forimproving the usability of real-time distributed groupware.However, there is little principled information available aboutawareness that can be used by groupware designers. In thisarticle, we develop a descriptive theory of awareness for thepurpose of aiding groupware design, focusing on one kind of groupawareness called workspace awareness. We focus on how smallgroups perform generation and execution tasks in medium-sizedshared workspaces \textendash{} tasks where group members frequently shiftbetween individual and shared activities during the work session.We have built a three-part framework that examines the concept ofworkspace awareness and that helps designers understand theconcept for purposes of designing awareness support in groupware.The framework sets out elements of knowledge that make upworkspace awareness, perceptual mechanisms used to maintainawareness, and the ways that people use workspace awareness incollaboration. The framework also organizes previous research onawareness and extends it to provide designers with a vocabularyand a set of ground rules for analysing work situations, forcomparing awareness devices, and for explaining evaluationresults. The basic structure of the theory can be used todescribe other kinds of awareness that are important to theusability of groupware.},
  langid = {english},
  keywords = {awareness,groupware design,groupware usability,notion,real-time distributed groupware,shared workspaces,situation awareness,workspace awareness},
  file = {/home/nunoa/Zotero/storage/R3FPDHKX/Gutwin and Greenberg - 2002 - A Descriptive Framework of Workspace Awareness for.pdf}
}

@inproceedings{habgoodHCILessonsPlayStation2017,
  title = {{{HCI Lessons From PlayStation VR}}},
  author = {Habgood, Jacob and Wilson, David and Moore, David and Alapont, Sergio},
  year = {2017},
  month = oct,
  pages = {125--135},
  doi = {10.1145/3130859.3131437},
  abstract = {PlayStation VR has quickly built up a significant user-base of over a million headsets and its own ecosystem of games across a variety of genres. These games form part of a rapidly evolving testing ground for design solutions which can usefully inform HCI design for virtual reality. This paper reviews every PlayStation VR title released in the first three months of its lifecycle in order to identify emerging themes for locomotion. These themes are discussed with respect to the lessons learned as part of the on-going development of an Environmental Narrative game for PlayStation VR as part of the Horizon 2020 REVEAL project.},
  isbn = {978-1-4503-5111-9},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/TSQAQQHV/Habgood et al. - 2017 - HCI Lessons From PlayStation VR.pdf}
}

@article{hinckleyTwohandedVirtualManipulation1998,
  title = {Two-Handed Virtual Manipulation},
  author = {Hinckley, Ken and Pausch, Randy and Proffitt, Dennis and Kassell, Neal F.},
  year = {1998},
  month = sep,
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {5},
  number = {3},
  pages = {260--302},
  issn = {1073-0516},
  doi = {10.1145/292834.292849},
  urldate = {2023-11-09},
  abstract = {We discuss a two-handed user interface designed to support three-dimesional neurosurgical visualization. By itself, this system is a ``point design,'' an example of an advanced user interface technique. In this work, we argue that in order to understand why interaction techniques do or do not work, and to suggest possibilities for new techniques, it is important to move beyond point design and to introduce careful scientific measurement of human behavioral principles. In particular, we argue that the common-sense viewpoint that ``two hands save time by working in parallel'' may not always be an effective way to think about two-handed interface design because the hands do not necessarily work in parallel (there is a structure to two-handed manipulation) and because two hands do more than just save time over one hand (two hands provide the user with more information and can structure how the user thinks about a task). To support these claims, we present an interface design developed in collaboration with neurosurgeons which has undergone extensive informal usability testing, as well as a pair of formal experimental studies which investigate behavioral aspects of two-handed virtual object manipulation. Our hope is that this discussion will help others to apply the lessons in our neurosurgery application to future two-handed user interface designs.},
  keywords = {bimanual asymmetry,haptic input,input devices,notion,three-dimensional interaction,two-handed interaction,virtual manipulation},
  file = {/home/nunoa/Zotero/storage/9BDRSDFA/Hinckley et al. - 1998 - Two-handed virtual manipulation.pdf}
}

@article{huangAugmented3DHands2018,
  title = {Augmented {{3D}} Hands: A Gesture-Based Mixed Reality System for Distributed Collaboration},
  shorttitle = {Augmented {{3D}} Hands},
  author = {Huang, Weidong and Alem, Leila and Tecchia, Franco and Duh, Henry Been-Lirn},
  year = {2018},
  month = jun,
  journal = {Journal on Multimodal User Interfaces},
  volume = {12},
  number = {2},
  pages = {77--89},
  issn = {1783-8738},
  doi = {10.1007/s12193-017-0250-2},
  urldate = {2023-10-22},
  abstract = {Distributed collaborations between two or more participants on a task involving tangible artifacts (e.g., a machine, a patient, a tool) have become increasingly common in recent years due to rapid development in information and communication technologies. In this paper we focus on a specific type of remote-collaboration system where a remote helper guides a local worker using audio communication and hand gestures to perform a repair or a maintenance task. An established ICT approach to supporting this type of collaboration is to provide a shared visual space and some forms of remote gesture. The shared space typically consists of a video capture of the remote workspace which is then displayed on a 2D screen. However, this type of approach has its limitations. Firstly, it does not provide the helper with sufficient understanding of the spatial relationships between objects in the remote workspace. Secondly, it does not allow the helper to gesture in 3D. In an attempt to address these issues, we propose a Mixed Reality multimodal system that improves on previous 2D systems by introducing 3D real-time capturing and rendering of both the remote workspace and the helping hands and by creating a 3D shared visual space as a result of co-locating the remote workspace with the helping hands. In this system, we explore the possibility of increasing the feeling of immersion and co-presence by using head tracking, stereoscopic rendering, inter-occlusion handling and virtual shadowing. In this paper, we introduce HandsIn3D, a system that has been developed for the purpose of the proof of concept. We also present the results of experiments to verify the feasibility of our approach.},
  langid = {english},
  keywords = {3D capture and rendering,Hand gesture,Microsoft Kinect,Mixed-reality,notion,Remote collaboration,Shared visual space,Tele-presence},
  file = {/home/nunoa/Zotero/storage/RBN2HWY8/Huang et al. - 2018 - Augmented 3D hands a gesture-based mixed reality .pdf}
}

@inproceedings{hudakAdvancedUserInteraction2020,
  title = {Advanced {{User Interaction}} for {{Web-based Collaborative Virtual Reality}}},
  booktitle = {2020 11th {{IEEE International Conference}} on {{Cognitive Infocommunications}} ({{CogInfoCom}})},
  author = {Hud{\'a}k, Mari{\'a}n and Kore{\v c}ko, {\v S}tefan and Sobota, Branislav},
  year = {2020},
  month = sep,
  pages = {000343--000348},
  issn = {2380-7350},
  doi = {10.1109/CogInfoCom50765.2020.9237899},
  urldate = {2023-10-12},
  abstract = {Despite their support in contemporary specifications for web-based virtual, augmented and mixed reality, such as WebXR, user interaction is limited in several ways in corresponding web applications. This paper introduces software components providing solution for two of these limitations: a lack of six degrees of freedom motion capturing for smartphone-based VR headsets and no hand gesture support for MS HoloLens. The components have been developed for LIRKIS Global - Collaborative Virtual Environments, but can be used for any web applications utilizing the A-Frame framework. The paper describes both components and evaluates their performance.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/T9LXWAIV/Hudák et al. - 2020 - Advanced User Interaction for Web-based Collaborat.pdf;/home/nunoa/Zotero/storage/SIBR8D9B/9237899.html}
}

@misc{IEEEStandardsAssociation,
  title = {{{IEEE Standards Association}}},
  journal = {IEEE Standards Association},
  urldate = {2023-10-25},
  howpublished = {https://standards.ieee.org},
  langid = {english},
  file = {/home/nunoa/Zotero/storage/ESGDVL96/3746.html}
}

@misc{ImmersiveWebDeveloper,
  title = {Immersive {{Web Developer Home}}},
  urldate = {2023-10-19},
  abstract = {Get started building XR applications through the Web with the WebXR Device API, demos, docs, samples and more.},
  howpublished = {https://immersiveweb.dev/},
  langid = {english},
  file = {/home/nunoa/Zotero/storage/TT6BH3BF/immersiveweb.dev.html}
}

@article{kaufmannConstruct3DVirtualReality2000,
  title = {{{Construct3D}}: {{A Virtual Reality Application}} for {{Mathematics}} and {{Geometry Education}}},
  shorttitle = {{{Construct3D}}},
  author = {Kaufmann, Hannes and Schmalstieg, Dieter and Wagner, Michael},
  year = {2000},
  month = dec,
  journal = {Education and Information Technologies},
  volume = {5},
  number = {4},
  pages = {263--276},
  issn = {1573-7608},
  doi = {10.1023/A:1012049406877},
  urldate = {2023-10-25},
  abstract = {Construct3D is a three dimensional geometric construction tool based on the collaborative augmented reality system `Studierstube'. Our setup uses a stereoscopic head mounted display (HMD) and the Personal Interaction Panel (PIP) - a two-handed 3D interaction tool that simplifies 3D model interaction. Means of application in mathematics and geometry education at high school as well as university level are being discussed. A pilot study summarizes the strengths and possible extensions of our system. Anecdotal evidence supports our claim that the use of Construct3D is easy to learn and encourages experimentation with geometric constructions.},
  langid = {english},
  keywords = {3D modelling.,enhancing spatial abilities,geometry education,user-interface design,virtual reality},
  file = {/home/nunoa/Zotero/storage/JC5J5TH4/Kaufmann et al. - 2000 - Construct3D A Virtual Reality Application for Mat.pdf}
}

@inproceedings{lacocheCollaboratorsAwarenessUser2017,
  title = {Collaborators Awareness for User Cohabitation in Co-Located Collaborative Virtual Environments},
  booktitle = {Proceedings of the 23rd {{ACM Symposium}} on {{Virtual Reality Software}} and {{Technology}}},
  author = {Lacoche, J{\'e}r{\'e}my and Pallamin, Nico and Boggini, Thomas and Royan, J{\'e}r{\^o}me},
  year = {2017},
  month = nov,
  series = {{{VRST}} '17},
  pages = {1--9},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3139131.3139142},
  urldate = {2023-10-19},
  abstract = {In a co-located collaborative virtual environment, multiple users share the same physical tracked space and the same virtual workspace. When the virtual workspace is larger than the real workspace, navigation interaction techniques must be deployed to let the users explore the entire virtual environment. When a user navigates in the virtual space while remaining static in the real space, his/her position in the physical workspace and in the virtual workspace are no longer the same. Thus, in the context where each user is immersed in the virtual environment with a Head-Mounted-Display, a user can still perceive where his/her collaborators are in the virtual environment but not where they are in real world. In this paper, we propose and compare three methods to warn users about the position of collaborators in the shared physical workspace to ensure a proper cohabitation and safety of the collaborators. The frst one is based on a virtual grid shaped as a cylinder, the second one is based on a ghost representation of the user and the last one displays the physical safe-navigation space on the foor of the virtual environment. We conducted a user-study with two users wearing a Head-Mounted-Display in the context of a collaborative First-Person-Shooter game. Our three methods were compared with a condition where the physical tracked space was separated into two zones, one per user, to evaluate the impact of each condition on safety, displacement freedom and global satisfaction of users. Results suggest that the ghost avatar and the cylinder grid can be good alternatives to the separation of the tracked space.},
  isbn = {978-1-4503-5548-3},
  keywords = {collaborative virtual environment,notion,virtual reality},
  file = {/home/nunoa/Zotero/storage/85SKZZ47/Lacoche et al. - 2017 - Collaborators awareness for user cohabitation in c.pdf}
}

@misc{langIntroductionPositionalTracking2013,
  title = {An {{Introduction}} to {{Positional Tracking}} and {{Degrees}} of {{Freedom}} ({{DOF}})},
  author = {Lang, Ben},
  year = {2013},
  month = feb,
  journal = {Road to VR},
  urldate = {2023-10-22},
  abstract = {Positional tracking and degrees of freedom are important concepts needed for understanding how people interact in virtual reality games and other VR spaces.},
  chapter = {Body Tracking},
  langid = {american},
  file = {/home/nunoa/Zotero/storage/M5A4JXHC/introduction-positional-tracking-degrees-freedom-dof.html}
}

@article{leeAdaptiveConsistentManagement2023,
  title = {Adaptive {{Consistent Management}} to {{Prevent System Collapse}} on {{Shared Object Manipulation}} in {{Mixed Reality}}},
  author = {Lee, Jun and Kwon, Hyun},
  year = {2023},
  journal = {Computers, Materials \& Continua},
  volume = {75},
  number = {1},
  pages = {2025--2042},
  publisher = {{Tech Science Press}},
  issn = {1546-2218, 1546-2226},
  doi = {10.32604/cmc.2023.036051},
  urldate = {2023-11-09},
  abstract = {A concurrency control mechanism for collaborative work is a key element in a mixed reality environment. However, conventional locking mechanisms restrict potential tasks or the support of non-owners, thus increasing the working time because of waiting to avoid conflicts. Herein, we propose an adaptive concurrency control approach that can reduce conflicts and work time. We classify shared object manipulation in mixed reality into detailed goals and tasks. Then, we model the relationships among goal, task, and ownership. As the collaborative work progresses, the proposed system adapts the different concurrency control mechanisms of shared object manipulation according to the modeling of goal\textendash task\textendash ownership. With the proposed concurrency control scheme, users can hold shared objects and move and rotate together in a mixed reality environment similar to real industrial sites. Additionally, this system provides MS Hololens and Myo sensors to recognize inputs from a user and provides results in a mixed reality environment. The proposed method is applied to install an air conditioner as a case study. Experimental results and user studies show that, compared with the conventional approach, the proposed method reduced the number of conflicts, waiting time, and total working time.},
  langid = {english},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/9XNQRG9G/Lee and Kwon - 2023 - Adaptive Consistent Management to Prevent System C.pdf}
}

@article{leeApproximateResolutionAsynchronous2016,
  title = {Approximate Resolution of Asynchronous Conflicts among Sequential Collaborations in Dynamic Virtual Environments},
  author = {Lee, Jun and Lim, Mingyu and Park, SungJun and Kim, HyungSeok and Ko, Heedong and Kim, Jee-In},
  year = {2016},
  journal = {Computer Animation and Virtual Worlds},
  volume = {27},
  number = {2},
  pages = {163--180},
  issn = {1546-427X},
  doi = {10.1002/cav.1669},
  urldate = {2023-11-09},
  abstract = {Asynchronous collaboration for a networked virtual environment (NVE) has emerged as a promising area in collaborative computer-aided design applications. The concept of asynchronous collaboration is a sequential collaboration of temporal processes in an NVE where the participants are not required to be present at the time of the collaboration. Conflicts in asynchronous collaboration occur because the preceding task of a participant can influence the output of the ensuing task of another participant. The conflicted tasks must be modified manually. However, it requires considerable time and effort to resolve conflicts in a sequential collaboration. In this paper, we present an asynchronous collaborative framework that converts the conflict states of the shared objects into approximately resolved states. We develop a novel approximate resolution algorithm using a task-based modeling mechanism to resolve the asynchronous conflicts with their corresponding tasks. Moreover, we propose a visual relation editor for convenient management. The participants can set flexible relations among shared objects using the proposed visual editor. The proposed approximate resolution approach can significantly reduce the average resolution time and the number of required manual task resolutions in a virtual environment compared to a manual resolution approach. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {asynchronous collaboration,computer-aided design,networked virtual environment,notion},
  file = {/home/nunoa/Zotero/storage/B68HZRSY/cav.html}
}

@incollection{leeSharedObjectManipulation2016,
  title = {Shared {{Object Manipulation}}},
  booktitle = {Context {{Aware Human-Robot}} and {{Human-Agent Interaction}}},
  author = {Lee, Jun and {Magnenat-Thalmann}, Nadia and Thalmann, Daniel},
  editor = {{Magnenat-Thalmann}, Nadia and Yuan, Junsong and Thalmann, Daniel and You, Bum-Jae},
  year = {2016},
  series = {Human\textendash{{Computer Interaction Series}}},
  pages = {191--207},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-19947-4_9},
  urldate = {2023-11-01},
  abstract = {In this chapter, we introduce a concept of shared object manipulation between real and virtual humans. The shared object manipulation allows both real and virtual humans to collaborate in real-time. These can be applied to 3D telepresence applications such as computer-aided design and virtual simulation and training. However, it is still far from achieving the expected results from this area because it consists of three different and complex research domains. Firstly, we need to consider a virtual object grasping method for intuitive and convenient virtual object manipulation. Secondly, human-like animation is required for virtual object manipulation by virtual humans. Thirdly, consistency management of shared object manipulation is required to avoid conflicts from multiple simultaneous inputs. After review of state of the art, solution approaches and their limitations are introduced. We conclude with a discussion of future directions for the shared object manipulation between real and virtual humans.},
  isbn = {978-3-319-19947-4},
  langid = {english},
  keywords = {Concurrency Control,Conflict Task,Haptic Feedback,notion,Object Manipulation,Virtual Object},
  file = {/home/nunoa/Zotero/storage/2PT35LGA/Lee et al. - 2016 - Shared Object Manipulation.pdf}
}

@article{leeSupportingFineGrainedConcurrent2012,
  title = {Supporting {{Fine-Grained Concurrent Tasks}} and {{Personal Workspaces}} for a {{Hybrid Concurrency Control Mechanism}} in a {{Networked Virtual Environment}}},
  author = {Lee, Jun and Lim, Mingyu and Kim, HyungSeok and Kim, Jee-In},
  year = {2012},
  month = nov,
  journal = {Presence},
  volume = {21},
  number = {4},
  pages = {452--469},
  issn = {1054-7460},
  doi = {10.1162/PRES_a_00127},
  urldate = {2023-10-25},
  abstract = {A concurrency control mechanism for a networked virtual environment is a key element in many collaborative computer-aided design applications. However, conventional object-based locking mechanisms restrict the behaviors of nonowners, and an attribute-based locking mechanism may produce another problem called task-surprise, which disturbs users' collaboration. In this paper, we propose a hybrid concurrency control mechanism that reduces restrictions of nonowners' behaviors and task-surprises in a networked virtual environment. The proposed method consists of two concurrency control approaches: task-based concurrency control and personal workspaces. The task-based concurrency control approach allows nonowners to do some tasks if they do not conflict with the tasks of the owner of the shared object. The personal workspaces approach provides an independent workspace where a user can manipulate copies of the shared objects. The proposed method was applied to a collaborative level design for a large-scale online game as a case study. We evaluated its performance by experiments and user studies to check acceptance and usability of the proposed method.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/9S87ZNMZ/6797717.html}
}

@article{linebargerConcurrencyControlMechanisms2004,
  title = {Concurrency {{Control Mechanisms}} for {{Closely Coupled Collaboration}} in {{Multithreaded Peer-to-Peer Virtual Environments}}},
  author = {Linebarger, John M. and Kessler, G. Drew},
  year = {2004},
  month = jun,
  journal = {Presence},
  volume = {13},
  number = {3},
  pages = {296--314},
  issn = {1054-7460},
  doi = {10.1162/1054746041422316},
  urldate = {2023-10-26},
  abstract = {As collaboration in virtual environments becomes more object-focused and closely coupled, the frequency of conflicts in accessing shared objects can increase. In addition, two kinds of concurrency control ``surprises'' become more disruptive to the collaboration. Undo surprises can occur when a previously visible change is undone because of an access conflict. Intention surprises can happen when a concurrent action by a remote session changes the structure of a shared object at the same perceived time as a local access of that object, such that the local user might not get what they expect because they have not had time to visually process the change. A hierarchy of three concurrency control mechanisms is presented in descending order of collaborative surprises, which allows the concurrency scheme to be tailored to the tolerance for such surprises. One mechanism is semioptimistic; the other two are pessimistic. Designed for peer-to-peer virtual environments in which several threads have access to the shared scene graph, these algorithms are straightforward and relatively simple. They can be implemented using C/C++ and Java, under Windows and Unix, on both desktop and immersive systems. In a series of usability experiments, the average performance of the most conservative concurrency control mechanism on a local LAN was found to be quite acceptable.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/3QPL4EFE/6789125.html}
}

@article{liuManipulationGuidanceField2023,
  title = {Manipulation {{Guidance Field}} for {{Collaborative Object Manipulation}} in {{VR}}},
  author = {Liu, Xiaolong and Wang, Lili and Luan, Shuai},
  year = {2023},
  journal = {International Journal of Human\textendash Computer Interaction},
  volume = {0},
  number = {0},
  pages = {1--17},
  publisher = {{Taylor \& Francis}},
  issn = {1044-7318},
  doi = {10.1080/10447318.2023.2250941},
  urldate = {2023-11-02},
  abstract = {Object manipulation is a fundamental interaction in virtual reality (VR). Efficient and accurate manipulation is important for many VR applications, especially collaborative VR applications. We introduce a collaborative method based on the manipulation guidance field (MGF) to improve manipulation accuracy and efficiency. MGF aims to guide users of different manipulation types to different manipulation viewpoints to efficiently and collaboratively manipulate objects. First, we introduce the concept of MGF and its construction method. Two strategies are offered to accelerate the MGF updating process. A collaborative manipulation method to manipulate objects using the guidance of MGF is then proposed. Finally, a user study (n = 36 participants) was conducted to evaluate the efficiency and accuracy of our MGF-based collaborative object manipulation method in three scenes: (1) Livingroom scene; (2) WaveHouse scene; (3) Pipe scene. Compared to a control method without MGF, the results show that our MGF-based method has significantly reduced task completion time, position error, rotation error, and task load.},
  keywords = {collaboration,guiding field,human-computer interaction,notion,object manipulation,Virtual reality},
  file = {/home/nunoa/Zotero/storage/3VUFAKF4/Liu et al. - 2023 - Manipulation Guidance Field for Collaborative Obje.pdf}
}

@inproceedings{margeryGeneralFrameworkCooperative1999,
  title = {A {{General Framework}} for {{Cooperative Manipulation}} in {{Virtual Environments}}},
  booktitle = {Virtual {{Environments}} '99},
  author = {Margery, David and Arnaldi, Bruno and Plouzeau, No{\"e}l},
  editor = {Gervautz, Michael and Schmalstieg, Dieter and Hildebrand, Axel},
  year = {1999},
  series = {Eurographics},
  pages = {169--178},
  publisher = {{Springer}},
  address = {{Vienna}},
  doi = {10.1007/978-3-7091-6805-9_17},
  abstract = {Whereas cooperation and collaboration have become two popular words in virtual reality, the problem of cooperative manipulation has been mainly left aside due to the great number of other challenges facing anyone trying to setup multi-user worlds. We define cooperative manipulation as a situation where two or more users interact on the same object in a concurrent but cooperative way. The focus of this paper is to describe an experiment whose goal was to experiment problems specific of cooperative manipulation setups. Those problems include synchronizing user's input over the network, mapping user's input into a meaningful 3-D movement thanks to what we call a model of activity and giving him relevant visual information. In this paper, we present a general framework able to take into account these problems.It is compatible with physically simulated objects and has been implemented using Java, VRML and a distributed approach.},
  isbn = {978-3-7091-6805-9},
  langid = {english},
  keywords = {Activity Model,Cooperation Level,notion,Virtual Environment,Virtual Reality,Virtual World}
}

@article{mendesSurvey3DVirtual2019,
  title = {A {{Survey}} on {{3D Virtual Object Manipulation}}: {{From}} the {{Desktop}} to {{Immersive Virtual Environments}}},
  shorttitle = {A {{Survey}} on {{3D Virtual Object Manipulation}}},
  author = {Mendes, D. and Caputo, F. M. and Giachetti, A. and Ferreira, A. and Jorge, J.},
  year = {2019},
  journal = {Computer Graphics Forum},
  volume = {38},
  number = {1},
  pages = {21--45},
  issn = {1467-8659},
  doi = {10.1111/cgf.13390},
  urldate = {2023-11-02},
  abstract = {Interactions within virtual environments often require manipulating 3D virtual objects. To this end, researchers have endeavoured to find efficient solutions using either traditional input devices or focusing on different input modalities, such as touch and mid-air gestures. Different virtual environments and diverse input modalities present specific issues to control object position, orientation and scaling: traditional mouse input, for example, presents non-trivial challenges because of the need to map between 2D input and 3D actions. While interactive surfaces enable more natural approaches, they still require smart mappings. Mid-air gestures can be exploited to offer natural manipulations mimicking interactions with physical objects. However, these approaches often lack precision and control. All these issues and many others have been addressed in a large body of work. In this article, we survey the state-of-the-art in 3D object manipulation, ranging from traditional desktop approaches to touch and mid-air interfaces, to interact in diverse virtual environments. We propose a new taxonomy to better classify manipulation properties. Using our taxonomy, we discuss the techniques presented in the surveyed literature, highlighting trends, guidelines and open challenges, that can be useful both to future research and to developers of 3D user interfaces.},
  copyright = {\textcopyright{} 2018 The Authors Computer Graphics Forum \textcopyright{} 2018 The Eurographics Association and John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {3D user interfaces,H.5.2 Information Interfaces and Presentation: User Interfaces Interaction styles,interaction techniques,notion,virtual object manipulation},
  file = {/home/nunoa/Zotero/storage/7X4GV7QA/cgf.html}
}

@article{monahanVirtualRealityCollaborative2008,
  title = {Virtual Reality for Collaborative E-Learning},
  author = {Monahan, Teresa and McArdle, Gavin and Bertolotto, Michela},
  year = {2008},
  month = may,
  journal = {Computers \& Education},
  volume = {50},
  number = {4},
  pages = {1339--1353},
  issn = {0360-1315},
  doi = {10.1016/j.compedu.2006.12.008},
  urldate = {2023-10-25},
  abstract = {In the past, the term e-learning referred to any method of learning that used electronic delivery methods. With the advent of the Internet however, e-learning has evolved and the term is now most commonly used to refer to online courses. A multitude of systems are now available to manage and deliver learning content online. While these have proved popular, they are often single-user learning environments which provide little in the way of interaction or stimulation for the student. As the concept of lifelong learning now becomes a reality and thus more and more people are partaking in online courses, researchers are constantly exploring innovative techniques to motivate online students and enhance the e-learning experience. This article presents our research in this area and the resulting development of CLEV-R, a Collaborative Learning Environment with Virtual Reality. This web-based system uses Virtual Reality (VR) and multimedia and provides communication tools to support collaboration among students. In this article, we describe the features of CLEV-R, its adaptation for mobile devices and present the findings from an initial evaluation.},
  keywords = {Cooperative/collaborative learning,Interactive learning environments,Lifelong learning,Multimedia/hypermedia systems,notion,Virtual reality},
  file = {/home/nunoa/Zotero/storage/2EUF5ISC/Monahan et al. - 2008 - Virtual reality for collaborative e-learning.pdf;/home/nunoa/Zotero/storage/9IBU6QAL/S0360131506001989.html}
}

@misc{morganIntroductionDistributedVirtual2005,
  title = {Introduction to {{Distributed Virtual Environments}}},
  author = {Morgan, Graham},
  year = {2005},
  month = jan,
  journal = {Introduction to Distributed Virtual Environments},
  urldate = {2023-10-26},
  howpublished = {http://homepages.cs.ncl.ac.uk/graham.morgan/dve.htm},
  file = {/home/nunoa/Zotero/storage/DJNF659H/dve.html}
}

@article{mortensenCollaborationTeleImmersiveEnvironments2002,
  title = {Collaboration in {{Tele-Immersive Environments}}},
  author = {Mortensen, Jesper and Vinayagamoorthy, Vinoba and Slater, Mel and Steed, Anthony and Lok, Benjamin and Whitton, Mary},
  year = {2002},
  month = jan,
  pages = {93--101},
  issn = {1-58113-535-1},
  abstract = {This paper describes a study of remote collaboration between people in a shared virtual environment. Seventeen subjects were recruited at University College London, who worked with a confederate at University of North Carolina Chapel Hill. Each pair was required to negotiate the task of handling an object together, and moving a few metres into a building. The DIVE system was used throughout, and the network support was Internet-2. This was an observational study to examine the extent to which such collaboration was possible, to explore the limitations of DIVE within this context, and to examine the relationship between several variables such as co-presence and task performance. The results suggest that although the task is possible under this framework, it could only be achieved by various software tricks within the DIVE framework. A new Virtual Environment system is required that has better knowledge of network performance, and that supports shared object manipulation across a network. The participant-study suggests that co-presence, the sense of being together with another person, was significantly and positively correlated with task performance.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/7CVRBVJN/Mortensen et al. - 2002 - Collaboration in Tele-Immersive Environments.pdf}
}

@inproceedings{newcombeKinectFusionRealtimeDense2011,
  title = {{{KinectFusion}}: {{Real-time}} Dense Surface Mapping and Tracking},
  shorttitle = {{{KinectFusion}}},
  booktitle = {2011 10th {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}}},
  author = {Newcombe, Richard A. and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J. and Kohi, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
  year = {2011},
  month = oct,
  pages = {127--136},
  doi = {10.1109/ISMAR.2011.6092378},
  urldate = {2023-10-22},
  abstract = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
  file = {/home/nunoa/Zotero/storage/DF6TQZ4L/Newcombe et al. - 2011 - KinectFusion Real-time dense surface mapping and .pdf;/home/nunoa/Zotero/storage/9GK3VKIH/6162880.html}
}

@inproceedings{nicolaLeapMotionSupporting2016,
  title = {Leap {{Motion}} Supporting Medical Education},
  booktitle = {2016 12th {{IEEE International Symposium}} on {{Electronics}} and {{Telecommunications}} ({{ISETC}})},
  author = {Nicola, Stelian and {Stoicu-Tivadar}, L{\u a}cr{\u a}mioara and Virag, Ioan and {Cri{\c s}an-Vida}, Mihaela},
  year = {2016},
  month = oct,
  pages = {153--156},
  doi = {10.1109/ISETC.2016.7781080},
  urldate = {2023-09-26},
  abstract = {The paper presents an application that enables medical students, health professionals, and individuals passionate about medicine to control human skeleton bones through gesture interaction using Leap Motion sensor. The application contains a main application and three applications derived from the main application with 3D bone images. The Leap Motion sensor is based on hands gesture recognition previously defined for a good control on the bones of the human skeleton. Tests made on items of the application show that it is easy to use and control.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/EYKF48LW/Nicola et al. - 2016 - Leap Motion supporting medical education.pdf;/home/nunoa/Zotero/storage/L9F97KGB/7781080.html}
}

@inproceedings{niemantsverdrietDesigningMultiUserInteraction2016,
  title = {Designing for {{Multi-User Interaction}} in the {{Home Environment}}: {{Implementing Social Translucence}}},
  shorttitle = {Designing for {{Multi-User Interaction}} in the {{Home Environment}}},
  booktitle = {Proceedings of the 2016 {{ACM Conference}} on {{Designing Interactive Systems}}},
  author = {Niemantsverdriet, Karin and Broekhuijsen, Mendel and {van Essen}, Harm and Eggen, Berry},
  year = {2016},
  month = jun,
  series = {{{DIS}} '16},
  pages = {1303--1314},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2901790.2901808},
  urldate = {2023-10-26},
  abstract = {Interfaces of interactive systems for domestic use are usually designed for individual interactions although these interactions influence multiple users. In order to prevent conflicts and unforeseen influences on others we propose to leverage the human ability to take each other into consideration in the interaction. A promising approach for this is found in the social translucence framework, which was originally described by Erickson \& Kellogg. In this paper, we investigate how to design multi-user interfaces for domestic interactive systems through two design cases where we focus on the implementation of social translucence constructs (visibility, awareness, and accountability) in the interaction. We use the resulting designs to extract design considerations: interfaces should not prescribe behavior, need to offer sufficient interaction alternatives, and previous settings need to be retrievable. We also identify four steps that can be integrated in any design process to help designers in creating interfaces that support multi-user interaction through social translucence.},
  isbn = {978-1-4503-4031-1},
  keywords = {home environment,interaction design,interactive systems,multi-user interaction,research-through-design,social translucence.},
  file = {/home/nunoa/Zotero/storage/TY73IQ69/Niemantsverdriet et al. - 2016 - Designing for Multi-User Interaction in the Home E.pdf}
}

@article{noohiModelHumanHuman2016,
  title = {A {{Model}} for {{Human}}\textendash{{Human Collaborative Object Manipulation}} and {{Its Application}} to {{Human}}\textendash{{Robot Interaction}}},
  author = {Noohi, Ehsan and {\v Z}efran, Milo{\v s} and Patton, James L.},
  year = {2016},
  month = aug,
  journal = {IEEE Transactions on Robotics},
  volume = {32},
  number = {4},
  pages = {880--896},
  issn = {1941-0468},
  doi = {10.1109/TRO.2016.2572698},
  urldate = {2023-11-08},
  abstract = {During collaborative object manipulation, the interaction forces provide a communication channel through which humans coordinate their actions. In order for the robots to engage in physical collaboration with humans, it is necessary to understand this coordination process. Unfortunately, there is no intrinsic way to define the interaction forces. In this study, we propose a model that allows us to compute the interaction force during a dyadic cooperative object manipulation task. The model is derived directly from the existing theories on human arm movements. The results of a user study with 22 human subjects prove the validity of the proposed model. The model is then embedded in a control strategy that enables the robot to engage in a cooperative task with a human. The performance evaluation of the controller through simulation shows that the control strategy is a promising candidate for a cooperative human-robot interaction.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/7ANSLELU/Noohi et al. - 2016 - A Model for Human–Human Collaborative Object Manip.pdf;/home/nunoa/Zotero/storage/SHGIJ894/7534756.html}
}

@inproceedings{odaVirtualReplicasRemote2015,
  title = {Virtual {{Replicas}} for {{Remote Assistance}} in {{Virtual}} and {{Augmented Reality}}},
  booktitle = {Proceedings of the 28th {{Annual ACM Symposium}} on {{User Interface Software}} \& {{Technology}}},
  author = {Oda, Ohan and Elvezio, Carmine and Sukan, Mengu and Feiner, Steven and Tversky, Barbara},
  year = {2015},
  month = nov,
  series = {{{UIST}} '15},
  pages = {405--415},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2807442.2807497},
  urldate = {2023-10-22},
  abstract = {In many complex tasks, a remote subject-matter expert may need to assist a local user to guide actions on objects in the local user's environment. However, effective spatial referencing and action demonstration in a remote physical environment can be challenging. We introduce two approaches that use Virtual Reality (VR) or Augmented Reality (AR) for the remote expert, and AR for the local user, each wearing a stereo head-worn display. Both approaches allow the expert to create and manipulate virtual replicas of physical objects in the local environment to refer to parts of those physical objects and to indicate actions on them. This can be especially useful for parts that are occluded or difficult to access. In one approach, the expert points in 3D to portions of virtual replicas to annotate them. In another approach, the expert demonstrates actions in 3D by manipulating virtual replicas, supported by constraints and annotations. We performed a user study of a 6DOF alignment task, a key operation in many physical task domains, comparing both approaches to an approach in which the expert uses a 2D tablet-based drawing system similar to ones developed for prior work on remote assistance. The study showed the 3D demonstration approach to be faster than the others. In addition, the 3D pointing approach was faster than the 2D tablet in the case of a highly trained expert.},
  isbn = {978-1-4503-3779-3},
  keywords = {3D referencing techniques,assembly,collaborative mixed/augmented reality,maintenance and repair,notion,remote guidance,remote task assistance},
  file = {/home/nunoa/Zotero/storage/HB6QNRG2/Oda et al. - 2015 - Virtual Replicas for Remote Assistance in Virtual .pdf}
}

@inproceedings{orts-escolanoHoloportationVirtual3D2016a,
  title = {Holoportation: {{Virtual 3D Teleportation}} in {{Real-time}}},
  shorttitle = {Holoportation},
  booktitle = {Proceedings of the 29th {{Annual Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {{Orts-Escolano}, Sergio and Rhemann, Christoph and Fanello, Sean and Chang, Wayne and Kowdle, Adarsh and Degtyarev, Yury and Kim, David and Davidson, Philip L. and Khamis, Sameh and Dou, Mingsong and Tankovich, Vladimir and Loop, Charles and Cai, Qin and Chou, Philip A. and Mennicken, Sarah and Valentin, Julien and Pradeep, Vivek and Wang, Shenlong and Kang, Sing Bing and Kohli, Pushmeet and Lutchyn, Yuliya and Keskin, Cem and Izadi, Shahram},
  year = {2016},
  month = oct,
  series = {{{UIST}} '16},
  pages = {741--754},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2984511.2984517},
  urldate = {2023-10-22},
  abstract = {We present an end-to-end system for augmented and virtual reality telepresence, called Holoportation. Our system demonstrates high-quality, real-time 3D reconstructions of an entire space, including people, furniture and objects, using a set of new depth cameras. These 3D models can also be transmitted in real-time to remote users. This allows users wearing virtual or augmented reality displays to see, hear and interact with remote participants in 3D, almost as if they were present in the same physical space. From an audio-visual perspective, communicating and interacting with remote users edges closer to face-to-face communication. This paper describes the Holoportation technical system in full, its key interactive capabilities, the application scenarios it enables, and an initial qualitative study of using this new communication medium.},
  isbn = {978-1-4503-4189-9},
  keywords = {3d capture,depth cameras,gpu,mixed reality,non-rigid reconstruction,notion,real-time,telepresence},
  file = {/home/nunoa/Zotero/storage/2TBYXM3Y/Orts-Escolano et al. - 2016 - Holoportation Virtual 3D Teleportation in Real-ti.pdf}
}

@inproceedings{pierceVoodooDollsSeamless1999,
  title = {Voodoo {{Dolls}}: {{Seamless}} Interaction at Multiple Scales in Virtual Environments},
  shorttitle = {Voodoo {{Dolls}}},
  author = {Pierce, Jeffrey and Stearns, Brian and Pausch, Randy},
  year = {1999},
  month = apr,
  pages = {141--145},
  doi = {10.1145/300523.300540},
  abstract = {The Voodoo Dolls technique is a two-handed interaction technique for manipulating objects at a distance in immersive virtual environments. This technique addresses some limitations of existing techniques: they do not provide a lightweight method of interacting with objects of widely varying sizes, and many limit the objects that can be selected and the manipulations possible after making a selection. With the Voodoo Dolls technique, the user dynamically creates dolls: transient, hand held copies of objects whose effects on the objects they represent are determined by the hand holding them. For simplicity, we assume a right-handed user in the following discussion. When a user holds a doll in his right hand and moves it relative to a doll in his left hand, the object represented by the doll in his right hand moves to the same position and orientation relative to the object represented by the doll in his left hand. The system scales the dolls so that the doll in the left hand is half a meter along its longest dimension and the other dolls maintain the same relative size; this allows the user to work seamlessly at multiple scales. The Voodoo Dolls technique also allows both visible and occluded objects to be selected, and provides a stationary frame of reference for working relative to moving objects.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/SXEWS2QY/Pierce et al. - 1999 - Voodoo Dolls Seamless interaction at multiple sca.pdf}
}

@article{pinhoCooperativeObjectManipulation2008,
  title = {Cooperative Object Manipulation in Collaborative Virtual Environments},
  author = {Pinho, Marcio S. and Bowman, Doug A. and Freitas, Carla M. Dal Sasso},
  year = {2008},
  month = jun,
  journal = {Journal of the Brazilian Computer Society},
  volume = {14},
  number = {2},
  pages = {53--67},
  issn = {1678-4804},
  doi = {10.1007/BF03192559},
  urldate = {2023-11-01},
  abstract = {Cooperative manipulation refers to the simultaneous manipulation of a virtual object by multiple users in an immersive virtual environment (VE). In this work, we present techniques for cooperative manipulation based on existing single-user techniques. We discuss methods of combining simultaneous user actions, based on the separation of degrees of freedom between two users, and the awareness tools used to provide the necessary knowledge of the partner activities during the cooperative interaction process. We also present a framework for supporting the development of cooperative manipulation techniques, which are based on rules for combining single user interaction techniques. Finally, we report an evaluation of cooperative manipulation scenarios, the results indicating that, in certain situations, cooperative manipulation is more efficient and usable than singleuser manipulation.},
  langid = {english},
  keywords = {Collaborative interaction,Cooperative interaction,Interaction techniques,notion,Virtual environments,VR experiments},
  file = {/home/nunoa/Zotero/storage/UGXJIRGJ/Pinho et al. - 2008 - Cooperative object manipulation in collaborative v.pdf}
}

@misc{ReviewPresencePerformance,
  title = {A {{Review}} of {{Presence}} and {{Performance}} in {{Virtual Environments}}: {{International Journal}} of {{Human}}\textendash{{Computer Interaction}}: {{Vol}} 12, {{No}} 1},
  urldate = {2023-11-15},
  howpublished = {https://www.tandfonline.com/doi/abs/10.1207/S15327590IJHC1201\_1},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/6I343RNH/S15327590IJHC1201_1.html}
}

@inproceedings{rheinerBirdlyAttemptFly2014,
  title = {Birdly an Attempt to Fly},
  booktitle = {{{ACM SIGGRAPH}} 2014 {{Emerging Technologies}}},
  author = {Rheiner, Max},
  year = {2014},
  month = jul,
  series = {{{SIGGRAPH}} '14},
  pages = {1},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2614066.2614101},
  urldate = {2023-11-15},
  abstract = {'Birdly' is an installation which explores the experience of a bird in flight. It tries to capture the mediated flying experience, with several methods. Unlike a common flight simulator you do not control a machine you embody a bird, the Red Kite. To evoke this embodiment we mainly rely on the sensory-motor coupling. The participant can control the simulator with his hands and arms, which directly correlates to the wings and the primary feathers of the bird. Those inputs are reflected in the flight model of the bird and displayed physically by the simulator through nick, roll and heave movements. Visualized through HMD (Oculus Rift) the participant is embedded in a virtual landscape where his body is the body of a Red Kite. The whole scenery is perceived in the first person perspective of a bird. To intensify the embodiment we include additional sonic, olfactory and wind feedback. Sound wise you perceive only the roaring of the wind and the flaps of the wings. The olfactory feedback is based on the location which the bird flies over in the scenery and ranges from the scent of a forest, soil, to several other odors of the wilderness. According to the speed of the bird the simulator regulates the headwind with a fan.},
  isbn = {978-1-4503-2961-3},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/EA2M9I2P/Rheiner - 2014 - Birdly an attempt to fly.pdf}
}

@inproceedings{riegeBentPickRay2006,
  title = {The {{Bent Pick Ray}}: {{An Extended Pointing Technique}} for {{Multi-User Interaction}}},
  shorttitle = {The {{Bent Pick Ray}}},
  booktitle = {{{3D User Interfaces}} ({{3DUI}}'06)},
  author = {Riege, Kai and Holtkamper, T. and Wesche, G. and Frohlich, B.},
  year = {2006},
  month = mar,
  pages = {62--65},
  doi = {10.1109/VR.2006.127},
  urldate = {2023-11-09},
  abstract = {This paper presents a collaborative pointing technique for colocated multi-user interaction in projection-based virtual environments. Our approach uses bent pick rays to allow users to collaboratively work together without locking objects. Moreover, a user can manipulate distant objects in immediate reach, using a Scaled- Grab technique. The main purpose of the bent pick ray is to provide continuous visual user feedback, keeping a user informed about the collaborative manipulation.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/7IISF9YU/Riege et al. - 2006 - The Bent Pick Ray An Extended Pointing Technique .pdf;/home/nunoa/Zotero/storage/GJ6V5TEQ/1647508.html}
}

@article{robertsConstructingGazeboSupporting2003,
  title = {Constructing a {{Gazebo}}: {{Supporting Team Work}} in a {{Tightly Coupled}}, {{Distributed Task}} in {{Virtual Reality}}.},
  shorttitle = {Constructing a {{Gazebo}}},
  author = {Roberts, David and Wolff, Robin and Otto, Oliver and Steed, Anthony},
  year = {2003},
  month = dec,
  journal = {Presence Teleoperators \& Virtual Environments},
  volume = {12},
  pages = {644--657},
  doi = {10.1162/105474603322955932},
  abstract = {Many tasks require teamwork. Team members may work concurrently but there must be some occasions of coming together. Collaborative Virtual Environments (CVE) allow distributed teams to come together across distance to share a task. Studies of CVE system have tended to look at the sense of presence or co-presence with other people. They have avoided studying close interaction between users, such as the shared manipulation of objects, because CVEs suffer from inherent network delays and often have cumbersome user interfaces. Little is known about the effectiveness of collaboration in tasks requiring various forms of object sharing and, in particular, the concurrent manipulation of objects. This paper investigates the effectiveness of supporting teamwork between a geographically distributed group, in a task requiring the shared manipulation of objects. In order to complete the task, users must share objects through concurrent manipulation of both, the same, and distinct attributes. The effectiveness of teamwork is measured in terms of time taken to achieve each step, as well as the impression of users. The effect of interface is examined by comparing various combinations of walk-in cubic Immersive Projection Technology (IPT) displays and desktop devices.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/JLHZUVBJ/Roberts et al. - 2003 - Constructing a Gazebo Supporting Team Work in a T.pdf}
}

@inproceedings{robertsControllingConsistencyCollaborative2004,
  title = {Controlling {{Consistency}} within {{Collaborative Virtual Environments}}},
  booktitle = {Eighth {{IEEE International Symposium}} on {{Distributed Simulation}} and {{Real-Time Applications}}},
  author = {Roberts, David and Wolff, Robin},
  year = {2004},
  month = oct,
  pages = {46--52},
  issn = {1550-6525},
  doi = {10.1109/DS-RT.2004.13},
  urldate = {2023-10-25},
  abstract = {Collaborative Virtual Environments (CVE) are a form of telecommunication technology that bring together co-located or remote, participants within a spatial social and information context. Collaboration occurs between people and often around shared objects. Fruitful cooperation is helped by natural and intuitive ways of communicating and sharing, for which responsiveness and consistency are leading factors. Many CVEs maximise local responsiveness through a process of localisation and database replication, increasing responsiveness at the cost of lowering consistency. This is acceptable provided the application does not require the shared manipulation of objects. Those that do, require consistency control that provide sufficient synchronisation, ordering and update control, whilst maximising concurrence and thus the responsiveness of the system. This paper describes the major issues and principles of consistency control and demonstrates how we have applied many of these principles in three CVEs.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/YDGHJPJU/1364578.html}
}

@article{robertsInfluenceSupportingProtocol1999,
  title = {Influence of the Supporting Protocol on the Latencies Induced by Concurrency Control within a Large Scale Multi User Distributed Virtual Reality System},
  author = {Roberts, D. J. and Worthington, B. G. and Sharkey, Paul and Strassner, J.},
  year = {1999},
  month = jan,
  journal = {Simulation Series},
  volume = {31},
  pages = {70--75},
  publisher = {{SCS, Society for Computer Simulation}},
  urldate = {2023-10-25},
  abstract = {University Publications}
}

@inproceedings{robertsMaximisingConcurrencyScalability1997,
  title = {Maximising Concurrency and Scalability in a Consistent, Causal, Distributed Virtual Reality System, Whilst Minimising the Effect of Network Delays},
  booktitle = {Proceedings of {{IEEE}} 6th {{Workshop}} on {{Enabling Technologies}}: {{Infrastructure}} for {{Collaborative Enterprises}}},
  author = {Roberts, D.J. and Sharkey, P.M.},
  year = {1997},
  month = jun,
  pages = {161--166},
  doi = {10.1109/ENABL.1997.630808},
  urldate = {2023-11-09},
  abstract = {The development of large scale virtual reality and simulation systems have been mostly driven by the DIS and HLA standards community. A number of issues are coming to light about the applicability of these standards, in their present state, to the support of general multi-user VR systems. This paper pinpoints four issues that must be readdressed before large scale virtual reality systems become accessible to a larger commercial and public domain: a reduction in the effects of network delays; scalable causal event delivery; update control; and scalable reliable communication. Each of these issues is tackled through a common theme of combining wall clock and causal time-related entity behaviour, knowledge of network delays and prediction of entity behaviour, that together overcome many of the effects of network delays.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/GMSLZVCD/Roberts and Sharkey - 1997 - Maximising concurrency and scalability in a consis.pdf;/home/nunoa/Zotero/storage/UNTE7QXD/630808.html}
}

@article{robertsOptimisingExchangeAttribute1998,
  title = {Optimising Exchange of Attribute Ownership in the {{DMSO RTI}}},
  author = {Roberts, D. J. and Richardson, A. T. and Sharkey, Paul and Lake, T. W.},
  year = {1998},
  month = mar,
  journal = {Proceedings of the Spring Simulation Interoperability Workshop, SISO},
  pages = {379--386},
  urldate = {2023-10-26},
  abstract = {University Publications},
  file = {/home/nunoa/Zotero/storage/BW48Y9NC/27192.html}
}

@article{robertsSupportingCloselyCoupled2005a,
  title = {Supporting a {{Closely Coupled Task}} between a {{Distributed Team}}: {{Using Immersive Virtual Reality Technology}}},
  shorttitle = {Supporting a {{Closely Coupled Task}} between a {{Distributed Team}}},
  author = {Roberts, David J. and Wolff, Robin and Otto, Oliver},
  year = {2005},
  journal = {COMPUTING AND INFORMATICS},
  volume = {24},
  number = {1},
  pages = {7--29},
  issn = {2585-8807},
  urldate = {2023-10-26},
  abstract = {Collaboration and teamwork is important in many areas of our lives. People come together to share and discuss ideas, split and distribute work or help and support each other. The sharing of information and artefacts is a central part of collaboration. This often involves the manipulation of shared objects, both sequentially as well as concurrently. For coordinating an efficient collaboration, communication between the team members is necessary. This can happen verbally in form of speech or text and non-verbally through gesturing, pointing, gaze or facial expressions and the referencing and manipulation of shared objects. Collaborative Virtual Environments (CVE) allow remote users to come together and interact with each other and virtual objects within a computer simulated environment. Immersive display interfaces, such as a walk-in display (e.g. CAVE), that place a human physically into the synthetic environment, lend themselves well to support a natural manipulation of objects as well a set of natural non-verbal human communication, as they can both capture and display human movement. Communication of tracking data, however, can saturate the network and result in delay or loss of messages vital to the manipulation of shared objects. This paper investigates the reality of shared object manipulation between remote users collaborating through linked walk-in displays and extends our research in [27]. Various forms of shared interaction are examined through a set of structured sub tasks within a representative construction task. We report on extensive user-trials between three walk-in displays in the UK and Austria linked over the Internet using a CVE, and demonstrate such effects on a naive implementation of a benchmark application, the Gazebo building task. We then present and evaluate application-level workarounds and conclude by suggesting solutions that may be implemented within next-generation CVE infrastructures.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {Immersive Virtual Reality,notion},
  file = {/home/nunoa/Zotero/storage/DA5LDC99/Roberts et al. - 2005 - Supporting a Closely Coupled Task between a Distri.pdf}
}

@article{ruddleSymmetricAsymmetricAction2002,
  title = {Symmetric and Asymmetric Action Integration during Cooperative Object Manipulation in Virtual Environments},
  author = {Ruddle, Roy A. and Savage, Justin C. D. and Jones, Dylan M.},
  year = {2002},
  month = dec,
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {9},
  number = {4},
  pages = {285--308},
  issn = {1073-0516},
  doi = {10.1145/586081.586084},
  urldate = {2023-11-01},
  abstract = {Cooperation between multiple users in a virtual environment (VE) can take place at one of three levels. These are defined as where users can perceive each other (Level 1), individually change the scene (Level 2), or simultaneously act on and manipulate the same object (Level 3). Despite representing the highest level of cooperation, multiuser object manipulation has rarely been studied. This paper describes a behavioral experiment in which the piano movers' problem (maneuvering a large object through a restricted space) was used to investigate object manipulation by pairs of participants in a VE. Participants' interactions with the object were integrated together either symmetrically or asymmetrically. The former only allowed the common component of participants' actions to take place, but the latter used the mean. Symmetric action integration was superior for sections of the task when both participants had to perform similar actions, but if participants had to move in different ways (e.g., one maneuvering him/herself through a narrow opening while the other traveled down a wide corridor) then asymmetric integration was superior. With both forms of integration, the extent to which participants coordinated their actions was poor and this led to a substantial cooperation overhead (the reduction in performance caused by having to cooperate with another person).},
  keywords = {notion,object manipulation,piano movers' problem,rules of interaction,Virtual environments},
  file = {/home/nunoa/Zotero/storage/A8I2ME3U/Ruddle et al. - 2002 - Symmetric and asymmetric action integration during.pdf}
}

@article{s.richmanSOFTWARECATCHESTEAM1987,
  title = {{{SOFTWARE CATCHES THE TEAM SPIRIT New}} Computer Programs May Soon Change the Way Groups of People Work Together -- and Start Delivering the Long-Awaited Payoff from Office Automation. - {{June}} 8, 1987},
  author = {S. Richman, Louis and Slovak, Julianne},
  year = {1987},
  month = jun,
  journal = {Fortune},
  urldate = {2023-10-25},
  file = {/home/nunoa/Zotero/storage/SZ6M5692/index.html}
}

@inproceedings{shirmohammadiSharedObjectManipulation2004,
  title = {Shared {{Object Manipulation}} with {{Decorators}} in {{Virtual Environments}}},
  booktitle = {Eighth {{IEEE International Symposium}} on {{Distributed Simulation}} and {{Real-Time Applications}}},
  author = {Shirmohammadi, S. and Woo, Nancy Ho},
  year = {2004},
  month = oct,
  pages = {230--233},
  issn = {1550-6525},
  doi = {10.1109/DS-RT.2004.36},
  urldate = {2023-11-15},
  abstract = {One of the known problems with shared object manipulation in virtual environments is the disruptive effect of network lag in collaboration sessions. Most solutions to this problem revolve around techniques to compensate for this lag at the network communication level. In this article, we examine a different approach: informing the user about the lag, using decorators, and allowing the user to react to it intuitively.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/U886WTBL/Shirmohammadi and Woo - 2004 - Shared Object Manipulation with Decorators in Virt.pdf;/home/nunoa/Zotero/storage/9WQJBUDQ/1364603.html}
}

@inproceedings{sidorakisBinocularEyetrackingControl2015,
  title = {Binocular Eye-Tracking for the Control of a {{3D}} Immersive Multimedia User Interface},
  booktitle = {2015 {{IEEE}} 1st {{Workshop}} on {{Everyday Virtual Reality}} ({{WEVR}})},
  author = {Sidorakis, Nikolaos and Koulieris, George Alex and Mania, Katerina},
  year = {2015},
  month = mar,
  pages = {15--18},
  doi = {10.1109/WEVR.2015.7151689},
  urldate = {2023-11-01},
  abstract = {In this paper, we present an innovative approach to design a gazecontrolled Multimedia User Interface for modern, immersive headsets. The wide-spread availability of consumer grade Virtual Reality Head Mounted Displays such as the Oculus RiftTM transformed VR to a commodity available for everyday use. However, Virtual Environments require new paradigms of User Interfaces, since standard 2D interfaces are designed to be viewed from a static vantage point only, e.g. the computer screen. Additionally, traditional input methods such as the keyboard and mouse are hard to manipulate when the user wears a Head Mounted Display. We present a 3D Multimedia User Interface based on eye-tracking and develop six applications which cover commonly operated actions of everyday computing such as mail composing and multimedia viewing. We perform a user study to evaluate our system by acquiring both quantitative and qualitative data. The study indicated that users make less type errors while operating the eye-controlled interface compared to using the standard keyboard during immersive viewing. Subjects stated that they enjoyed the eye-tracking 3D interface more than the keyboard/mouse combination.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/DBYY5KTI/Sidorakis et al. - 2015 - Binocular eye-tracking for the control of a 3D imm.pdf;/home/nunoa/Zotero/storage/ZWVUSYUV/7151689.html}
}

@inproceedings{silvaPHANToMOMNIHaptic2009,
  title = {{{PHANToM OMNI Haptic Device}}: {{Kinematic}} and {{Manipulability}}},
  shorttitle = {{{PHANToM OMNI Haptic Device}}},
  booktitle = {2009 {{Electronics}}, {{Robotics}} and {{Automotive Mechanics Conference}} ({{CERMA}})},
  author = {Silva, Alejandro Jarillo and Ramirez, Omar A. Dom{\'i}nguez and Vega, Vicente Parra and Oliver, Jesus P. Ordaz},
  year = {2009},
  month = sep,
  pages = {193--198},
  doi = {10.1109/CERMA.2009.55},
  urldate = {2023-10-26},
  abstract = {The haptic device kinematics (position and its derivates) allows evaluate the virtual representation of the human operator in a virtual visualization at the same form defines the interaction with virtual objects programs across of a contact and deformation algorithm. The PHANToM OMNI haptic device, allows the kinematicinteraction with complex virtual environments, and the potentials ofapplication require of the available of its mathematical models. In this paper we present the kinematic results and the experimental proofs across of knowledge trajectories, such as the evaluation of the kinematic manipulability allows verify the limits of inherent admissible operation at admissible configuration space or workspace, and to this end allows the free architecture to more applications ondifferent engineering fields.},
  file = {/home/nunoa/Zotero/storage/82WV37QY/Silva et al. - 2009 - PHANToM OMNI Haptic Device Kinematic and Manipula.pdf}
}

@incollection{simeoneIntroductionEverydayVirtual2023,
  title = {Introduction to~{{Everyday Virtual}} and~{{Augmented Reality}}},
  booktitle = {Everyday {{Virtual}} and {{Augmented Reality}}},
  author = {Simeone, Adalberto and Weyers, Benjamin and Bialkova, Svetlana and Lindeman, Robert W.},
  editor = {Simeone, Adalberto and Weyers, Benjamin and Bialkova, Svetlana and Lindeman, Robert W.},
  year = {2023},
  series = {Human\textendash{{Computer Interaction Series}}},
  pages = {1--20},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-05804-2_1},
  urldate = {2023-10-19},
  abstract = {Due to emerging consumer hardware solutions, virtual and augmented reality technologies are gaining increasing relevance in everyday contexts, such as living rooms or office spaces. This raises various challenges such as getting immersed in small and cluttered spaces, integrating immersive tools into existing processes and workflows, as well as the involvement of highly heterogeneous user groups in VR and AR applications. The current chapter aims to introduce and characterise this emerging research field by identifying various challenges in terms of the development and investigation of everyday VR and AR systems. Therefore, we give an overview of everyday VR and AR, discuss challenges for the field that we deem central to the continued adoption and integration of VR and AR into the wider public, as well as provide an overview of current everyday VR and AR in various application contexts and discuss some things from a users' perspective. We then review works from previous WEVR workshops, which were established as a platform for the exchange of everyday VR and AR research, to face the main challenges and provide possible solutions. Finally, we discuss the WEVR impact and point out future research avenues.},
  isbn = {978-3-031-05804-2},
  langid = {english},
  file = {/home/nunoa/Zotero/storage/3X699UX9/Simeone et al. - 2023 - Introduction to Everyday Virtual and Augmented Rea.pdf}
}

@inproceedings{simeoneSubstitutionalRealityUsing2015,
  title = {Substitutional {{Reality}}: {{Using}} the {{Physical Environment}} to {{Design Virtual Reality Experiences}}},
  shorttitle = {Substitutional {{Reality}}},
  booktitle = {Proceedings of the 33rd {{Annual ACM Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Simeone, Adalberto L. and Velloso, Eduardo and Gellersen, Hans},
  year = {2015},
  month = apr,
  series = {{{CHI}} '15},
  pages = {3307--3316},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2702123.2702389},
  urldate = {2023-10-05},
  abstract = {Experiencing Virtual Reality in domestic and other uncontrolled settings is challenging due to the presence of physical objects and furniture that are not usually defined in the Virtual Environment. To address this challenge, we explore the concept of Substitutional Reality in the context of Virtual Reality: a class of Virtual Environments where every physical object surrounding a user is paired, with some degree of discrepancy, to a virtual counterpart. We present a model of potential substitutions and validate it in two user studies. In the first study we investigated factors that affect participants' suspension of disbelief and ease of use. We systematically altered the virtual representation of a physical object and recorded responses from 20 participants. The second study investigated users' levels of engagement as the physical proxy for a virtual object varied. From the results, we derive a set of guidelines for the design of future Substitutional Reality experiences.},
  isbn = {978-1-4503-3145-6},
  keywords = {notion,passive haptics,substitutional reality,virtual reality},
  file = {/home/nunoa/Zotero/storage/TKIIVTXM/Simeone et al. - 2015 - Substitutional Reality Using the Physical Environ.pdf}
}

@inproceedings{simeoneVRMotionTracker2016,
  title = {The {{VR}} Motion Tracker: Visualising Movement of Non-Participants in Desktop Virtual Reality Experiences},
  shorttitle = {The {{VR}} Motion Tracker},
  booktitle = {2016 {{IEEE}} 2nd {{Workshop}} on {{Everyday Virtual Reality}} ({{WEVR}})},
  author = {Simeone, Adalberto L.},
  year = {2016},
  month = mar,
  pages = {1--4},
  doi = {10.1109/WEVR.2016.7859535},
  urldate = {2023-11-01},
  abstract = {In this paper we present the VR Motion Tracker: a widget that informs users of VR applications of the movements of nonparticipants. The design of the widget is inspired by the the motion tracker used in the Alien film franchise. It uses a Kinect to detect other people in the room, besides the user of the VR application. Our system maps this information to a sphere placed within a triangular plane representing the Kinect's field of view. When these non-participants move, the position of the sphere is updated, allowing the user to be aware of nearby movements. We performed a preliminary study where we presented nine participants with our widget design. We found that they considered the widget to be useful and not distracting. We discuss which features they found interesting, and other information and features they considered useful for a future version of this widget.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/XS38PDD9/Simeone - 2016 - The VR motion tracker visualising movement of non.pdf;/home/nunoa/Zotero/storage/8SLU55SL/7859535.html}
}

@inproceedings{soaresCollaborativeHybridVirtual2016,
  title = {Collaborative Hybrid Virtual Environment},
  booktitle = {2016 {{IEEE Symposium}} on {{3D User Interfaces}} ({{3DUI}})},
  author = {Soares, Leonardo Pavanatto and {de Oliveira}, Thomas Volpato and Sangalli, Vicenzo Abichequer and Pinho, Marcio Sarroglia and Kopper, Regis},
  year = {2016},
  month = mar,
  pages = {283--284},
  doi = {10.1109/3DUI.2016.7460081},
  urldate = {2023-11-09},
  abstract = {Supposing that, in a system operated by two users in different positions, it is easier for one of them to perform some operations, we developed a 3D User Interface (3DUI) that allows two users to interact together with an object, using the three modification operations (scale, rotate and translate) to reach a goal. The operations can be performed using two augmented reality cubes, which can obtain up to 6 degrees of freedom, and every user can select any operation by using a button on the keyboard to cycle through them. To the cubes are assigned two different points of view: an exocentric view, where the user will stand at a given distance from the object, with a point of view similar to the one of a human being; and an egocentric view, where the user will stand much closer to the object, having the point of view from the object's perspective. These points of view are locked to each user, which means that one user cannot use both views, just the one assigned to his ID. The cameras have a small margin of movement, allowing just a tilt to the sides, according to the Oculus's movements. With these features, this 3DUI aims to test which point of view is better for each operation, and how the degrees of freedom should be separated between the users.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/YVZ7LMLT/Soares et al. - 2016 - Collaborative hybrid virtual environment.pdf;/home/nunoa/Zotero/storage/LJ4WEHZE/7460081.html}
}

@inproceedings{sousaVRRRRoomVirtualReality2017,
  title = {{{VRRRRoom}}: {{Virtual Reality}} for {{Radiologists}} in the {{Reading Room}}},
  shorttitle = {{{VRRRRoom}}},
  booktitle = {Proceedings of the 2017 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Sousa, Maur{\'i}cio and Mendes, Daniel and Paulo, Soraia and Matela, Nuno and Jorge, Joaquim and Lopes, Daniel Sim{\~o}es},
  year = {2017},
  month = may,
  series = {{{CHI}} '17},
  pages = {4057--4062},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3025453.3025566},
  urldate = {2023-10-12},
  abstract = {Reading room conditions such as illumination, ambient light, human factors and display luminance, play an important role on how radiologists analyze and interpret images. Indeed, serious diagnostic errors can appear when observing images through everyday monitors. Typically, these occur whenever professionals are ill-positioned with respect to the display or visualize images under improper light and luminance conditions. In this work, we show that virtual reality can assist radiodiagnostics by considerably diminishing or cancel out the effects of unsuitable ambient conditions. Our approach combines immersive head-mounted displays with interactive surfaces to support professional radiologists in analyzing medical images and formulating diagnostics. We evaluated our prototype with two senior medical doctors and four seasoned radiology fellows. Results indicate that our approach constitutes a viable, flexible, portable and cost-efficient option to traditional radiology reading rooms.},
  isbn = {978-1-4503-4655-9},
  keywords = {interaction design,medical visualization,multitouch surfaces,notion,virtual reality},
  file = {/home/nunoa/Zotero/storage/ZITQF4HG/Sousa et al. - 2017 - VRRRRoom Virtual Reality for Radiologists in the .pdf}
}

@inproceedings{sraProcedurallyGeneratedVirtual2016,
  title = {Procedurally Generated Virtual Reality from {{3D}} Reconstructed Physical Space},
  booktitle = {Proceedings of the 22nd {{ACM Conference}} on {{Virtual Reality Software}} and {{Technology}}},
  author = {Sra, Misha and {Garrido-Jurado}, Sergio and Schmandt, Chris and Maes, Pattie},
  year = {2016},
  month = nov,
  series = {{{VRST}} '16},
  pages = {191--200},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2993369.2993372},
  urldate = {2023-09-26},
  abstract = {We present a novel system for automatically generating immersive and interactive virtual reality (VR) environments using the real world as a template. The system captures indoor scenes in 3D, detects obstacles like furniture and walls, and maps walkable areas (WA) to enable real-walking in the generated virtual environment (VE). Depth data is additionally used for recognizing and tracking objects during the VR experience. The detected objects are paired with virtual counterparts to leverage the physicality of the real world for a tactile experience. Our approach is new, in that it allows a casual user to easily create virtual reality worlds in any indoor space of arbitrary size and shape without requiring specialized equipment or training. We demonstrate our approach through a fully working system implemented on the Google Project Tango tablet device.},
  isbn = {978-1-4503-4491-3},
  keywords = {3D reconstruction,computer vision,depth cameras,locomotion,mobile computing,notion,obstacle avoidance,procedural generation,tracking,virtual reality},
  file = {/home/nunoa/Zotero/storage/2JMPSC96/Sra et al. - 2016 - Procedurally generated virtual reality from 3D rec.pdf}
}

@inproceedings{thomasCommunicationFocusedFrameworkUnderstanding2023a,
  title = {A {{Communication-Focused Framework}} for {{Understanding Immersive Collaboration Experiences}}},
  booktitle = {2023 {{IEEE Conference}} on {{Virtual Reality}} and {{3D User Interfaces Abstracts}} and {{Workshops}} ({{VRW}})},
  author = {Thomas, Jerald and Lee, Sang Won and Giovannelli, Alexander and Lane, Logan and Bowman, Doug},
  year = {2023},
  month = mar,
  pages = {301--304},
  doi = {10.1109/VRW58643.2023.00070},
  urldate = {2023-10-22},
  abstract = {The ability to collaborate with other people across barriers created by time and/or space is one of the greatest features of modern communication. Immersive technologies are positioned to enhance this ability to collaborate even further. However, we do not have a firm understanding of how specific immersive technologies, or components thereof, alter the ability for two or more people to communicate, and hence collaborate. In this work-in-progress position paper, we propose a new framework for immersive collaboration experiences and provide an example of how it could be used to understand a hybrid collaboration among two co-located users and one remote user. We are seeking feedback from the community before conducting a formal evaluation of the framework. We also present some future work that this framework could facilitate.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/BW52QGFU/Thomas et al. - 2023 - A Communication-Focused Framework for Understandin.pdf;/home/nunoa/Zotero/storage/GH4DGZUK/10108770.html}
}

@inproceedings{tsengDarkSidePerceptual2022,
  title = {The {{Dark Side}} of {{Perceptual Manipulations}} in {{Virtual Reality}}},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Tseng, Wen-Jie and Bonnail, Elise and McGill, Mark and Khamis, Mohamed and Lecolinet, Eric and Huron, Samuel and Gugenheimer, Jan},
  year = {2022},
  month = apr,
  series = {{{CHI}} '22},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3491102.3517728},
  urldate = {2023-10-12},
  abstract = {``Virtual-Physical Perceptual Manipulations'' (VPPMs) such as redirected walking and haptics expand the user's capacity to interact with Virtual Reality (VR) beyond what would ordinarily physically be possible. VPPMs leverage knowledge of the limits of human perception to effect changes in the user's physical movements, becoming able to (perceptibly and imperceptibly) nudge their physical actions to enhance interactivity in VR. We explore the risks posed by the malicious use of VPPMs. First, we define, conceptualize and demonstrate the existence of VPPMs. Next, using speculative design workshops, we explore and characterize the threats/risks posed, proposing mitigations and preventative recommendations against the malicious use of VPPMs. Finally, we implement two sample applications to demonstrate how existing VPPMs could be trivially subverted to create the potential for physical harm. This paper aims to raise awareness that the current way we apply and publish VPPMs can lead to malicious exploits of our perceptual vulnerabilities.},
  isbn = {978-1-4503-9157-3},
  keywords = {notion,physical harm,virtual-physical perceptual manipulation,VPPM,VR security},
  file = {/home/nunoa/Zotero/storage/MBWXCCWC/Tseng et al. - 2022 - The Dark Side of Perceptual Manipulations in Virtu.pdf}
}

@article{ullahMultimodalInteractionCollaborative2011,
  title = {Multi-Modal {{Interaction}} in {{Collaborative Virtual Environments}}: {{Study}} and Analysis of Performance in Collaborative Work},
  shorttitle = {Multi-Modal {{Interaction}} in {{Collaborative Virtual Environments}}},
  author = {Ullah, Sehat},
  year = {2011},
  month = jan,
  abstract = {The recent advancement in the field of high quality computer graphics and the capability of inexpensive computers to render realistic 3D scenes have made it possible to develop virtual environments where two or more users can co-exist and work collaboratively to achieve a common goal. Such environments are called Collaborative Virtual Environments (CVEs). The potential application domains of CVEs are many, such as military, medical, assembling, computer aided designing, teleoperation, education, games and social networks etc.. One of the problems related to CVEs is the user's low level of awareness about the status, actions and intentions of his/her collaborator, which not only reduces users' performance but also leads to non satisfactory results. In addition, collaborative tasks without using any proper computer generated assistance are very dicult to perform and are more prone to errors. The basic theme of this thesis is to provide assistance in collaborative 3D interaction in CVEs. In this context, we study and develop the concept of multimodal (audio, visual and haptic) assistance of a user or group of users. Our study focuses on how we can assist users to collaboratively interact with the entities of CVEs. We propose here to study and analyze the contribution of multimodal assistance in collaborative (synchronous and asynchronous) interaction with objects in the virtual environment. Indeed, we propose and implement various multimodal virtual guides. These guides are evaluated through a series of experiments where selection/manipulation task is carried out by users both in synchronous and asynchronous mode. The experiments were carried out in LISA (Laboratoire d'Ing\'enierie et Syst\`emes Automatis\'es) lab at University of Angers and IBISC (Informatique, Biologie Integrative et Systemes Complexes) lab at University of Evry. In these experiments users were asked to perform a task under various conditions ( with and without guides). Analysis was done on the basis of task completion time, errors and users' learning. For subjective evaluations questionnaires were used. The ndings of this research work can contribute to the development of collaborative systems for teleoperation, assembly tasks, e-learning, rehabilitation, computer aided design and entertainment.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/A5BDBXY9/Ullah - 2011 - Multi-modal Interaction in Collaborative Virtual E.pdf}
}

@inproceedings{valkovReachPredictionUsing2023,
  title = {Reach {{Prediction}} Using {{Finger Motion Dynamics}}},
  booktitle = {Extended {{Abstracts}} of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Valkov, Dimitar and Kockwelp, Pascal and Daiber, Florian and Kr{\"u}ger, Antonio},
  year = {2023},
  month = apr,
  series = {{{CHI EA}} '23},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3544549.3585773},
  urldate = {2023-10-12},
  abstract = {The ability to predict the object the user intends to grasp or to recognize the one she is already holding offers essential contextual information and may help to leverage the effects of point-to-point latency in interactive environments. This paper investigates the feasibility and accuracy of recognizing un-instrumented objects based on hand kinematics during reach-to-grasp and transport actions. In a data collection study, we recorded the hand motions of 16 participants while reaching out to grasp and then moving real and synthetic objects. Our results demonstrate that even a simple LSTM network can predict the time point at which the user grasps an object with 23 ms precision and the current distance to it with a precision better than 1 cm. The target's size can be determined in advance with an accuracy better than 97\%. Our results have implications for designing adaptive and fine-grained interactive user interfaces in ubiquitous and mixed-reality environments.},
  isbn = {978-1-4503-9422-2},
  keywords = {datasets,grasp prediction,hand gesture,neural networks,notion}
}

@article{vinayagamoorthyEyeGazeModel2004,
  title = {An {{Eye Gaze Model}} for {{Dyadic Interaction}} in an {{Immersive Virtual Environment}}: {{Practice}} and {{Experience}}},
  shorttitle = {An {{Eye Gaze Model}} for {{Dyadic Interaction}} in an {{Immersive Virtual Environment}}},
  author = {Vinayagamoorthy, V. and Garau, M. and Steed, A. and Slater, M.},
  year = {2004},
  journal = {Computer Graphics Forum},
  volume = {23},
  number = {1},
  pages = {1--11},
  issn = {1467-8659},
  doi = {10.1111/j.1467-8659.2004.00001.x},
  urldate = {2023-10-22},
  abstract = {This paper describes a behavioural model used to simulate realistic eye-gaze behaviour and body animations for avatars representing participants in a shared immersive virtual environment (IVE). The model was used in a study designed to explore the impact of avatar realism on the perceived quality of communication within a negotiation scenario. Our eye-gaze model was based on data and studies carried out on the behaviour of eye-gaze during face-to-face communication. The technical features of the model are reported here. Information about the motivation behind the study, experimental procedures and a full analysis of the results obtained are given in [17].},
  langid = {english},
  keywords = {and virtual realities,augmented,avatars,behaviour modelling,co-presence,DIVE,embodiment,eye-gaze,H.5.1 Multimedia Information Systems\textemdash Artificial,human-computer interaction,I.3.7 General\textemdash Human Factors,I.3.7 Three-Dimensional Graphics and Realism\textemdash Animation,I.3.7 Three-Dimensional Graphics and Realism\textemdash Virtual Reality,immersive virtual environments,non-verbal behaviour,notion,real-time animation,virtual reality},
  file = {/home/nunoa/Zotero/storage/TGW5E4DI/j.1467-8659.2004.00001.html}
}

@inproceedings{wielandSeparationCompositionHybrid2021,
  title = {Separation, {{Composition}}, or {{Hybrid}}? \textendash{} {{Comparing Collaborative 3D Object Manipulation Techniques}} for {{Handheld Augmented Reality}}},
  shorttitle = {Separation, {{Composition}}, or {{Hybrid}}?},
  booktitle = {2021 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},
  author = {Wieland, Jonathan and Zagermann, Johannes and M{\"u}ller, Jens and Reiterer, Harald},
  year = {2021},
  month = oct,
  pages = {403--412},
  issn = {1554-7868},
  doi = {10.1109/ISMAR52148.2021.00057},
  urldate = {2023-11-09},
  abstract = {Augmented Reality (AR) supported collaboration is a popular topic in HCI research. Previous work has shown the benefits of collaborative 3D object manipulation and identified two possibilities: Either separate or compose users' inputs. However, their experimental comparison using handheld AR displays is still missing. We, therefore, conducted an experiment in which we tasked 24 dyads with collaboratively positioning virtual objects in handheld AR using three manipulation techniques: 1) Separation \textendash{} performing only different manipulation tasks (i. e., translation or rotation) simultaneously, 2) Composition \textendash{} performing only the same manipulation tasks simultaneously and combining individual inputs using a merge policy, and 3) Hybrid \textendash{} performing any manipulation tasks simultaneously, enabling dynamic transitions between Separation and Composition. While all techniques were similarly effective, Composition was least efficient, with higher subjective workload and worse user experience. Preferences were polarized between clear work division (Separation) and freedom of action (Hybrid). Based on our findings, we offer research and design implications.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/SGEK2XVC/Wieland et al. - 2021 - Separation, Composition, or Hybrid – Comparing Co.pdf;/home/nunoa/Zotero/storage/D4F9256H/9583823.html}
}

@inproceedings{wilsonPianoMoversProblem2013,
  title = {A "{{Piano Movers}}" {{Problem Reformulated}}},
  booktitle = {2013 15th {{International Symposium}} on {{Symbolic}} and {{Numeric Algorithms}} for {{Scientific Computing}}},
  author = {Wilson, David and Davenport, James H. and England, Matthew and Bradford, Russell},
  year = {2013},
  month = sep,
  pages = {53--60},
  doi = {10.1109/SYNASC.2013.14},
  urldate = {2023-10-26},
  abstract = {It has long been known that cylindrical algebraic decompositions (CADs) can in theory be used for robot motion planning. However, in practice even the simplest examples can be too complicated to tackle. We consider in detail a ``Piano Mover's Problem'' which considers moving an infinitesimally thin piano (or ladder) through a right-angled corridor. Producing a CAD for the original formulation of this problem is still infeasible after 25 years of improvements in both CAD theory and computer hardware. We review some alternative formulations in the literature which use differing levels of geometric analysis before input to a CAD algorithm. Simpler formulations allow CAD to easily address the question of the existence of a path. We provide a new formulation for which both a CAD can be constructed and from which an actual path could be determined if one exists, and analyse the CADs produced using this approach for variations of the problem. This emphasises the importance of the precise formulation of such problems for CAD. We analyse the formulations and their CADs considering a variety of heuristics and general criteria, leading to conclusions about tackling other problems of this form.},
  file = {/home/nunoa/Zotero/storage/SRDI64B8/Wilson et al. - 2013 - A Piano Movers Problem Reformulated.pdf;/home/nunoa/Zotero/storage/BXZCZY8C/6821131.html}
}

@article{wolffStudyEventTraffic2004a,
  title = {A {{Study}} of {{Event Traffic During}} the {{Shared Manipulation}} of {{Objects Within}} a {{Collaborative Virtual Environment}}},
  author = {Wolff, Robin and Roberts, David J. and Otto, Oliver},
  year = {2004},
  month = jun,
  journal = {Presence: Teleoperators and Virtual Environments},
  volume = {13},
  number = {3},
  pages = {251--262},
  doi = {10.1162/1054746041422280},
  urldate = {2023-10-25},
  abstract = {Event management must balance consistency and responsiveness above the requirements of shared object interaction within a Collaborative Virtual Environment (CVE) system. An understanding of the event traffic during collaborative tasks helps in the design of all aspects of a CVE system. The application, user activity, the display interface, and the network resources, all play a part in determining the characteristics of event management.Linked cubic displays lend themselves well to supporting natural social human communication between remote users. To allow users to communicate naturally and subconsciously, continuous and detailed tracking is necessary. This, however, is hard to balance with the real-time consistency constraints of general shared object interaction.This paper aims to explain these issues through a detailed examination of event traffic produced by a typical CVE, using both immersive and desktop displays, while supporting a variety of collaborative activities. We analyze event traffic during a highly collaborative task requiring various forms of shared object manipulation, including the concurrent manipulation of a shared object. Event sources are categorized and the influence of the form of object sharing as well as the display device interface are detailed. With the presented findings the paper wishes to aid the design of future systems.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/YQHRGARR/Wolff et al. - 2004 - A Study of Event Traffic During the Shared Manipul.pdf;/home/nunoa/Zotero/storage/IQAWTILI/A-Study-of-Event-Traffic-During-the-Shared.html}
}

@inproceedings{yangScalablePredictionBased2000,
  title = {Scalable Prediction Based Concurrency Control for Distributed Virtual Environments},
  booktitle = {Proceedings {{IEEE Virtual Reality}} 2000 ({{Cat}}. {{No}}.{{00CB37048}})},
  author = {Yang, Jeonghwa and Lee, Dongman},
  year = {2000},
  month = mar,
  pages = {151--158},
  issn = {1087-8270},
  doi = {10.1109/VR.2000.840493},
  urldate = {2023-11-09},
  abstract = {Replication is often used to provide users of distributed virtual environments with high-performance interactions. Concurrency control is required to avoid inconsistent views among replicas due to multiple concurrent updates. G. Lann has developed a prediction-based concurrency control scheme to allow real-time interactions for users and to eliminate the need for repairs. The existing scheme does not scale in terms of delivering ownership on time as the number of users increases. In this paper, we propose a scalable prediction-based concurrency control scheme with entity-centric multicasting: only the users surrounding a target entity multicast the ownership requests, by using the multicast address assigned to the entity. The experimental results and analysis reported in this paper show that the proposed scheme achieves the benefits of prediction-based concurrency control with efficiency and scalability for large distributed virtual environments.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/CHFW27AY/Yang and Lee - 2000 - Scalable prediction based concurrency control for .pdf;/home/nunoa/Zotero/storage/8CZNJMXP/840493.html}
}

@article{yuanMEinVRMultimodalInteraction2023,
  title = {{{MEinVR}}: {{Multimodal}} Interaction Techniques in Immersive Exploration},
  shorttitle = {{{MEinVR}}},
  author = {Yuan, Ziyue and He, Shuqi and Liu, Yu and Yu, Lingyun},
  year = {2023},
  month = sep,
  journal = {Visual Informatics},
  volume = {7},
  number = {3},
  pages = {37--48},
  issn = {2468-502X},
  doi = {10.1016/j.visinf.2023.06.001},
  urldate = {2023-10-12},
  abstract = {Immersive environments have become increasingly popular for visualizing and exploring large-scale, complex scientific data because of their key features: immersion, engagement, and awareness. Virtual reality offers numerous new interaction possibilities, including tactile and tangible interactions, gestures, and voice commands. However, it is crucial to determine the most effective combination of these techniques for a more natural interaction experience. In this paper, we present MEinVR, a novel multimodal interaction technique for exploring 3D molecular data in virtual reality. MEinVR combines VR controller and voice input to provide a more intuitive way for users to manipulate data in immersive environments. By using the VR controller to select locations and regions of interest and voice commands to perform tasks, users can efficiently perform complex data exploration tasks. Our findings provide suggestions for the design of multimodal interaction techniques in 3D data exploration in virtual reality.},
  keywords = {Multimodal interaction,notion,Scientific visualization,Virtual reality},
  file = {/home/nunoa/Zotero/storage/DXGYNZPN/Yuan et al. - 2023 - MEinVR Multimodal interaction techniques in immer.pdf;/home/nunoa/Zotero/storage/FKZ8S5W8/S2468502X23000190.html}
}

@inproceedings{zamanVicariousContextawareViewpoints2023,
  title = {Vicarious: {{Context-aware Viewpoints Selection}} for {{Mixed Reality Collaboration}}},
  shorttitle = {Vicarious},
  booktitle = {Proceedings of the 29th {{ACM Symposium}} on {{Virtual Reality Software}} and {{Technology}}},
  author = {Zaman, Faisal and Anslow, Craig and Rhee, Taehyun James},
  year = {2023},
  month = oct,
  series = {{{VRST}} '23},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3611659.3615709},
  urldate = {2023-10-19},
  abstract = {Mixed-perspective, combining egocentric (first-person) and exocentric (third-person) viewpoints, have been shown to improve the collaborative experience in remote settings. Such experiences allow remote users to switch between different viewpoints to gain alternative perspectives of the remote space. However, existing systems lack seamless selection and transition between multiple perspectives that better fit the task at hand. To address this, we present a new approach called Vicarious, which simplifies and automates the selection between egocentric and exocentric viewpoints. Vicarious employs a context-aware method for dynamically switching or highlighting the optimal viewpoint based on user actions and the current context. To evaluate the effectiveness of the viewpoint selection method, we conducted a user study (n = 27) using an asymmetric AR-VR setup where users performed remote collaboration tasks under four distinct conditions: No-view, Manual, Guided, and Automatic selection. The results showed that Guided and Automatic viewpoint selection improved users' understanding of the task space and task performance, and reduced cognitive load compared to Manual or No-view selection. The results also suggest that the asymmetric setup had minimal impact on spatial and social presence, except for differences in task load and preference. Based on these findings, we provide design implications for future research in mixed reality collaboration.},
  isbn = {9798400703287},
  keywords = {360-degree Panoramic Video,Mixed Reality,notion,Perspective Sharing.,Remote Collaboration,Telepresence,Viewpoint Sharing},
  file = {/home/nunoa/Zotero/storage/B2GTUADS/Zaman et al. - 2023 - Vicarious Context-aware Viewpoints Selection for .pdf}
}

@inproceedings{zielaskoNonStationaryOfficeDesk2019,
  title = {A {{Non-Stationary Office Desk Substitution}} for {{Desk-Based}} and {{HMD-Projected Virtual Reality}}},
  booktitle = {2019 {{IEEE Conference}} on {{Virtual Reality}} and {{3D User Interfaces}} ({{VR}})},
  author = {Zielasko, Daniel and Weyers, Benjamin and Kuhlen, Torsten W.},
  year = {2019},
  month = mar,
  pages = {1884--1889},
  issn = {2642-5254},
  doi = {10.1109/VR.2019.8797837},
  urldate = {2023-11-01},
  abstract = {The ongoing migration of HMDs to the consumer market also allows the integration of immersive environments into analysis workflows that are often bound to an (office) desk. However, a critical factor when considering VR solutions for professional applications is the prevention of cybersickness. In the given scenario the user is usually seated and the surrounding real world environment is very dominant, where the most dominant part is maybe the desk itself. Including this desk in the virtual environment could serve as a resting frame and thus reduce cybersickness next to a lot of further possibilities. In this work, we evaluate the feasibility of a substitution like this in the context of a visual data analysis task involving travel, and measure the impact on cybersickness as well as the general task performance and presence. In the conducted user study ( n=52), surprisingly, and partially in contradiction to existing work, we found no significant differences for those core measures between the control condition without a virtual table and the condition containing a virtual table. However, the results also support the inclusion of a virtual table in desk-based use cases.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/XYA7DMTP/Zielasko et al. - 2019 - A Non-Stationary Office Desk Substitution for Desk.pdf;/home/nunoa/Zotero/storage/7JNMCMR9/8797837.html}
}

@inproceedings{zielaskoRemainSeatedFullyimmersive2017,
  title = {Remain Seated: Towards Fully-Immersive Desktop {{VR}}},
  shorttitle = {Remain Seated},
  booktitle = {2017 {{IEEE}} 3rd {{Workshop}} on {{Everyday Virtual Reality}} ({{WEVR}})},
  author = {Zielasko, Daniel and Weyers, Benjamin and Bellgardt, Martin and Pick, Sebastian and Meibner, Alexander and Vierjahn, Tom and Kuhlen, Torsten W.},
  year = {2017},
  month = mar,
  pages = {1--6},
  doi = {10.1109/WEVR.2017.7957707},
  urldate = {2023-11-01},
  abstract = {In this work we describe the scenario of fully-immersive desktop VR, which serves the overall goal to seamlessly integrate with existing workflows and workplaces of data analysts and researchers, such that they can benefit from the gain in productivity when immersed in their data-spaces. Furthermore, we provide a literature review showing the status quo of techniques and methods available for realizing this scenario under the raised restrictions. Finally, we propose a concept of an analysis framework and the decisions made and the decisions still to be taken, to outline how the described scenario and the collected methods are feasible in a real use case.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/J3E99PFW/Zielasko et al. - 2017 - Remain seated towards fully-immersive desktop VR.pdf;/home/nunoa/Zotero/storage/5BBBGJXE/7957707.html}
}

@article{zielaskoSitNotSit2021,
  title = {To {{Sit}} or {{Not}} to {{Sit}} in {{VR}}: {{Analyzing Influences}} and ({{Dis}}){{Advantages}} of {{Posture}} and {{Embodied Interaction}}},
  shorttitle = {To {{Sit}} or {{Not}} to {{Sit}} in {{VR}}},
  author = {Zielasko, Daniel and Riecke, Bernhard E.},
  year = {2021},
  month = jun,
  journal = {Computers},
  volume = {10},
  number = {6},
  pages = {73},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2073-431X},
  doi = {10.3390/computers10060073},
  urldate = {2023-10-12},
  abstract = {Virtual Reality (VR) users typically either sit or stand/walk when using VR; however, the impact of this is little researched, and there is a lack of any broad or systematic analysis of how this difference in physical posture might affect user experience and behavior. To address this gap, we propose such a systematic analysis that was refined through discussions and iterations during a dedicated workshop with VR experts. This analysis was complemented by an online survey to integrate the perspectives of a larger and more diverse group of VR experts, including developers and power users. The result is a validated expert assessment of the impact of posture and degree of embodiment on the most relevant aspects of VR experience and behavior. In particular, we posit potential strong effects of posture on user comfort, safety, self-motion perception, engagement, and accessibility. We further argue that the degree of embodiment can strongly impact cybersickness, locomotion precision, safety, self-motion perception, engagement, technical complexity, and accessibility. We provide a compact visualization of key findings and discuss areas where posture and embodiment do or do not have a known influence, as well as highlight open questions that could guide future research and VR design efforts.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {embodiment,locomotion,notion,posture,survey,systematic analysis,virtual reality},
  file = {/home/nunoa/Zotero/storage/V4KU82AQ/Zielasko and Riecke - 2021 - To Sit or Not to Sit in VR Analyzing Influences a.pdf}
}

@article{singhBrickNetSoftwareToolkit1994,
  title = {{{BrickNet}}: {{A Software Toolkit}} for {{Network-Based Virtual Worlds}}},
  shorttitle = {{{BrickNet}}},
  author = {Singh, Gurminder and Serra, Luis and Png, Willie and Ng, Hern},
  year = {1994},
  month = feb,
  journal = {Presence: Teleoperators and Virtual Environments},
  volume = {3},
  number = {1},
  pages = {19--34},
  doi = {10.1162/pres.1994.3.1.19},
  urldate = {2023-11-21},
  abstract = {Network-based virtual worlds allow multiple virtual worlds connected on a network to share information with one another. The development effort required to produce a network-based virtual world is quite large. The BrickNet toolkit simplifies this development by providing the standard facilities required by a wide range of network-based virtual worlds. It provides support for graphical, behavioral and network modeling of virtual worlds in an object-oriented fashion. BrickNet enables graphic objects to be maintained, managed, and used efficiently, and permits objects to be shared by multiple virtual worlds. In this paper, the architecture and implementation of BrickNet are described.},
  keywords = {notion},
  file = {/home/nunoa/Zotero/storage/WVUWN9KN/BrickNet-A-Software-Toolkit-for-Network-Based.html}
}

@article{adamsSituationAwarenessCognitive1995,
  title = {Situation {{Awareness}} and the {{Cognitive Management}} of {{Complex Systems}}},
  author = {Adams, Marilyn Jager and Tenney, Yvette J. and Pew, Richard W.},
  year = {1995},
  month = mar,
  journal = {Human Factors},
  volume = {37},
  number = {1},
  pages = {85--104},
  publisher = {{SAGE Publications Inc}},
  issn = {0018-7208},
  doi = {10.1518/001872095779049462},
  urldate = {2023-12-14},
  abstract = {The issue of how to support situation awareness among operators of complex systems or vehicles is a growing concern in a number of industries, especially when automation takes the operators partly "out of the loop." Cognitive theory suggests that comprehension of the flow of events is an active process, constrained by the dynamics and modularity of attention and memory. Focusing on issues of commercial aviation, we review the meaning of the term situation awareness both definition ally and by way of examples. We then discuss the cognitive factors involved in achieving and maintaining situation awareness, drawing on recent cognitive theory to clarify their interactions. Finally, having emphasized the need for a more analytic and precise understanding of situation awareness, we discuss strengths and weaknesses of various empirical approaches that might be used toward that end.},
  langid = {english},
  keywords = {notion}
}

@article{endsleyTheorySituationAwareness1995,
  title = {Toward a {{Theory}} of {{Situation Awareness}} in {{Dynamic Systems}}},
  author = {Endsley, Mica R.},
  year = {1995},
  month = mar,
  journal = {Human Factors},
  volume = {37},
  number = {1},
  pages = {32--64},
  publisher = {{SAGE Publications Inc}},
  issn = {0018-7208},
  doi = {10.1518/001872095779049543},
  urldate = {2023-12-14},
  abstract = {This paper presents a theoretical model of situation awareness based on its role in dynamic human decision making in a variety of domains. Situation awareness is presented as a predominant concern in system operation, based on a descriptive view of decision making. The relationship between situation awareness and numerous individual and environmental factors is explored. Among these factors, attention and working memory are presented as critical factors limiting operators from acquiring and interpreting information from the environment to form situation awareness, and mental models and goal-directed behavior are hypothesized as important mechanisms for overcoming these limits. The impact of design features, workload, stress, system complexity, and automation on operator situation awareness is addressed, and a taxonomy of errors in situation awareness is introduced, based on the model presented. The model is used to generate design implications for enhancing operator situation awareness and future directions for situation awareness research.},
  langid = {english},
  keywords = {notion}
}

@book{normanThingsThatMake1993,
  title = {Things That Make Us Smart: Defending Human Attributes in the Age of the Machine},
  shorttitle = {Things That Make Us Smart},
  author = {Norman, Donald A.},
  year = {1993},
  publisher = {{Addison-Wesley Longman Publishing Co., Inc.}},
  address = {{USA}},
  isbn = {978-0-201-62695-7},
  keywords = {notion}
}

@article{gabaSituationAwarenessAnesthesiology1995,
  title = {Situation {{Awareness}} in {{Anesthesiology}}},
  author = {Gaba, David M. and Howard, Steven K. and Small, Stephen D.},
  year = {1995},
  month = mar,
  journal = {Human Factors},
  volume = {37},
  number = {1},
  pages = {20--31},
  publisher = {{SAGE Publications Inc}},
  issn = {0018-7208},
  doi = {10.1518/001872095779049435},
  urldate = {2023-12-14},
  abstract = {Situation awareness has primarily been confined to the aviation field. We believe that situation awareness is an equally important characteristic in the complex, dynamic, and risky field of anesthesiology. We describe three aspects of situations of which the decision maker must remain aware: subtle cues, evolving situations, and special knowledge elements. We provide examples of real or simulated anesthesia situations in which situation awareness is clearly involved in the provision of optimal patient care, and we map the elements of situation awareness onto a cognitive process model of the anesthesiologist. Finally, we consider how situation awareness can be further investigated and taught in this medical domain using anesthesia simulators and analyses of real cases. The study of situation awareness in anesthesiology may provide a good example of the wider application of the concept of situation awareness to nonaerospace environments.},
  langid = {english},
  keywords = {notion}
}

@inproceedings{dourishAwarenessCoordinationShared1992,
  title = {Awareness and Coordination in Shared Workspaces},
  booktitle = {Proceedings of the 1992 {{ACM}} Conference on {{Computer-supported}} Cooperative Work},
  author = {Dourish, Paul and Bellotti, Victoria},
  year = {1992},
  month = dec,
  series = {{{CSCW}} '92},
  pages = {107--114},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/143457.143468},
  urldate = {2023-12-17},
  isbn = {978-0-89791-542-7},
  keywords = {awareness,coordination,information sharing,shared feedback,shared workspaces},
  file = {/home/nunoa/Zotero/storage/E2A94KRJ/Dourish and Bellotti - 1992 - Awareness and coordination in shared workspaces.pdf}
}

@inproceedings{mcdanielAwarenessCollaborativeSystems1997,
  title = {Awareness in {{Collaborative Systems}}: {{A CHI}} 97 {{Workshop}}},
  booktitle = {The {{SIGCHI Bulletin}}},
  author = {McDaniel, Susan E. and Brinck, Tom},
  year = {1997},
  month = oct,
  volume = {29},
  urldate = {2023-12-17},
  file = {/home/nunoa/Zotero/storage/ZSAK3WFD/mcdaniel.html}
}

@inproceedings{sohlenkampIntegratingCommunicationCooperation1994,
  title = {Integrating Communication, Cooperation, and Awareness: The {{DIVA}} Virtual Office Environment},
  shorttitle = {Integrating Communication, Cooperation, and Awareness},
  booktitle = {Proceedings of the 1994 {{ACM}} Conference on {{Computer}} Supported Cooperative Work},
  author = {Sohlenkamp, Markus and Chwelos, Greg},
  year = {1994},
  month = oct,
  series = {{{CSCW}} '94},
  pages = {331--343},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/192844.193041},
  urldate = {2023-12-17},
  abstract = {DIVA, a novel environment for group work, is presented. This prototype virtual office environment provides support for communication, cooperation, and awareness in both the synchronous and asynchronous modes, smoothly integrated into a simple and intuitive interface which may be viewed as a replacement for the standard graphical user interface desktop. In order to utilize the skills that people have acquired through years of shared work in real offices, DIVA is modeled after the standard office, abstracting elements of physical offices required to support collaborative work: people, rooms, desks, and documents.},
  isbn = {978-0-89791-689-9},
  keywords = {awareness,CSCW,groupware,integration,synchronous/asynchronous,virtual office},
  file = {/home/nunoa/Zotero/storage/DV7KCCUB/Sohlenkamp and Chwelos - 1994 - Integrating communication, cooperation, and awaren.pdf}
}

@book{dixHumanComputerInteraction2003,
  title = {Human-{{Computer Interaction}}},
  author = {Dix, Alan and Finlay, Janet E. and Abowd, Gregory D. and Beale, Russell},
  year = {2003},
  month = sep,
  edition = {3},
  publisher = {{Pearson}},
  urldate = {2023-12-17},
  isbn = {978-0-13-046109-4},
  file = {/home/nunoa/Zotero/storage/P6DWK2KV/0130461091.html}
}

@incollection{hutchinsTechnologyTeamNavigation1990,
  title = {The {{Technology}} of {{Team Navigation}}},
  booktitle = {Intellectual {{Teamwork}}: {{Social}} and {{Technological Foundations}} of {{Cooperative Work}}},
  author = {Hutchins, E.},
  editor = {Galegher, Jolene and Kraut, Robert E. and Egido, Carmen},
  year = {1990},
  month = jun,
  edition = {1},
  pages = {552},
  publisher = {{Routledge}},
  urldate = {2023-12-17},
  isbn = {978-0-8058-0534-5},
  langid = {english},
  file = {/home/nunoa/Zotero/storage/XZL7U4I8/9780805805345.html}
}

@techreport{segalEffectsChecklistInterface1994,
  title = {Effects of Checklist Interface on Non-Verbal Crew Communications},
  author = {Segal, Leon D.},
  year = {1994},
  month = may,
  number = {NASA-CR-177639},
  urldate = {2023-12-17},
  abstract = {The investigation looked at the effects of the spatial layout and functionality of cockpit displays and controls on crew communication. Specifically, the study focused on the intra-cockpit crew interaction, and subsequent task performance, of airline pilots flying different configurations of a new electronic checklist, designed and tested in a high-fidelity simulator at NASA Ames Research Center. The first part of this proposal establishes the theoretical background for the assumptions underlying the research, suggesting that in the context of the interaction between a multi-operator crew and a machine, the design and configuration of the interface will affect interactions between individual operators and the machine, and subsequently, the interaction between operators. In view of the latest trends in cockpit interface design and flight-deck technology, in particular, the centralization of displays and controls, the introduction identifies certain problems associated with these modern designs and suggests specific design issues to which the expected results could be applied. A detailed research program and methodology is outlined and the results are described and discussed. Overall, differences in cockpit design were shown to impact the activity within the cockpit, including interactions between pilots and aircraft and the cooperative interactions between pilots.},
  keywords = {Air Transportation And Safety},
  annotation = {NTRS Author Affiliations: Western Aerospace Labs., Inc. NTRS Document ID: 19940030409 NTRS Research Center: Legacy CDMS (CDMS)},
  file = {/home/nunoa/Zotero/storage/H76J63RL/Segal - 1994 - Effects of checklist interface on non-verbal crew .pdf;/home/nunoa/Zotero/storage/N86DX2K3/19940030409.html}
}

@incollection{gaverSoundSupportCollaboration1991,
  title = {Sound {{Support For Collaboration}}},
  booktitle = {Proceedings of the {{Second European Conference}} on {{Computer-Supported Cooperative Work ECSCW}} '91},
  author = {Gaver, William W.},
  editor = {Bannon, Liam and Robinson, Mike and Schmidt, Kjeld},
  year = {1991},
  pages = {293--308},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-011-3506-1_22},
  urldate = {2023-12-17},
  abstract = {Shared work often involves fluid transitions between relatively focussed collaboration, division of labour, general awareness and serendipitous communication. This leads to a tension in the design of software systems meant to support shared work: focussed collaboration implies the need to coordinate people's views of work objects, while division of labour requires individual control over views. A similar tension exists in the office environment as well: group engagement in the workplace depends on a shared context, but individual work is facilitated by privacy and freedom of action. Auditory cues have the potential to reduce these tensions because graphics and sound can provide two independent ways to present and obtain information. I illustrate the potential of sound in collaborative systems with observations drawn from two systems: the ARKola simulation, which explores the effects of sound on collaboration within a workstation environment; and EAR, in which auditory cues are used to increase general awareness of events and encourage group engagement within the workplace itself. These examples suggest useful functions sound can play in collaborative systems.},
  isbn = {978-94-011-3506-1},
  langid = {english},
  keywords = {Auditory Feedback,Group Engagement,Individual Work,Office Environment,Visual Attention}
}

@book{birdwhistellIntroductionKinesicsAnnotation1952,
  title = {Introduction to {{Kinesics}}: (An {{Annotation System}} for {{Analysis}} of {{Body Motion}} and {{Gesture}})},
  shorttitle = {Introduction to {{Kinesics}}},
  author = {Birdwhistell, Ray L.},
  year = {1952},
  publisher = {{Department of State, Foreign Service Institute}},
  googlebooks = {Ad99AAAAMAAJ},
  langid = {english}
}

@book{clarkUsingLanguage1996,
  title = {Using {{Language}}},
  author = {Clark, Herbert H.},
  year = {1996},
  series = {'{{Using}}' {{Linguistic Books}}},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511620539},
  urldate = {2023-12-17},
  abstract = {This book, first published in 1996, argues that language use is more than the sum of a speaker speaking and a listener listening. It is the joint action that emerges when speakers and listeners - writers and readers - perform their individual actions in coordination, as ensembles. The author argues strongly that language use embodies both individual and social processes.},
  isbn = {978-0-521-56158-7},
  file = {/home/nunoa/Zotero/storage/RLANIM52/4E7EBC4EC742C26436F6CF187C43F239.html}
}

@article{heathUnpackingCollaborationInteractional1994,
  title = {Unpacking Collaboration: The Interactional Organisation of Trading in a City Dealing Room},
  shorttitle = {Unpacking Collaboration},
  author = {Heath, Christian and Jirotka, Marina and Luff, Paul and Hindmarsh, Jon},
  year = {1994},
  month = jun,
  journal = {Computer Supported Cooperative Work (CSCW)},
  volume = {3},
  number = {2},
  pages = {147--165},
  issn = {1573-7551},
  doi = {10.1007/BF00773445},
  urldate = {2023-12-17},
  abstract = {It is has been widely recognised that whilst CSCW has led to a number of impressive technological developments, examples of successful applications remain few. In part, this may be due to our relative ignorance of the organisation of real world, cooperative activity. Focusing on share trading in a securities house in the City of London, we explore the interactional organisation of particular tasks and the ways in whcih dealers interweave individual and collaborative activity. These observations suggest ways in which we might reconsider a number of central concepts in CSCW and begin. to draw design implications from naturalistic studies of work and interaction.},
  langid = {english},
  keywords = {design,Interaction,work},
  file = {/home/nunoa/Zotero/storage/ISF8P5BV/Heath et al. - 1994 - Unpacking collaboration the interactional organis.pdf}
}

@article{salvadorDenverModelGroupware1996,
  title = {The {{Denver}} Model for Groupware Design},
  author = {Salvador, Tony and Scholtz, Jean and Larson, James},
  year = {1996},
  month = jan,
  journal = {ACM SIGCHI Bulletin},
  volume = {28},
  number = {1},
  pages = {52--58},
  issn = {0736-6906},
  doi = {10.1145/249170.249185},
  urldate = {2023-12-18},
  abstract = {The Denver Model is offered as a framework with which to plan or evaluate the capabilities associated with a particular groupware application. This model was the output of 14 participants at the two day workshop on Designing and Evaluating Groupware, held at CHI'95, Denver Colorado. The Denver Model consists of three submodels: goals and requirements, design and technology. A description of the framework is provided and evaluation strategies are described in this paper.},
  file = {/home/nunoa/Zotero/storage/QLVTSWPT/Salvador et al. - 1996 - The Denver model for groupware design.pdf}
}

@incollection{benfordSpatialModelInteraction1993,
  title = {A {{Spatial Model}} of {{Interaction}} in {{Large Virtual Environments}}},
  booktitle = {Proceedings of the {{Third European Conference}} on {{Computer-Supported Cooperative Work}} 13{\textendash}17 {{September}} 1993, {{Milan}}, {{Italy ECSCW}} '93},
  author = {Benford, Steve and Fahl{\'e}n, Lennart},
  editor = {{de Michelis}, Giorgio and Simone, Carla and Schmidt, Kjeld},
  year = {1993},
  pages = {109--124},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-011-2094-4_8},
  urldate = {2023-12-18},
  abstract = {We present a spatial model of group interaction in virtual environments. The model aims to provide flexible and natural support for managing conversations among large groups gathered in virtual space. However, it can also be used to control more general interactions among other kinds of objects inhabiting such spaces. The model defines the key abstractions of object aura, nimbus, focus and adapters to control mutual levels of awareness. Furthermore, these are defined in a sufficiently general way so as to apply to any CSCW system where a spatial metric can be identified {\textemdash} i.e. a way of measuring position and direction. Several examples are discussed, including virtual reality and text conferencing applications. Finally, the paper provides a more formal computational architecture for the spatial model by relating it to the object oriented modelling approach for distributed systems.},
  isbn = {978-94-011-2094-4},
  langid = {english},
  keywords = {Object Management Group,Spatial Model,Virtual Environment,Virtual Reality,Virtual Space}
}

@inproceedings{otmaneCollaborative3DInteraction2007,
  title = {Towards a {{Collaborative 3D Interaction Model}} for {{Cooperative Design}} in {{Virtual Environments}}},
  booktitle = {2007 11th {{International Conference}} on {{Computer Supported Cooperative Work}} in {{Design}}},
  author = {Otmane, Samir and {Ouramdane-Djerah}, Nassima and Mallem, Malik},
  year = {2007},
  month = apr,
  pages = {198--203},
  doi = {10.1109/CSCWD.2007.4281434},
  urldate = {2023-12-18},
  abstract = {Design of complex systems requires multi-user cooperation with virtual reality technologies to provide 3D interactions between users and virtual objects. Recent advances in both virtual reality (VR) systems and computer-supported cooperative work (CSCW) technologies have resulted in a convergence of the appearance of the collaborative virtual environments (CVEs) systems supporting different forms of collaboration and interaction between users. The collaboration in these systems refers to the simultaneous interactions (collaborative interaction) of multiple users on a virtual object in an immersive or semi-immersive virtual environment. In this paper, we propose a method to modeling collaborative 3D interactions that supports group interaction in CVEs. The proposed method is based on group awareness concepts (focus and nimbus) combined astutely with 3D interaction paradigms (navigation, selection and manipulation) to provide us a collaborative 3D interaction model needed to design an adaptive workflow for coordination of multi-user 3D interactions in CVEs.}
}

@inproceedings{greenbergRealTimeGroupware1994,
  title = {Real Time Groupware as a Distributed System: Concurrency Control and Its Effect on the Interface},
  shorttitle = {Real Time Groupware as a Distributed System},
  booktitle = {Proceedings of the 1994 {{ACM}} Conference on {{Computer}} Supported Cooperative Work},
  author = {Greenberg, Saul and Marwood, David},
  year = {1994},
  month = oct,
  series = {{{CSCW}} '94},
  pages = {207--217},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/192844.193011},
  urldate = {2023-12-23},
  abstract = {This paper exposes the concurrency control problem in groupware when it is implemented as a distributed system. Traditional concurrency control methods cannot be applied directly to groupware because system interactions include people as well as computers. Methods, such as locking, serialization, and their degree of optimism, are shown to have quite different impacts on the interface and how operations are displayed and perceived by group members. The paper considers both human and technical considerations that designers should ponder before choosing a particular concurrency control method. It also reviews our work-in-progress designing and implementing a library of concurrency schemes in GROUPKIT, a groupware toolkit.},
  isbn = {978-0-89791-689-9},
  keywords = {computer supported cooperative work,concurrency control algorithms,distributed systems,real time groupware},
  file = {/home/nunoa/Zotero/storage/5WBYH7GQ/Greenberg and Marwood - 1994 - Real time groupware as a distributed system concu.pdf}
}

@article{hagsandInteractiveMultiuserVEs1996,
  title = {Interactive Multiuser {{VEs}} in the {{DIVE}} System},
  author = {Hagsand, O.},
  year = {1996},
  journal = {IEEE MultiMedia},
  volume = {3},
  number = {1},
  pages = {30--39},
  issn = {1941-0166},
  doi = {10.1109/93.486702},
  urldate = {2023-12-23},
  abstract = {Multiuser virtual environments (VEs) raise challenging research questions concerning how users interact with objects, applications, and other users, and how distributed VEs behave when the number of users increases. The Distributed Interactive Virtual Environment (DIVE) is a software platform for multiuser VEs that has served as a toolkit for many distributed VE applications. It emphasizes networking and human-computer interaction and supports autonomous behavior-driven objects, collision detection, and audio and 3D navigation.}
}

@inproceedings{leaIssuesDesignScalable1997,
  title = {Issues in the Design of a Scalable Shared Virtual Environment for the {{Internet}}},
  booktitle = {Proceedings of the {{Thirtieth Hawaii International Conference}} on {{System Sciences}}},
  author = {Lea, R. and Honda, Y. and Matsuda, K. and Hagsand, O. and Stenius, M.},
  year = {1997},
  month = jan,
  volume = {1},
  pages = {653-662 vol.1},
  issn = {1060-3425},
  doi = {10.1109/HICSS.1997.667432},
  urldate = {2023-12-23},
  abstract = {Building a distributed virtual environment that scales to many participants in low bandwidth, high latency networks is a technical challenge. The key issues are maintaining acceptable performance in the face of high latency links, and maintaining consistency of shared world data between multiple participants. The paper describes the overall architecture that enables one to build such a wide area, shared virtual environment targeted to the Internet. The architecture relies on spatial partitioning of the shared scene to reduce communication, replication to hide latency, and group communications to maintain replica consistency. The paper discusses the generic architecture, the key issues that must be solved and then presents two implementations of that architecture and gives performance results from one of those implementations.},
  file = {/home/nunoa/Zotero/storage/YZD7ELK9/Lea et al. - 1997 - Issues in the design of a scalable shared virtual .pdf}
}

@article{leighCAVERNDistributedArchitecture1997,
  title = {{{CAVERN}}: {{A}} Distributed Architecture for Supporting Scalable Persistence and Interoperability in Collaborative Virtual Environments},
  shorttitle = {{{CAVERN}}},
  author = {Leigh, Jason and Johnson, Andrew and Defanti, Thomas},
  year = {1997},
  month = dec,
  journal = {Virtual Reality: Research, Development and Applications},
  volume = {2},
  pages = {217--237},
  abstract = {CAVERN, the CAVE Research Network, is an alliance of industrial and research institutions equipped with CAVE s, ImmersaDesks, and high-performance computing resources all interconnected by high-speed networks to support collaboration in design, training, scientific visualization, and computational steering, in virtual reality. CAVERNsoft is the common collaborative software architecture for CAVERN. CAVERNsoft uses light-weight distributed data-stores as the mechanism for managing a wide range of data volumes (from a few bytes to several terrabytes) that are typically needed for sustaining persistence in virtual environments. Multiple networking interfaces support customizable-latency, data consistency, and scalability that is needed to support a broad spectrum of networking requirements. This paper begins by describing a number of collaborative virtual reality scenarios developed by our laboratory as well as our collaborators, with the goal of identifying the key issues of networking and database management that are unique to collaborative virtual reality. From these the CAVERNsoft architecture is presented including a report on the current status of CAVERNsoft's development.},
  file = {/home/nunoa/Zotero/storage/KHTSV67G/Leigh et al. - 1997 - CAVERN A distributed architecture for supporting .pdf}
}

@article{singhBrickNetSoftwareToolkit1994a,
  title = {{{BrickNet}}: {{A Software Toolkit}} for {{Network-Based Virtual Worlds}}},
  shorttitle = {{{BrickNet}}},
  author = {Singh, Gurminder and Serra, Luis and Png, Willie and Ng, Hern},
  year = {1994},
  month = feb,
  journal = {Presence: Teleoperators and Virtual Environments},
  volume = {3},
  number = {1},
  pages = {19--34},
  doi = {10.1162/pres.1994.3.1.19},
  urldate = {2023-12-23},
  abstract = {Network-based virtual worlds allow multiple virtual worlds connected on a network to share information with one another. The development effort required to produce a network-based virtual world is quite large. The BrickNet toolkit simplifies this development by providing the standard facilities required by a wide range of network-based virtual worlds. It provides support for graphical, behavioral and network modeling of virtual worlds in an object-oriented fashion. BrickNet enables graphic objects to be maintained, managed, and used efficiently, and permits objects to be shared by multiple virtual worlds. In this paper, the architecture and implementation of BrickNet are described.},
  file = {/home/nunoa/Zotero/storage/MIAIYP6W/BrickNet-A-Software-Toolkit-for-Network-Based.html}
}

@inproceedings{sungConcurrencyControlCIAO1999,
  title = {Concurrency Control in {{CIAO}}},
  booktitle = {Proceedings {{IEEE Virtual Reality}} ({{Cat}}. {{No}}. {{99CB36316}})},
  author = {Sung, Un-Jae and Yang, Jae-Heon and Wohn, Kwang-Yun},
  year = {1999},
  month = mar,
  pages = {22--28},
  issn = {1087-8270},
  doi = {10.1109/VR.1999.756919},
  urldate = {2023-12-23},
  abstract = {This paper is concerned with the concurrency control for collaborative virtual environments. In particular, we describe how concurrent actions are coordinated in a multi-user, large-scale 3-D layout system CIAO. In contrast to many existing systems that sacrifice responsiveness in order to maintain consistency, CIAO achieves optimal response and notification time without compromising consistency. The optimal responsiveness is achieved by a new multicast-based, optimistic concurrency control mechanism. Even operations on a group of related objects do not entail any latency for concurrency control. We also present the multi-user interfaces of CIAO that provide some sense of isolation as well as rich awareness.},
  file = {/home/nunoa/Zotero/storage/D2YWAZNM/Sung et al. - 1999 - Concurrency control in CIAO.pdf;/home/nunoa/Zotero/storage/PFKBP6U8/756919.html}
}

@inproceedings{watersDesignInteractiveSharing1997,
  title = {Design of the {{Interactive Sharing Transfer Protocol}}},
  booktitle = {Proceedings of {{IEEE}} 6th {{Workshop}} on {{Enabling Technologies}}: {{Infrastructure}} for {{Collaborative Enterprises}}},
  author = {Waters, R. and Anderson, D.B. and Schwenke, D.L.},
  year = {1997},
  month = jun,
  pages = {140--147},
  doi = {10.1109/ENABL.1997.630805},
  urldate = {2023-12-23},
  abstract = {The Interactive Sharing Transfer Protocol (ISTP) supports the sharing of information about a virtual world among a group of processes. The key advantages of ISTP are that it supports: near real-time interaction among users; the communication of every kind of information required in a virtual world; and scalability to large numbers of users and large virtual worlds. ISTP was developed in the context of the Spline platform for distributed virtual environments (DVEs).},
  file = {/home/nunoa/Zotero/storage/FJA2KJCE/Waters et al. - 1997 - Design of the Interactive Sharing Transfer Protoco.pdf;/home/nunoa/Zotero/storage/NBG849DQ/630805.html}
}

@article{watersDiamondParkSpline1997,
  title = {Diamond {{Park}} and {{Spline}}:{{Social Virtual Reality}} with {{3D Animation}}, {{Spoken Interaction}}, and {{Runtime Extendability}}},
  shorttitle = {Diamond {{Park}} and {{Spline}}},
  author = {Waters, Richard C. and Anderson, David B. and Barrus, John W. and Brogan, David C. and Casey, Michael A. and McKeown, Stephan G. and Nitta, Tohei and Sterns, Ilene B. and Yerazunis, William S.},
  year = {1997},
  month = aug,
  journal = {Presence: Teleoperators and Virtual Environments},
  volume = {6},
  number = {4},
  pages = {461--481},
  doi = {10.1162/pres.1997.6.4.461},
  urldate = {2023-12-23},
  abstract = {Diamond Park is a social virtual reality system in which multiple geographically separated users can speak to each other and participate in joint activities. The central theme of the park is cycling. Human visitors to the park are represented by 3D animated avatars and can explore a square mile of 3D terrain. In addition to human visitors, the park hosts a number of computer simulations, including tour buses and autonomous animated figures. Diamond Park is implemented using a software platform called Spline, which makes it easy to build virtual worlds where multiple people interact with each other and with computer simulations in a 3D visual and audio environment. Spline performs all the processing necessary to maintain a distributed, modifiable, and extendable model of a virtual world that is shared between the participants. For more information visit http://www.merl.com.},
  file = {/home/nunoa/Zotero/storage/ML25C8SA/Diamond-Park-and-Spline-Social-Virtual-Reality.html}
}

@article{IEEEStandardModeling2010,
  title = {{{IEEE Standard}} for {{Modeling}} and {{Simulation}} ({{M}}\&{{S}}) {{High Level Architecture}} ({{HLA}}){\textendash} {{Object Model Template}} ({{OMT}}) {{Specification}}},
  year = {2010},
  month = aug,
  journal = {IEEE Std 1516.2-2010 (Revision of IEEE Std 1516.2-2000)},
  pages = {1--110},
  doi = {10.1109/IEEESTD.2010.5557731},
  urldate = {2024-01-06},
  abstract = {High Level Architecture (HLA)-Object Model Template (OMT) specification defines the format and syntax (but not content) of HLA object models. Simulations are abstractions of the real world, and no one simulation can solve all of the functional needs for the modeling and simulation community. It is anticipated that advances in technology will allow for new and different modeling and simulation (M\&S) implementations within the framework of the HLA. The standards contained in this architecture are interrelated and need to be considered as a product set, as a change in one is likely to have an impact on the others. As such, the HLA is an integrated approach that has been developed to provide a common architecture for simulation. Keywords: architecture, class attribute, data distribution management, federate, federation, federation execution, federation object model, framework, High Level Architecture, instance attribute, instance attribute ownership, interaction class, joined federate, object class, object model template, rules, runtime infrastructure, simulation object model, time-constrained, time-regulating.},
  file = {/home/nunoa/Zotero/storage/EU4CDD8L/5557731.html}
}

@article{lamportTimeClocksOrdering1978,
  title = {Time, Clocks, and the Ordering of Events in a Distributed System},
  author = {Lamport, Leslie},
  year = {1978},
  month = jul,
  journal = {Communications of the ACM},
  volume = {21},
  number = {7},
  pages = {558--565},
  issn = {0001-0782},
  doi = {10.1145/359545.359563},
  urldate = {2024-01-06},
  abstract = {The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.},
  keywords = {clock synchronization,computer networks,distributed systems,multiprocess systems},
  file = {/home/nunoa/Zotero/storage/8A2LLKRM/Lamport - 1978 - Time, clocks, and the ordering of events in a dist.pdf}
}

@article{linebargerConcurrencyControlMechanisms2004a,
  title = {Concurrency {{Control Mechanisms}} for {{Closely Coupled Collaboration}} in {{Multithreaded Peer-to-Peer Virtual Environments}}},
  author = {Linebarger, John M. and Kessler, G. Drew},
  year = {2004},
  month = jun,
  journal = {Presence: Teleoperators and Virtual Environments},
  volume = {13},
  number = {3},
  pages = {296--314},
  issn = {1054-7460},
  doi = {10.1162/1054746041422316},
  urldate = {2024-01-06},
  abstract = {As collaboration in virtual environments becomes more object-focused and closely coupled, the frequency of conflicts in accessing shared objects can increase. In addition, two kinds of concurrency control ``surprises'' become more disruptive to the collaboration. Undo surprises can occur when a previously visible change is undone because of an access conflict. Intention surprises can happen when a concurrent action by a remote session changes the structure of a shared object at the same perceived time as a local access of that object, such that the local user might not get what they expect because they have not had time to visually process the change. A hierarchy of three concurrency control mechanisms is presented in descending order of collaborative surprises, which allows the concurrency scheme to be tailored to the tolerance for such surprises. One mechanism is semioptimistic; the other two are pessimistic. Designed for peer-to-peer virtual environments in which several threads have access to the shared scene graph, these algorithms are straightforward and relatively simple. They can be implemented using C/C++ and Java, under Windows and Unix, on both desktop and immersive systems. In a series of usability experiments, the average performance of the most conservative concurrency control mechanism on a local LAN was found to be quite acceptable.},
  langid = {english}
}

@article{yeungInternetScalingBackbone1997,
  title = {Internet 2: Scaling up the Backbone for {{R}}\&{{D}}},
  shorttitle = {Internet 2},
  author = {Yeung, F.},
  year = {1997},
  month = mar,
  journal = {IEEE Internet Computing},
  volume = {1},
  number = {2},
  pages = {36--37},
  issn = {1941-0131},
  doi = {10.1109/4236.601096},
  urldate = {2024-01-06},
  abstract = {In 1996 universities in the United States announced a plan to create a faster, more powerful network for academic purposes, calling their project Internet 2. The announcement capped work from several meetings held over the previous year to articulate the network requirements of the research community, requirements that participants felt would likely not be addressed by the private sector. At about the same time, President Clinton committed the government to spend \$100 million in developing Internet technology, calling his project the Next Generation Internet Initiative (NGII). Thus, the groundwork has been laid for another collaboration between the university research community and government agencies. Its backers hope it will pay off in advanced applications development and ultimately in transfer to the commercial sector, based on the overwhelming impact of the privatization of the NSFnet. The paper discusses the first phase of the project which will implement Internet 2 connections to the National Science Foundation's very high speed backbone network service.},
  file = {/home/nunoa/Zotero/storage/NXSUNTXL/601096.html}
}

@inproceedings{pinhoCooperativeObjectManipulation2002,
  title = {Cooperative Object Manipulation in Immersive Virtual Environments: Framework and Techniques},
  shorttitle = {Cooperative Object Manipulation in Immersive Virtual Environments},
  booktitle = {Proceedings of the {{ACM}} Symposium on {{Virtual}} Reality Software and Technology},
  author = {Pinho, M{\'a}rcio S. and Bowman, Doug A. and Freitas, Carla M. Dal Sasso},
  year = {2002},
  month = nov,
  series = {{{VRST}} '02},
  pages = {171--178},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/585740.585769},
  urldate = {2024-01-06},
  abstract = {Cooperative manipulation refers to the simultaneous manipulation of a virtual object by multiple users in an immersive virtual environment. This paper describes a framework supporting the development of collaborative manipulation techniques, and example techniques we have tested within this framework. We describe the modeling of cooperative interaction techniques, methods of combining simultaneous user actions, and the awareness tools used to provide the necessary knowledge of partner activities during the cooperative interaction process. Our framework is based on a Collaborative Metaphor concept that defines rules to combine user interaction techniques. The combination is based on the separation of degrees of freedom between two users. Finally, we present novel combinations of two interaction techniques (Simple Virtual Hand and Ray-casting).},
  isbn = {978-1-58113-530-5},
  keywords = {cooperative interaction,interaction in virtual environments}
}

@inproceedings{martinetEffectDOFSeparation2010,
  title = {The Effect of {{DOF}} Separation in {{3D}} Manipulation Tasks with Multi-Touch Displays},
  booktitle = {Proceedings of the 17th {{ACM Symposium}} on {{Virtual Reality Software}} and {{Technology}}},
  author = {Martinet, Anthony and Casiez, G{\'e}ry and Grisoni, Laurent},
  year = {2010},
  month = nov,
  series = {{{VRST}} '10},
  pages = {111--118},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1889863.1889888},
  urldate = {2024-01-07},
  abstract = {Multi-touch displays represent a promising technology for the display and manipulation of data. While the manipulation of 2D data has been widely explored, 3D manipulation with multi-touch displays remains largely uncovered. Based on an analysis of the integration and separation of degrees of freedom, we propose a taxonomy for 3D manipulation techniques with multi-touch displays. Using that taxonomy, we introduce DS3 (Depth-Separated Screen Space), a new 3D manipulation technique based on the separation of translation and rotation. In a controlled experiment, we compare DS3 with Sticky Tools and Screen-Space. Results show that separating the control of translation and rotation significantly affects performance for 3D manipulation, with DS3 being at least 22\% faster.},
  isbn = {978-1-4503-0441-2},
  keywords = {3D manipulation task,direct manipulation,multi-touch displays},
  file = {/home/nunoa/Zotero/storage/JGIL742T/Martinet et al. - 2010 - The effect of DOF separation in 3D manipulation ta.pdf}
}

@article{almeidaSIT6IndirectTouchbased2023,
  title = {{{SIT6}}: {{Indirect}} Touch-Based Object Manipulation for {{DeskVR}}},
  shorttitle = {{{SIT6}}},
  author = {Almeida, Diogo and Mendes, Daniel and Rodrigues, Rui},
  year = {2023},
  month = dec,
  journal = {Computers \& Graphics},
  volume = {117},
  pages = {51--60},
  issn = {0097-8493},
  doi = {10.1016/j.cag.2023.10.013},
  urldate = {2024-01-06},
  abstract = {Virtual reality (VR) has the potential to significantly boost productivity in professional settings, especially those that can benefit from immersive environments that allow a better and more thorough way of visualizing information. However, the physical demands of mid-air movements make it difficult to use VR for extended periods. DeskVR offers a solution that allows users to engage in VR while seated at a desk, minimizing physical exhaustion. However, developing appropriate motion techniques for this context is challenging due to limited mobility and space constraints. This work focuses on object manipulation techniques, exploring touch-based and mid-air-based approaches to design a suitable solution for DeskVR, hypothesizing that touch-based object manipulation techniques could be as effective as mid-air object manipulation in a DeskVR scenario while less physically demanding. Thus, we propose Scaled Indirect Touch 6-DOF (SIT6), an indirect touch-based object manipulation technique incorporating scaled input mapping to address precision and out-of-reach manipulation issues. The implementation of our solution consists of a state machine with error-handling mechanisms and visual indicators to enhance interaction. User experiments were conducted to compare the SIT6 technique with a baseline mid-air approach, revealing comparable effectiveness while demanding less physical exertion. These results validated our hypothesis and established SIT6 as a viable option for object manipulation in DeskVR scenarios.},
  keywords = {DeskVR,Object manipulation,Virtual reality},
  file = {/home/nunoa/Zotero/storage/BXMZKMUT/S0097849323002509.html}
}

@article{bowmanFormalizingDesignEvaluation1999,
  title = {Formalizing the {{Design}}, {{Evaluation}}, and {{Application}} of {{Interaction Techniques}} for {{Immersive Virtual Environments}}},
  author = {Bowman, DOUG A. and Hodges, LARRY F.},
  year = {1999},
  month = feb,
  journal = {Journal of Visual Languages \& Computing},
  volume = {10},
  number = {1},
  pages = {37--53},
  issn = {1045-926X},
  doi = {10.1006/jvlc.1998.0111},
  urldate = {2024-01-07},
  abstract = {Immersive virtual environments (VEs) have potential in many application areas, but many complex VE systems exhibit usability and interaction problems. This is partly due to a lack of consideration or understanding of 3D interaction tasks and techniques. This paper proposes the systematic study of the design, evaluation, and application of VE interaction techniques. In this methodology, design and evaluation are based on a formal task analysis and categorization of techniques, using multiple performance measures. As a direct consequence of our use of this methodology, we also present a variety of novel designs and evaluation results with respect to interaction techniques for three common VE tasks.},
  file = {/home/nunoa/Zotero/storage/WYL2BCVU/S1045926X98901112.html}
}

@article{leerssenEndShadowBanning2023,
  title = {An End to Shadow Banning? {{Transparency}} Rights in the {{Digital Services Act}} between Content Moderation and Curation},
  shorttitle = {An End to Shadow Banning?},
  author = {Leerssen, Paddy},
  year = {2023},
  month = apr,
  journal = {Computer Law \& Security Review},
  volume = {48},
  pages = {105790},
  issn = {0267-3649},
  doi = {10.1016/j.clsr.2023.105790},
  urldate = {2024-01-16},
  abstract = {This paper offers a legal perspective on the phenomenon of shadow banning: content moderation sanctions which are undetectable to those affected. Drawing on recent social science research, it connects current concerns about shadow banning to novel visibility management techniques in content moderation, such as delisting and demotion. Conventional moderation techniques such as outright content removal or account suspension can be observed by those affected, but these new visibility often cannot. This lends newfound significance to the legal question of moderation transparency rights. The EU Digital Services Act (DSA) is analysed in this light, as the first major legislation to regulate transparency of visibility remedies. In effect, its due process framework prohibits shadow banning with only limited exceptions. In doing so, the DSA surfaces tensions between two competing models for content moderation: as rule-bound administration or as adversarial security conflict. I discuss possible interpretations and trade-offs for this regime, and then turn to a more fundamental problem: how to define visibility reduction as a category of content moderation actions. The concept of visibility reduction or `demotions' is central to both the shadow banning imaginary and to the DSA's safeguards, but its meaning is far from straightforward. Responding to claims that demotion is entirely relative, and therefore not actionable as a category of content moderation sanctions, I show how visibility reduction can still be regulated when defined as ex post adjustments to engagement-based relevance scores. Still, regulating demotion in this way will not cover all exercises of ranking power, since it manifests not only in individual cases of moderation but also through structural acts of content curation; not just by reducing visibility, but by producing visibility.},
  keywords = {Content curation,Content moderation,Platforms,Shadow banning,Transparency},
  file = {C\:\\Users\\nuno_\\Zotero\\storage\\BYJZJ7UC\\Leerssen - 2023 - An end to shadow banning Transparency rights in t.pdf;C\:\\Users\\nuno_\\Zotero\\storage\\SK55J5KP\\S0267364923000018.html}
}

@inproceedings{mendesBenefitsDOFSeparation2016a,
  title = {The Benefits of {{DOF}} Separation in Mid-Air {{3D}} Object Manipulation},
  booktitle = {Proceedings of the 22nd {{ACM Conference}} on {{Virtual Reality Software}} and {{Technology}}},
  author = {Mendes, Daniel and Relvas, Filipe and Ferreira, Alfredo and Jorge, Joaquim},
  year = {2016},
  month = nov,
  pages = {261--268},
  publisher = {{ACM}},
  address = {{Munich Germany}},
  doi = {10.1145/2993369.2993396},
  urldate = {2024-01-18},
  abstract = {Object manipulation is a key feature in almost every virtual environment. However, it is difficult to accurately place an object in immersive virtual environments using mid-air gestures that mimic interactions in the physical world, although being a direct and natural approach. Previous research studied mouse and touch based interfaces concluding that separation of degrees-of-freedom (DOF) led to improved results. In this paper, we present the first user evaluation to assess the impact of explicit 6 DOF separation in mid-air manipulation tasks. We implemented a technique based on familiar virtual widgets that allow single DOF control, and compared it against a direct approach and PRISM, which dynamically adjusts the ratio between hand and object motions. Our results suggest that full DOF separation benefits precision in spatial manipulations, at the cost of additional time for complex tasks. From our results we draw guidelines for 3D object manipulation in mid-air.},
  isbn = {978-1-4503-4491-3},
  langid = {english},
  file = {/home/nunoa/Zotero/storage/ENYWV5XK/Mendes et al. - 2016 - The benefits of DOF separation in mid-air 3D objec.pdf}
}

@inproceedings{zielaskoMenusDeskSystem2019,
  title = {Menus on the {{Desk}}? {{System Control}} in {{DeskVR}}},
  shorttitle = {Menus on the {{Desk}}?},
  booktitle = {2019 {{IEEE Conference}} on {{Virtual Reality}} and {{3D User Interfaces}} ({{VR}})},
  author = {Zielasko, Daniel and Kr{\"u}ger, Marcel and Weyers, Benjamin and Kuhlen, Torsten W.},
  year = {2019},
  month = mar,
  pages = {1287--1288},
  issn = {2642-5254},
  doi = {10.1109/VR.2019.8797900},
  urldate = {2024-01-18},
  abstract = {In this work, we evaluate the impact of passive haptic feedback on touch-based menus, given the constraints and possibilities of a seated, desk-based scenario in VR. Therefore, we compare a menu that once is placed on the surface of a desk and once mid-air on a surface in front of the user. The study design is completed by two conditions without passive haptic feedback. In the conducted user study (n=33), we found effects of passive haptics (present vs-non-present) and menu alignment (desk vs. mid-air) on the task performance and subjective look \& feel. However, the race between the conditions was close. An overall winner was the mid-air menu with passive haptic feedback, which however raises hardware requirements.},
  file = {/home/nunoa/Zotero/storage/4YIUBTUI/Zielasko et al. - 2019 - Menus on the Desk System Control in DeskVR.pdf;/home/nunoa/Zotero/storage/EJ5EDMEN/8797900.html}
}

@phdthesis{amaroNavigationTechniquesSeated2021,
  title = {Navigation {{Techniques}} for {{Seated VR}}},
  author = {Amaro, Guilherme dos Santos},
  year = {2021},
  month = oct,
  urldate = {2024-01-18},
  copyright = {openAccess},
  langid = {english},
  school = {Faculdade de Engenharia da Universidade do Porto},
  annotation = {Accepted: 2023-01-30T01:03:16Z},
  file = {/home/nunoa/Zotero/storage/5XKIV5TF/Amaro - 2021 - Navigation Techniques for Seated VR.pdf}
}

@inproceedings{bowmanEvaluationTechniquesGrabbing1997,
  title = {An Evaluation of Techniques for Grabbing and Manipulating Remote Objects in Immersive Virtual Environments},
  booktitle = {Proceedings of the 1997 Symposium on {{Interactive 3D}} Graphics},
  author = {Bowman, Doug A. and Hodges, Larry F.},
  year = {1997},
  month = apr,
  series = {{{I3D}} '97},
  pages = {35--ff.},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/253284.253301},
  urldate = {2024-01-18},
  isbn = {978-0-89791-884-8},
  file = {/home/nunoa/Zotero/storage/C7UCAD2S/Bowman and Hodges - 1997 - An evaluation of techniques for grabbing and manip.pdf}
}

@inproceedings{mendesMidairInteractionsStereoscopic2014,
  title = {Mid-Air Interactions above Stereoscopic Interactive Tables},
  booktitle = {2014 {{IEEE Symposium}} on {{3D User Interfaces}} ({{3DUI}})},
  author = {Mendes, Daniel and Fonseca, Fernando and Ara{\`u}jo, Bruno and Ferreira, Alfredo and Jorge, Joaquim},
  year = {2014},
  month = mar,
  pages = {3--10},
  doi = {10.1109/3DUI.2014.6798833},
  urldate = {2024-01-18},
  abstract = {Stereoscopic tabletops offer unique visualization capabilities, enabling users to perceive virtual objects as if they were lying above the surface. While allowing virtual objects to coexist with user actions in the physical world, interaction with these virtual objects above the surface presents interesting challenges. In this paper, we aim to understand which approaches to 3D virtual object manipulations are suited to this scenario. To this end, we implemented five different techniques based on the literature. Four are mid-air techniques, while the remainder relies on multi-touch gestures, which act as a baseline. Our setup combines affordable non-intrusive tracking technologies with a multi-touch stereo tabletop, providing head and hands tracking, to improve both depth perception and seamless interactions above the table. We conducted a user evaluation to find out which technique appealed most to participants. Results suggest that mid-air interactions, combining direct manipulation with six degrees of freedom for the dominant hand, are both more satisfying and efficient than the alternatives tested.},
  file = {/home/nunoa/Zotero/storage/72QCNVCQ/6798833.html}
}

@inproceedings{simeoneIndirectTouchManipulation2016,
  title = {Indirect Touch Manipulation for Interaction with Stereoscopic Displays},
  booktitle = {2016 {{IEEE Symposium}} on {{3D User Interfaces}} ({{3DUI}})},
  author = {Simeone, Adalberto L.},
  year = {2016},
  month = mar,
  pages = {13--22},
  doi = {10.1109/3DUI.2016.7460025},
  urldate = {2024-01-18},
  abstract = {Research on 3D interaction has explored the application of multi-touch technologies to 3D stereoscopic displays. However, the ability to perceive 3D objects at different depths (in front or behind the screen surface) conflicts with the necessity of expressing inputs on the screen surface. Touching the screen increases the risk of causing the vergence-accommodation conflict which can lead to the loss of the stereoscopic effect or cause discomfort. In this work, we present two studies evaluating a novel approach based on the concept of indirect touch interaction via an external multi-touch device. We compare indirect touch techniques to two state-of-the-art 3D interaction techniques: DS3 and the Triangle Cursor. The first study offers a quantitative and qualitative study of direct and indirect interaction on a 4 DOF docking task. The second presents a follow-up experiment focusing on a 6 DOF docking task. Results show that indirect touch interaction techniques provide a more comfortable viewing experience than both techniques. It also shows that there are no drawbacks when switching to indirect touch, as their performances in terms of net manipulation times are comparable.},
  file = {/home/nunoa/Zotero/storage/BY4P4RJC/Simeone - 2016 - Indirect touch manipulation for interaction with s.pdf;/home/nunoa/Zotero/storage/5K4LMR33/7460025.html}
}

@inproceedings{wilkesAdvantagesVelocitybasedScaling2008,
  title = {Advantages of Velocity-Based Scaling for Distant {{3D}} Manipulation},
  booktitle = {Proceedings of the 2008 {{ACM}} Symposium on {{Virtual}} Reality Software and Technology},
  author = {Wilkes, Curtis and Bowman, Doug A.},
  year = {2008},
  month = oct,
  series = {{{VRST}} '08},
  pages = {23--29},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1450579.1450585},
  urldate = {2024-01-18},
  abstract = {Immersive virtual environments (VEs) have the potential to offer rich three-dimensional interaction to users. In many instances, however, 3D interaction tasks are difficult due to both the imprecision of tracking devices and the inability of users to achieve and maintain precise hand positions in 3D space. One way to improve upon existing interaction techniques is to dynamically change the sensitivity of the interaction technique based on user input. Previous research has applied this principle to virtual hand-based manipulation techniques; when the user slows down the movement of her physical hand, the virtual hand slows down even more to allow precise manipulation. In this study we extend the prior research by applying the velocity-based scaling principle to HOMER, an existing at-a-distance manipulation technique based on ray-casting. The scaled HOMER technique offers the user the freedom to accomplish both long- and short-distance manipulation tasks with higher levels of precision without compromising speed. We present results from a user study that shows that the addition of scaling to HOMER significantly improves user performance on 3D manipulation tasks.},
  isbn = {978-1-59593-951-7},
  keywords = {3D interaction,usability,user studies}
}

@article{tianUsingVirtualReplicas2023,
  title = {Using {{Virtual Replicas}} to {{Improve Mixed Reality Remote Collaboration}}},
  author = {Tian, Huayuan and Lee, Gun A. and Bai, Huidong and Billinghurst, Mark},
  year = {2023},
  month = may,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {29},
  number = {5},
  pages = {2785--2795},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2023.3247113},
  urldate = {2024-01-22},
  abstract = {In this paper, we explore how virtual replicas can enhance Mixed Reality (MR) remote collaboration with a 3D reconstruction of the task space. People in different locations may need to work together remotely on complicated tasks. For example, a local user could follow a remote expert's instructions to complete a physical task. However, it could be challenging for the local user to fully understand the remote expert's intentions without effective spatial referencing and action demonstration. In this research, we investigate how virtual replicas can work as a spatial communication cue to improve MR remote collaboration. This approach segments the foreground manipulable objects in the local environment and creates corresponding virtual replicas of physical task objects. The remote user can then manipulate these virtual replicas to explain the task and guide their partner. This enables the local user to rapidly and accurately understand the remote expert's intentions and instructions. Our user study with an object assembly task found that using virtual replica manipulation was more efficient than using 3D annotation drawing in an MR remote collaboration scenario. We report and discuss the findings and limitations of our system and study, and present directions for future research.},
  file = {C:\Users\nuno_\Zotero\storage\ZLWMTC4G\10049700.html}
}

@inproceedings{pereiraExtendedRealityFramework2019,
  title = {Extended {{Reality Framework}} for {{Remote Collaborative Interactions}} in {{Virtual Environments}}},
  booktitle = {2019 {{International Conference}} on {{Graphics}} and {{Interaction}} ({{ICGI}})},
  author = {Pereira, Vasco and Matos, Teresa and Rodrigues, Rui and N{\'o}brega, Rui and Jacob, Jo{\~a}o},
  year = {2019},
  month = nov,
  pages = {17--24},
  doi = {10.1109/ICGI47575.2019.8955025},
  urldate = {2024-01-25},
  abstract = {This paper proposes the implementation of a framework for the development of collaborative extended reality (XR) applications. Using the framework, developers can focus on understanding which collaborative mechanisms they need to implement for the respective reality model application. In this paper we specifically study collaborative mechanisms around object manipulation in Virtual Reality (VR). As such, we planned a VR prototype using the proposed framework, which was used to validate the various interaction and collaboration features in VR. The gathered data from the user tests revealed that they enjoyed the experience and the collaborative mechanisms helped them work together. Furthermore, to understand whether the framework allowed for the development of XR applications, we decided to implement an augmented reality prototype as well. Afterwards, we ran an experiment with 4 VR and 3 AR users sharing the same virtual environment. The experiment was successful at allowing them to interact in real-time in the same shared environment. Therefore, the framework enables the development of XR applications that support different mixed-reality technologies.},
  keywords = {Augmented Reality,Avatars,Collaboration,Extended reality,Extended Reality,Interaction,Prototypes,Tools,Virtual Reality,Visualization,X reality},
  file = {C:\Users\nuno_\Zotero\storage\CUNSSV7S\Pereira et al. - 2019 - Extended Reality Framework for Remote Collaborativ.pdf}
}

@article{rauSpeedReadingVirtual2018,
  title = {Speed Reading on Virtual Reality and Augmented Reality},
  author = {Rau, Pei-Luen Patrick and Zheng, Jian and Guo, Zhi and Li, Jiaqi},
  year = {2018},
  month = oct,
  journal = {Computers \& Education},
  volume = {125},
  pages = {240--245},
  issn = {0360-1315},
  doi = {10.1016/j.compedu.2018.06.016},
  urldate = {2024-01-25},
  abstract = {Many virtual reality (VR) and augmented reality (AR) applications in education require speed reading. The current study aimed to explore whether the reading performance on VR and AR is different from that on traditional desktop display, and whether the difference is moderated by the reading speed. Sixty-three college students read Chinese passages at normal (650{\textendash}750 characters per minute [cpm]) or fast speeds (1000{\textendash}1400 cpm), and then answered multiple-choice questions. They spent approximately 10\% more time in making choice on VR and AR than they did on the desktop display. Teachers should be aware of this difference and allow 10\% more time when using VR and AR applications containing text components.},
  keywords = {Augmented reality,Human-computer interface,Interactive learning environments,Reading performance,Virtual reality},
  file = {C:\Users\nuno_\Zotero\storage\K7EK9RZL\S0360131518301593.html}
}

@article{kimUserDiscomfortUsing2021,
  title = {User Discomfort While Using a Virtual Reality Headset as a Personal Viewing System for Text-Intensive Office Tasks},
  author = {Kim, Eunjee and Shin, Gwanseob},
  year = {2021},
  month = jul,
  journal = {Ergonomics},
  volume = {64},
  number = {7},
  pages = {891--899},
  publisher = {{Taylor \& Francis}},
  issn = {0014-0139},
  doi = {10.1080/00140139.2020.1869320},
  urldate = {2024-01-25},
  abstract = {Ergonomics issues while using virtual reality (VR) headsets for text-intensive applications have not been studied. Measures of neck and shoulder discomfort and simulator sickness symptoms were quantified while participants were performing a document creation task for 60 min using a VR headset and a desktop monitor. During the task with the headset, participants rotated the head 2.7 times more frequently and used the neck extensor muscles 25.9\% more, in average. They also rated the neck and shoulder discomfort 60\% and 17.5\% higher after the task. The simulator sickness symptoms were also rated significantly higher (p {$<$} .05) for the headset condition, with more pronounced differences in the symptoms related to visual discomfort. Results indicate that the physical discomforts due to the frequent head rotations and the headset weight, and visual discomforts due to difficulty in reading texts were the main issues of the VR headset for common office tasks. Practitioner summary: Ergonomics issues associated with the use of a VR headset for conducting office productivity work tasks have been evaluated in an experiment. Study results indicate that the development in the neck physical discomfort and visual discomfort may be the main barriers to the use of current VR headsets for office works. Abbreviations: VR: virtual reality; VDT: video display terminal; EMG: electromyography; MVC: maximum voluntary contraction; SSQ: simulator sickness questionnaire; ECG: electrocardiogram; NEMG: normalised electromyography},
  pmid = {33357004},
  keywords = {office ergonomics,text typing,virtual office,VR HMD}
}

@article{ruddleLevelsControlCollaborative2003,
  title = {Levels of {{Control During}} a {{Collaborative Carrying Task}}},
  author = {Ruddle, Roy A. and Savage, Justin C. D. and Jones, Dylan M.},
  year = {2003},
  month = apr,
  journal = {Presence},
  volume = {12},
  number = {2},
  pages = {140--155},
  issn = {1054-7460},
  doi = {10.1162/105474603321640914},
  urldate = {2024-01-07},
  abstract = {Three experiments investigated the effect of implementing low-level aspects of motor control for a collaborative carrying task within a VE interface, leaving participants free to devote their cognitive resources to the higher-level components of the task. In the task, participants collaborated with an autonomous virtual human in an immersive virtual environment (VE) to carry an object along a predefined path. In experiment 1, participants took up to three times longer to perform the task with a conventional VE interface, in which they had to explicitly coordinate their hand and body movements, than with an interface that controlled the low-level tasks of grasping and holding onto the virtual object. Experiments 2 and 3 extended the study to include the task of carrying an object along a path that contained obstacles to movement. By allowing participants' virtual arms to stretch slightly, the interface software was able to take over some aspects of obstacle avoidance (another low-level task), and this led to further significant reductions in the time that participants took to perform the carrying task. Improvements in performance also occurred when participants used a tethered viewpoint to control their movements because they could see their immediate surroundings in the VEs. This latter finding demonstrates the superiority of a tethered view perspective to a conventional, human'seye perspective for this type of task.},
  file = {C\:\\Users\\nuno_\\Zotero\\storage\\ID3DUBN9\\Ruddle et al. - 2003 - Levels of Control During a Collaborative Carrying .pdf;C\:\\Users\\nuno_\\Zotero\\storage\\3FG5QP6E\\6791168.html}
}

@inproceedings{mossel3DTouchHOMERSIntuitive2013,
  title = {{{3DTouch}} and {{HOMER-S}}: Intuitive Manipulation Techniques for One-Handed Handheld Augmented Reality},
  shorttitle = {{{3DTouch}} and {{HOMER-S}}},
  booktitle = {Proceedings of the {{Virtual Reality International Conference}}: {{Laval Virtual}}},
  author = {Mossel, Annette and Venditti, Benjamin and Kaufmann, Hannes},
  year = {2013},
  month = mar,
  series = {{{VRIC}} '13},
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2466816.2466829},
  urldate = {2024-02-14},
  abstract = {Existing interaction techniques for mobile AR often use the multi-touch capabilities of the device's display for object selection and manipulation. To provide full 3D manipulation by touch in an integral way, existing approaches use complex multi finger and hand gestures. However, they are difficult or impossible to use in one-handed handheld AR scenarios and their usage requires prior knowledge. Furthermore, a handheld's touch screen offers only two dimensions for interaction and limits manipulation to physical screen size. To overcome these problems, we present two novel intuitive six degree-of-freedom (6DOF) manipulation techniques, 3DTouch and HOMER-S. While 3DTouch uses only simple touch gestures and decomposes the degrees of freedom, Homer-S provides full 6DOF and is decoupled from screen input to overcome physical limitations. In a comprehensive user study, we explore performance, usability and accuracy of both techniques. Therefore, we compare 3DTouch with HOMER-S in four different scenarios with varying transformation requirements. Our results reveal both techniques to be intuitive to translate and rotate objects. HOMER-S lacks accuracy compared to 3DTouch but achieves significant performance increases in terms of speed for transformations addressing all 6DOF.},
  isbn = {978-1-4503-1875-4},
  keywords = {3D interaction techniques,3D manipulation,handheld augmented reality}
}

@inproceedings{benkoBalloonSelectionMultiFinger2007,
  title = {Balloon {{Selection}}: {{A Multi-Finger Technique}} for {{Accurate Low-Fatigue 3D Selection}}},
  shorttitle = {Balloon {{Selection}}},
  booktitle = {2007 {{IEEE Symposium}} on {{3D User Interfaces}}},
  author = {Benko, Hrvoje and Feiner, Steven},
  year = {2007},
  month = mar,
  doi = {10.1109/3DUI.2007.340778},
  urldate = {2024-06-02},
  abstract = {Balloon selection is a 3D interaction technique that is modeled after the real world metaphor of manipulating a helium balloon attached to a string. Balloon selection allows for precise 3D selection in the volume above a tabletop surface by using multiple fingers on a multi-touch-sensitive surface. The 3DOF selection tasks is decomposed in part into a 2DOF positioning task performed by one finger on the tabletop in an absolute 2D Cartesian coordinate system and a 1DOF positioning task performed by another finger on the tabletop in a relative 2D polar coordinate system. We have evaluated balloon selection in a formal user study that compared it to two well-known interaction techniques for selecting a static 3D target: a 3DOF tracked wand and keyboard cursor keys. We found that balloon selection was significantly faster than using cursor keys and had a significantly lower error rate than the wand. The lower error rate appeared to result from the user's hands being supported by the tabletop surface, resulting in significantly reduced hand tremor and arm fatigue.},
  keywords = {Augmented reality,Computer science,Electronic mail,Error analysis,Fatigue,Fingers,Helium,Target tracking,User interfaces,Virtual reality},
  file = {C\:\\Users\\nunom\\Zotero\\storage\\JGA6UWEJ\\Benko and Feiner - 2007 - Balloon Selection A Multi-Finger Technique for Ac.pdf;C\:\\Users\\nunom\\Zotero\\storage\\XY3QUK4H\\4142849.html}
}

@inproceedings{daiberBalloonSelectionRevisited2012,
  title = {Balloon Selection Revisited: Multi-Touch Selection Techniques for Stereoscopic Data},
  shorttitle = {Balloon Selection Revisited},
  booktitle = {Proceedings of the {{International Working Conference}} on {{Advanced Visual Interfaces}}},
  author = {Daiber, Florian and Falk, Eric and Kr{\"u}ger, Antonio},
  year = {2012},
  month = may,
  series = {{{AVI}} '12},
  pages = {441--444},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2254556.2254641},
  urldate = {2024-06-02},
  abstract = {With the increasing distribution of multi-touch capable devices multi-touch interaction becomes more and more ubiquitous. Especially the interaction with complex data (e.g. medical or geographical data), which until today mostly rely on mice and keyboard input or intense instrumentation, can benefit from this development. Multi-touch interaction offers new ways to deal with 3D data allowing a high degree of freedom (DOF) without instrumenting the user. This paper evaluates indirect multi-touch 3D selection techniques that can be used to interact with stereoscopic data. In this paper two gestural multi-touch selection techniques are presented and investigated with respect to positions on a stereoscopic multi-touch display and special consideration of objects displayed with different parallaxes. In an experiment it was shown that position and parallax have a significant impact on the interaction.},
  isbn = {978-1-4503-1287-5},
  keywords = {3D user interfaces,gestural interaction,selection techniques,stereoscopic display},
  file = {C:\Users\nunom\Zotero\storage\AH5KYREL\Daiber et al. - 2012 - Balloon selection revisited multi-touch selection.pdf}
}

@inproceedings{strothoffTriangleCursorInteractions2011,
  title = {Triangle Cursor: Interactions with Objects above the Tabletop},
  shorttitle = {Triangle Cursor},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Interactive Tabletops}} and {{Surfaces}}},
  author = {Strothoff, Sven and Valkov, Dimitar and Hinrichs, Klaus},
  year = {2011},
  month = nov,
  series = {{{ITS}} '11},
  pages = {111--119},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2076354.2076377},
  urldate = {2024-06-02},
  abstract = {Extending the tabletop display to the third dimension using a stereoscopic projection offers the possibility to improve applications by using the volume above the table surface. The combination of multi-touch input and stereoscopic projection usually requires an indirect technique to interact with objects above the tabletop, as touches can only be detected on the surface. Triangle Cursor is a 3D interaction technique that allows specification of a 3D position and yaw rotation above the interactive tabletop. It was designed to avoid occlusions that disturb the stereoscopic perception. While Triangle Cursor uses an indirect approach, the position, the height above the surface and the yaw rotation can be controlled simultaneously, resulting in a 4 DOF manipulation technique. We have evaluated Triangle Cursor in an initial user study and compared it to a related existing technique in a formal user study. Our experiments show that users were able to perform all tasks significantly faster with our technique without loosing any precision. Most of the subjects considered the technique easy to use and satisfying.},
  isbn = {978-1-4503-0871-7},
  keywords = {3D interaction,multi-touch interaction,selection technique,stereoscopic displays,tabletop displays},
  file = {C:\Users\nunom\Zotero\storage\4T8UPZD9\Strothoff et al. - 2011 - Triangle cursor interactions with objects above t.pdf}
}

@inproceedings{stoakleyVirtualRealityonaWim1995,
author = {Stoakley, Richard and Conway, Matthew J. and Pausch, Randy},
title = {Virtual reality on a WIM: interactive worlds in miniature},
year = {1995},
isbn = {0201847051},
publisher = {ACM Press/Addison-Wesley Publishing Co.},
address = {USA},
url = {https://doi.org/10.1145/223904.223938},
doi = {10.1145/223904.223938},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {265–272},
numpages = {8},
location = {Denver, Colorado, USA},
series = {CHI '95}
}

@misc{khronosopenxrworkinggroupOpenXR38Specification2024,
  title = {The {{OpenXR}}™ 1.1.38 {{Specification}} (with All Registered Extensions)},
  author = {{Khronos OpenXR Working Group}},
  year = {2024},
  month = jun,
  journal = {The OpenXR™ 1.1.38 Specification (with all registered extensions)},
  urldate = {2024-06-10},
  howpublished = {https://registry.khronos.org/OpenXR/specs/1.1/html/xrspec.html},
  file = {C:\Users\nunom\Zotero\storage\BAUIV9TY\xrspec.html}
}

@Misc{zbMATH03340881,
 Author = {MacQueen, J.},
 Title = {Some methods for classification and analysis of multivariate observations},
 Year = {1967},
 Language = {English},
 HowPublished = {Proc. 5th {Berkeley} {Symp}. {Math}. {Stat}. {Probab}., {Univ}. {Calif}. 1965/66, 1, 281-297 (1967).},
 Keywords = {62H30},
 zbMATH = {3340881},
 Zbl = {0214.46201}
}

@article{lloydLeastSquaresQuantization1982,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  year = {1982},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  issn = {1557-9654},
  doi = {10.1109/TIT.1982.1056489},
  urldate = {2024-06-11},
  abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2{\textasciicircum}bquanta,b=1,2, {\textbackslash}cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
  file = {C\:\\Users\\nunom\\Zotero\\storage\\EEL28K8P\\Lloyd - 1982 - Least squares quantization in PCM.pdf;C\:\\Users\\nunom\\Zotero\\storage\\Q2W86UHM\\1056489.html}
}


@article{1571980074621944832,
author="FORGY E. W.",
title="Cluster analysis of multivariate data : efficiency versus interpretability of classifications",
journal="Biometrics",
year="1965",
volume="21",
pages="768-769",
URL="https://cir.nii.ac.jp/crid/1571980074621944832"
}


@InProceedings{pmlr-v37-ding15,
  title = 	 {Yinyang K-Means: A Drop-In Replacement of the Classic K-Means with Consistent Speedup},
  author = 	 {Ding, Yufei and Zhao, Yue and Shen, Xipeng and Musuvathi, Madanlal and Mytkowicz, Todd},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {579--587},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ding15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/ding15.html},
  abstract = 	 {This paper presents Yinyang K-means, a new algorithm for K-means clustering. By clustering the centers in the initial stage, and leveraging efficiently maintained lower and upper bounds between a point and centers, it more effectively avoids unnecessary distance calculations than prior algorithms. It significantly outperforms classic K-means and prior alternative K-means algorithms consistently across all experimented data sets, cluster numbers, and machine configurations. The consistent, superior performance—plus its simplicity, user-control of overheads, and guarantee in producing the same clustering results as the standard K-means does—makes Yinyang K-means a drop-in replacement of the classic K-means with an order of magnitude higher performance.}
}

@inproceedings{ester1996density,
  title={A density-based algorithm for discovering clusters in large spatial databases with noise},
  author={Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei and others},
  booktitle={kdd},
  volume={96},
  number={34},
  pages={226--231},
  year={1996}
}

@misc{technologiesUnityManualCompute,
  title = {Unity - {{Manual}}: {{Compute}} Shaders},
  shorttitle = {Unity - {{Manual}}},
  author = {Technologies, Unity},
  urldate = {2024-06-16},
  howpublished = {https://docs.unity3d.com/Manual/class-ComputeShader.html},
  langid = {english},
  file = {C:\Users\nunom\Zotero\storage\IBM245W4\class-ComputeShader.html}
}

@incollection{hart1988development,
  title={Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research},
  author={Hart, Sandra G and Staveland, Lowell E},
  booktitle={Advances in psychology},
  volume={52},
  pages={139--183},
  year={1988},
  publisher={Elsevier}
}
